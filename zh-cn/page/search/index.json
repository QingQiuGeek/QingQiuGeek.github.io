[{"content":" 个人理解：每个公司每个团队所掌握的擅长的技术栈不同，所做的业务不同，因此要形成一套成体系规范化的开发流程，从构建工具、开发语言、框架选型、版本管理，到部署迭代，目的在于规范开发流程，提高开发效率，减少项目出错，确保项目稳定运行及后续迭代。 参考：\nhttps://segmentfault.com/a/1190000046908475\n前端工程化与工具链全解析：模块化、代码质量与框架生态-CSDN 博客\n2025 年细讲前端工程化 万字总结！！-CSDN 博客\n前端工程化 \u0026ndash; 工程化体系详解 | EnlightenCode\n工程化 构建工具 WebPack：将应用中的各种资源（JS、CSS、图片、字体等）都看作模块，然后将它们打包成一个或多个最终文件。同时提供开发模式下的热更新（HMR），加速开发过程，但是 Webpack 的配置文件通常比较复杂，学习成本高。 Vite：现代化的构建工具，设计理念是快速开发和高效的构建，它的核心优势是采用了原生 ES 模块（ESM）作为开发模式，并且利用浏览器对 ESM 的支持，来避免传统打包工具中常见的冗长的打包时间。同时支持局部热更新，不需要全局重载，更新速度比 Webpack 更加快速和精确。 Rspack\n基于 Rust 的打包工具，Rspack 包管理工具 npm（node package manager），Node.js 的默认包管理器，package-lock.json 文件确保了不同环境中依赖的一致性。 Yarn，Facebook 提供的一个 JavaScript 包管理工具，Yarn 通过缓存和并行化安装等方式，Yarn 的安装速度通常比 npm 更快。 pnpm，更高效的 JavaScript 包管理工具，旨在解决 npm 和 Yarn 在处理大量依赖时的性能瓶颈问题。通过硬链接和全局缓存，pnpm 极大地减少了磁盘占用，尤其是在多个项目共享相同依赖的情况下。安装依赖时，不会为每个项目创建独立的依赖副本，而是通过共享缓存来节省空间。 cnpm：淘宝镜像，解决国内访问的问题 模块化 代码规范 ESLint：静态代码分析，强制统一编码风格。 Prettier：自动格式化代码，解决团队风格冲突。 版本管理 集成 Git Hooks 工具如 Husky 和 Lint-Staged\nCI/CD github Action ","date":"2025-07-28T15:15:47Z","permalink":"/zh-cn/post/2025/07/%E5%89%8D%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%8C%96%E4%B8%8E%E5%B7%A5%E5%85%B7%E9%93%BE/","title":"前端工程化与工具链"},{"content":" 提示工程指南 | Prompt Engineering Guide\n提问的智慧\n为什么要优化Prompt 通过设计有效的提示词 来指导模型执行期望任务。\n用人话就是给模型一个任务，正确的告诉它怎么做、做什么。\n几个名词 Temperature temperature 决定了下一个词的随机程度，的参数值越小，模型就会返回越确定的一个结果。参数值越大，模型会返回更随机的结果，也就是说这可能会带来更多样化或创造性的产出。\nTop_p 与 temperature 一起称为核采样，用来控制模型返回结果的确定性。如果你需要准确和事实的答案，就把参数值调低，反之调高。\n**Temperature：**控制生成文本的随机性，决定模型选择下一个词时的随机程度。\nTop_p： 控制生成文本的多样性，决定模型只考虑累积概率总和不超过 P 的词。\n举例：假设模型预测下一个词的概率分布为：\n\u0026ldquo;apple\u0026rdquo;: 0.8\n\u0026ldquo;banana\u0026rdquo;: 0.1\n\u0026ldquo;cherry\u0026rdquo;: 0.05\n\u0026ldquo;date\u0026rdquo;: 0.03\n\u0026ldquo;elderberry\u0026rdquo;: 0.02\n如果top_p=0.9，那么会考虑选择\u0026quot;apple\u0026quot; 和 \u0026ldquo;banana\u0026rdquo;，（累积概率为 0.9），忽略 \u0026ldquo;cherry\u0026rdquo;、\u0026ldquo;date\u0026rdquo; 和 \u0026ldquo;elderberry\u0026rdquo;，所以top_p是增加生成文本的多样性，temperature是控制文本生成的随机性。\nStop Sequences 终止序列，当模型生成的文本包含终止序列中的字符串时会停止生成。\nFrequency Penalty Frequency penalty 是对下一个生成的 token 进行惩罚，某个token出现的次数越多，Frequency penalty 越高，那么再次出现的可能性就越小。\nPresence Penalty presence penalty 也是对重复的 token 施加惩罚，但与 frequency penalty 不同的是，对于所有重复 token 的惩罚都是相同的。出现2次的 token 和出现 10 次的 token 会受到相同的惩罚。\n提示词要素 明确的指令（任务）、上下文、期望的输出格式或类型，比如让模型写一个函数方法，我们需要提供：方法要实现的功能（任务）、环境变量（上下文）、出入参参数及类型（出入参格式）\n提示词技巧 1.明确的指令，告诉模型要做什么\n2.避免告诉模型不要做什么，而是告诉模型要做什么\n提示技术 零样本提示 所谓零样本就是不提供范例，直接给任务，如：\n少样本提示 提供范例给模型，如：\n少样本提示的限制：即使增加样本提示数量也不足以获得较为复杂的推理问题的可靠响应。\nCOT(Chain of Thought) 思维链相较于上面的少样本提示，仅添加了推理过程，只需1个样本就获得了正确的推理结果。\n零样本COT提示 不提供样本，让模型逐步思考，最终得出正确结果：\nReAct（Reasoning and Action） 推理、行动，像人一样进行推理，拆分复杂任务，调用工具，一步步执行。\n--- ","date":"2025-07-24T22:25:14Z","permalink":"/zh-cn/post/2025/07/%E5%A4%A7%E6%A8%A1%E5%9E%8Bprompt%E4%BC%98%E5%8C%96%E5%B7%A5%E7%A8%8B/","title":"大模型Prompt优化工程"},{"content":"CAS的关键实现 在 Java 中，实现 CAS操作的一个关键类是Unsafe，位于sun.misc包下：\n1 2 3 4 5 6 A collection of methods for performing low-level, unsafe operations. Although the class and all methods are public, use of this class is limited because only trusted code can obtain instances of it. Note: It is the resposibility of the caller to make sure arguments are checked before methods of this class are called. While some rudimentary checks are performed on the input, the checks are best effort and when performance is an overriding priority, as when methods of this class are optimized by the runtime compiler, some or all checks (if any) may be elided. Hence, the caller must not rely on the checks and corresponding exceptions! 该类并不推荐开发者在应用程序中使用，而是用于 JVM 内部或一些需要极高性能和底层访问的库中。\nsun.misc.Unsafe类提供了compareAndSwapObject、compareAndSwapInt、compareAndSwapLong方法来实现的对Object、int、long类型的 CAS 操作，如：\n1 2 3 4 5 6 7 8 9 private static final jdk.internal.misc.Unsafe theInternalUnsafe = jdk.internal.misc.Unsafe.getUnsafe(); @ForceInline public final boolean compareAndSwapInt(Object o, long offset, int expected, int x) { return theInternalUnsafe.compareAndSetInt(o, offset, expected, x); } 1 其中theInternalUnsafe是jdk.internal.misc包下Unsafe的实例，它的compareAndSetInt方法则是本地方法，如下： 1 2 3 4 @HotSpotIntrinsicCandidate public final native boolean compareAndSetInt(Object o, long offset, int expected, int x); Unsafe的具体使用 在juc包的atomic包提供了一些原子操作类，这些atomic类依赖于CAS乐观锁保证原子性，以AtomicInteger核心源码为例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // 获取 Unsafe 实例 private static final jdk.internal.misc.Unsafe U = jdk.internal.misc.Unsafe.getUnsafe(); private static final long valueOffset; static { try { // 获取\u0026#34;value\u0026#34;字段在AtomicInteger类中的内存偏移量 valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(\u0026#34;value\u0026#34;)); } catch (Exception ex) { throw new Error(ex); } } // 确保\u0026#34;value\u0026#34;字段的可见性 private volatile int value; // 如果当前值等于预期值，则原子地将值设置为newValue // 使用 Unsafe#compareAndSwapInt 方法进行CAS操作 public final boolean compareAndSet(int expect, int update) { return U.compareAndSwapInt(this, valueOffset, expect, update); } // 原子地将当前值加 delta 并返回旧值 public final int getAndAdd(int delta) { return U.getAndAddInt(this, valueOffset, delta); } // 原子地将当前值加 1 并返回加之前的值（旧值） // 使用 Unsafe#getAndAddInt 方法进行CAS操作。 public final int getAndIncrement() { return U.getAndAddInt(this, valueOffset, 1); } // 原子地将当前值减 1 并返回减之前的值（旧值） public final int getAndDecrement() { return U.getAndAddInt(this, valueOffset, -1); } sun.internal.misc.Unsafe类有如下方法：\n1 2 3 4 5 6 7 8 9 10 // 原子地获取并增加整数值 public final int getAndAddInt(Object o, long offset, int delta) { int v; do { // 以 volatile 方式获取对象 o 在内存偏移量 offset 处的整数值 v = getIntVolatile(o, offset); } while (!compareAndSwapInt(o, offset, v, v + delta)); // 返回旧值 return v; } CAS执行流程 线程从主存读取要修改的值存到本地线程缓存中 执行 CAS 操作，将本地线程缓存中的值与主内存中的值进行比较； 如果本地线程缓存中的值与主内存中的值相等，则将需要修改的值在本地线程缓存中修改； 如果修改成功，将修改后的值写入主内存，并返回修改结果；如果失败，则返回当前主内存中的值； 在多线程并发执行的情况下，如果多个线程同时执行 CAS 操作，只有一个线程的 CAS 操作会成功，其他线程的 CAS 操作都会失败，这也是 CAS 的原子性保证。 CAS问题 1.ABA问题\n线程读取某变量的时候值为A，再次读取的时候值仍为A，不能说明该变量是否被其他线程改过，解决思路是在变量前面追加上版本号或者时间戳 。JDK 1.5 以后的 AtomicStampedReference 类就是用来解决 ABA 问题的，其中的 compareAndSet() 方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。\n2.循环时间长开销大\n3.只能保证一个共享变量的原子操作 ","date":"2025-07-19T19:22:50Z","permalink":"/zh-cn/post/2025/07/java%E7%9A%84cas%E6%98%AF%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%9A%84aba%E9%97%AE%E9%A2%98/","title":"Java的CAS是如何实现的、ABA问题"},{"content":"传播行为 事务传播行为是为了解决业务层方法之间互相调用的事务问题。\n事务方法A被事务方法B调用，就要指定事务如何传播，是两者共用同一事务还是另起一个新事务。\n图解spring中七种事务传播行为 终于有人讲明白了_spring七种事务传播行为-CSDN博客\n1. REQUIRED\n@Transactional注解默认使用就是这个事务传播行为。\n如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。\n2. REQUIRES_NEW\n另起炉灶。 先创建一个新事务 ，如果当前存在事务则把当前事务挂起 。也就是说不管外部方法是否开启事务，Propagation.REQUIRES_NEW修饰的内部方法都会新开启自己的事务，且开启的事务相互独立，互不干扰。\n3. SUPPORTS\n有事务就蹭，没事务就裸奔。 如果当前存在事务，则加入该事务；如果当前没有事务，以非事务的方式运行。\n4. NOT_SUPPORTED\n不管有没有事务，必须裸奔。 如果当前存在事务，则把当前事务挂起，以非事务执行。如果当前没有事务，以非事务方式运行。\n5. MANDATORY（强制性）\n有事务就蹭，没事务就抛异常。 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性）\n6. NEVER\n如果当前存在事务，则抛出异常。如果当前没有事务，以非事务方式运行，\n7.NEST（嵌套）\n如果当前存在事务，则创建一个新事务作为当前事务的嵌套事务来运行；如果当前没有事务，则创建新事务。\n隔离级别 spring的事务隔离级别和mysql几乎一模一样。\nDEFAULT\n使用后端数据库默认隔离级别，Mysql默认是可重复读\nREAD_UNCOMMITED\n最低的隔离级别，允许读取尚未提交的数据变更，有脏读、幻读、不可重复读的问题。\nREAD_COMMITED\n允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生\nREPEATABLE_READ\n对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。\nSERIALIZABLE\n最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就不产生干扰，阻止幻读。\n--- ","date":"2025-07-19T19:20:50Z","permalink":"/zh-cn/post/2025/07/spring7%E4%B8%AA%E4%BA%8B%E5%8A%A1%E4%BC%A0%E6%92%AD%E8%A1%8C%E4%B8%BA%E5%92%8C5%E4%B8%AA%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/","title":"Spring7个事务传播行为和5个隔离级别"},{"content":" 两种方案的token、用户登录信息都存储在redis中！！\n方案一 该方案是前端把token和token有效期一起加密存储到浏览器的localStorage中，每次请求时调用前端的getTokenIsExpiry()获取token并检查token是否过期，过期则remove并跳转登录页，这样前端有个问题就是前端也要知道token的有效期，需要和后端的token有效期保持一致，而后端则提供两个拦截器，分别用来刷新token、判断是否是登录用户，这个参考了黑马外卖。\n后端 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 /** * @Author:懒大王Smile * @Date: 2024/9/14 * @Time: 18:07 * @Description: 登录拦截器 */ @Component public class LoginInterceptor implements HandlerInterceptor { /* * authorization为空和redis的token失效的都放行到登录拦截器 * */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler){ String requestURI = request.getRequestURI(); if (requestURI.contains(\u0026#34;/api/favicon.ico\u0026#34;) || requestURI.contains(\u0026#34;/api-docs\u0026#34;) || requestURI.contains(\u0026#34;/error\u0026#34;)) { return true; } if (UserContext.getUser() == null) { response.setStatus(401); //response.setHeader(\u0026#34;登录拦截器：\u0026#34;,\u0026#34;该请求被拦截，请登录！\u0026#34;); throw new BusinessException(ErrorCode.NOT_LOGIN_ERROR, ErrorInfo.NOT_LOGIN_ERROR); } return true; } /** * 目标 Controller 的方法执行完并且返回结果之后,视图解析器渲染视图之前执行。 * @param request * @param response * @param handler * @param ex * @throws Exception */ @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { HandlerInterceptor.super.afterCompletion(request, response, handler, ex); } } /** * @Author:懒大王Smile * @Date: 2024/9/14 * @Time: 18:24 * @Description: 该拦截器只负责刷新token（redis共享session），不负责拦截 */ @Component public class RefreshTokenInterceptor implements HandlerInterceptor { @Resource StringRedisTemplate stringRedisTemplate; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) { //前端请求时带上authorization String token = request.getHeader(\u0026#34;authorization\u0026#34;); if (StringUtils.isBlank(token)) { //未登录，直接放行，由登录拦截器拦截 return true; } //从redis获取token String tokenKey = Common.LOGIN_TOKEN_KEY + token; Map\u0026lt;Object, Object\u0026gt; map = stringRedisTemplate.opsForHash().entries(tokenKey); if (map.isEmpty()) { //redis中存储的登录态已失效，放行，让登录拦截器拦截 return true; } LoginUserVO loginUserVO = BeanUtil.fillBeanWithMap(map, new LoginUserVO(), false); //将用户信息保存到ThreadLocal中 UserContext.saveUser(loginUserVO); //刷新redis的token有效期 stringRedisTemplate.expire(tokenKey, Common.LOGIN_TOKEN_TTL, TimeUnit.MINUTES); return true; } @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) { //// 移除用户,防止内存泄漏!!! UserContext.removeUser(); } } /** * @Author:懒大王Smile * @Date: 2024/9/18 * @Time: 16:48 * @Description: 拦截器配置类，注册拦截器 */ @Component @Slf4j public class InterceptorsConfig extends WebMvcConfigurationSupport { @Resource LoginInterceptor loginInterceptor; @Resource RefreshTokenInterceptor refreshTokenInterceptor; @Override protected void addInterceptors(InterceptorRegistry registry) { log.info(\u0026#34;注册自定义拦截器\u0026#34;); registry.addInterceptor(refreshTokenInterceptor) .addPathPatterns(\u0026#34;/**\u0026#34;) .excludePathPatterns( \u0026#34;/doc.html/**\u0026#34;, \u0026#34;/swagger-resources/**\u0026#34;, \u0026#34;/webjars/**\u0026#34;, \u0026#34;/ai/**\u0026#34; ).order(0); // order越小，优先级越高 registry.addInterceptor(loginInterceptor) .addPathPatterns(\u0026#34;/**\u0026#34;) .excludePathPatterns( \u0026#34;/webjars/**\u0026#34;, \u0026#34;/doc.html/**\u0026#34;, \u0026#34;/swagger-resources/**\u0026#34;, \u0026#34;/v3/api-docs/\u0026#34;, \u0026#34;/api/favicon.ico\u0026#34; ); } //没有该配置将无法使用swagger API测试 @Override protected void addResourceHandlers(ResourceHandlerRegistry registry) { registry.addResourceHandler(\u0026#34;/**\u0026#34;) .addResourceLocations(\u0026#34;classpath:/static/\u0026#34;) .addResourceLocations(\u0026#34;classpath:/META-INF/resources/\u0026#34;); } } 前端 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 requestConfig.ts //前端配置请求拦截器，实现在每个请求发出前为请求头添加token requestInterceptors: [ (config: any) =\u0026gt; { const token = getTokenIsExpiry(); if (token) { config.headers[\u0026#39;authorization\u0026#39;] = token; } return config; }, ] utils.ts // 存储token和登录态 export const setTokenWithExpiry = (loginUser: API.LoginUserVO) =\u0026gt; { const encryptLoginUser = encrypt(loginUser); localStorage.setItem(\u0026#39;loginUser\u0026#39;, encryptLoginUser); // 存储 loginUser 和过期时间 const expiryTime = new Date().getTime() + TokenTTL * 60 * 1000; // 计算过期时间,单位 min const item = { token: loginUser.token, expiry: expiryTime, }; const encryptToken = encrypt(item); localStorage.setItem(\u0026#39;authorization\u0026#39;, encryptToken); }; // 获取 token 并检查是否过期，如果过期就删除 export const getTokenIsExpiry = () =\u0026gt; { const encryptToken = localStorage.getItem(\u0026#39;authorization\u0026#39;); if (!encryptToken) { return null; // 如果没有 token，返回 null } const tokenObj = decrypt(encryptToken); const currentTime = new Date().getTime(); if (currentTime \u0026gt; tokenObj.expiry) { localStorage.removeItem(\u0026#39;authorization\u0026#39;); // 如果过期了，删除 loginUser localStorage.removeItem(\u0026#39;loginUser\u0026#39;); // 如果过期了，删除 token setTimeout(() =\u0026gt; { window.location.reload(); }, 400); history.replace(\u0026#39;/home\u0026#39;); message.info(\u0026#39;登陆凭证过期，请重新登录\u0026#39;); } return tokenObj.token; // 如果没有过期，返回 token }; 方案二 后端使用sa-token框架Sa-Token实现用户登录注销、鉴权等操作，可以方便的集成redis\nSa-token框架 如图是3343@qq.com账号连续登录三次，redis中生成的3个token及一个account-session，此时仅作登陆操作\n\u0026ldquo;authorization:login:session:3343@qq.com\u0026quot;内容如下：\n可以看到\u0026quot;terminalList\u0026quot;中记录了3次登录产生的详细的token信息\n{ \u0026quot;@class\u0026quot;: \u0026quot;cn.dev33.satoken.session.SaSession\u0026quot;, \u0026quot;id\u0026quot;: \u0026quot;authorization:login:session:3343@qq.com\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Account-Session\u0026quot;, \u0026quot;loginType\u0026quot;: \u0026quot;login\u0026quot;, \u0026quot;loginId\u0026quot;: \u0026quot;3343@qq.com\u0026quot;, \u0026quot;token\u0026quot;: null, \u0026quot;historyTerminalCount\u0026quot;: 3, \u0026quot;createTime\u0026quot;: 1751087763506, \u0026quot;dataMap\u0026quot;: { \u0026quot;@class\u0026quot;: \u0026quot;java.util.concurrent.ConcurrentHashMap\u0026quot; }, \u0026quot;terminalList\u0026quot;: [ \u0026quot;java.util.Vector\u0026quot;, [ { \u0026quot;@class\u0026quot;: \u0026quot;cn.dev33.satoken.session.SaTerminalInfo\u0026quot;, \u0026quot;index\u0026quot;: 1, \u0026quot;tokenValue\u0026quot;: \u0026quot;9d3e2b34-a5ad-4059-bdf4-4add0c370ca0\u0026quot;, \u0026quot;deviceType\u0026quot;: \u0026quot;DEF\u0026quot;, \u0026quot;deviceId\u0026quot;: null, \u0026quot;extraData\u0026quot;: null, \u0026quot;createTime\u0026quot;: 1751087763575 }, { \u0026quot;@class\u0026quot;: \u0026quot;cn.dev33.satoken.session.SaTerminalInfo\u0026quot;, \u0026quot;index\u0026quot;: 2, \u0026quot;tokenValue\u0026quot;: \u0026quot;4a740c99-071c-4512-af02-a9519e058b4d\u0026quot;, \u0026quot;deviceType\u0026quot;: \u0026quot;DEF\u0026quot;, \u0026quot;deviceId\u0026quot;: null, \u0026quot;extraData\u0026quot;: null, \u0026quot;createTime\u0026quot;: 1751087826615 }, { \u0026quot;@class\u0026quot;: \u0026quot;cn.dev33.satoken.session.SaTerminalInfo\u0026quot;, \u0026quot;index\u0026quot;: 3, \u0026quot;tokenValue\u0026quot;: \u0026quot;36d7a224-1f8e-4605-84d7-cb5ecf594018\u0026quot;, \u0026quot;deviceType\u0026quot;: \u0026quot;DEF\u0026quot;, \u0026quot;deviceId\u0026quot;: null, \u0026quot;extraData\u0026quot;: null, \u0026quot;createTime\u0026quot;: 1751087850414 } ] ] } \u0026ldquo;authorization:login:token:4a740c99-071c-4512-af02-a9519e058b4d\u0026quot;内容如下：\n然后调用StpUtil.getTokenSession()，此时就会生成一个token-session\n{ \u0026quot;@class\u0026quot;: \u0026quot;cn.dev33.satoken.session.SaSession\u0026quot;, \u0026quot;id\u0026quot;: \u0026quot;authorization:login:token-session:36d7a224-1f8e-4605-84d7-cb5ecf594018\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Token-Session\u0026quot;, \u0026quot;loginType\u0026quot;: \u0026quot;login\u0026quot;, \u0026quot;loginId\u0026quot;: null, \u0026quot;token\u0026quot;: \u0026quot;36d7a224-1f8e-4605-84d7-cb5ecf594018\u0026quot;, \u0026quot;historyTerminalCount\u0026quot;: 0, \u0026quot;createTime\u0026quot;: 1751088437477, \u0026quot;dataMap\u0026quot;: { \u0026quot;@class\u0026quot;: \u0026quot;java.util.concurrent.ConcurrentHashMap\u0026quot; }, \u0026quot;terminalList\u0026quot;: [ \u0026quot;java.util.Vector\u0026quot;, [ ] ] } 发现token-session和account-session结构相同，因为它们都出自同一个SaSession类\n可知在Sa-Token框架中，session分别三种，我这里只关注account和token的session，前面提到在使用同一个账号连续登陆3次时只生成了account-session，其中记录了三次的登录的token，那么这就可以实现了同一账号多端登录 ，每个端的token隔离，比如同时在PC和IOS端登录，如果token不隔离（token共享），当在其中一端注销登录时，另一端也会被迫注销登录，显然不合常理，而如果实现的token隔离，每个端都有不同的token，那么这就不会出现另一端被迫注销的情况。所以说account-session记录了同一账号多端登录的token信息，而token-session则记录了该账号在某一端的token信息，更为详细。\nsa-token设置有效期 在yml配置timeout，单位是s，同一账号先后多端登录，token过期后先删除token-session，待该账号下所有token全部过期后才删除account-session。\nsa-token自动续期 SaTokenConfig.java，在yml配置autoRenew即可开启自动续期，每次要续期时直接或间接调用getLoginId()即可。\n后端 仅需一个拦截器即可，不再需要方案一的两个拦截器。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @Slf4j @Component public class SaTokenInterceptor implements HandlerInterceptor { @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) { String requestURI = request.getRequestURI(); if (requestURI.contains(\u0026#34;/api/webjars\u0026#34;) || requestURI.contains(\u0026#34;/api/favicon.ico\u0026#34;) || requestURI.contains(\u0026#34;/api-docs\u0026#34;) || requestURI.contains(\u0026#34;/error\u0026#34;)) { return true; } //刷新token有效期（这一步已经判断了名为authorization的token是否是真实有效的，如果是伪造或过期的token则不会刷新token，报错） Long userId; try { userId = Long.valueOf(StpUtil.getLoginId().toString()); } catch (Exception e) { if(requestURI.contains(\u0026#34;/ai\u0026#34;)){ return true; } throw new RuntimeException(e); } //虽然每次可以从stpUtil.getLoginId()获取userId，但是这样要读redis，会对其造成压力，因此这里取出来放到userContext，用的时候从userContext取 UserContext.saveUser(userId); //角色校验 if(requestURI.contains(\u0026#34;/admin\u0026#34;)){ StpUtil.checkRole(UserRoleEnum.ADMIN.getRole()); } return true; } // 移除用户,防止内存泄漏!!! @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) { UserContext.removeUser(); } } 注册该拦截器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 @Slf4j @Configuration public class SaTokenConfigure implements WebMvcConfigurer { @Resource SaTokenInterceptor saTokenInterceptor; /** * 注册 Sa-Token 拦截器打开注解鉴权功能 */ @Override public void addInterceptors(InterceptorRegistry registry) { // 注册 Sa-Token 拦截器打开注解鉴权功能 log.info(\u0026#34;注册自定义拦截器\u0026#34;); registry.addInterceptor(saTokenInterceptor) .addPathPatterns(\u0026#34;/**\u0026#34;) .excludePathPatterns( \u0026#34;/user/login\u0026#34;, \u0026#34;/user/register\u0026#34;, \u0026#34;/user/getUserInfo/{uid}\u0026#34;, \u0026#34;/user/sendRegisterCode\u0026#34;, \u0026#34;/user/find/{userName}\u0026#34;, \u0026#34;/user/userInfoData\u0026#34;, \u0026#34;/passage/otherPassages/{uid}\u0026#34;, \u0026#34;/passage/topCollects\u0026#34;, \u0026#34;/passage/content/{uid}/{pid}\u0026#34;, \u0026#34;/passage/homePassageList\u0026#34;, \u0026#34;/passage/search\u0026#34;, \u0026#34;/passage/passageInfo/{pid}\u0026#34;, \u0026#34;/passage/topPassages\u0026#34;, \u0026#34;/comment/getCommentByCursor\u0026#34;, \u0026#34;/category/getCategories\u0026#34;, \u0026#34;/tag/getRandomTags\u0026#34;, \u0026#34;/doc.html/**\u0026#34; ); } /** * 注册 [Sa-Token 全局过滤器] */ @Bean public SaServletFilter getSaServletFilter() { return new SaServletFilter() // 指定 [拦截路由] 与 [放行路由] .addInclude(\u0026#34;/**\u0026#34;) // 认证函数: 每次请求执行 .setAuth(obj -\u0026gt; { SaManager.getLog().info(\u0026#34;----- 请求path={},authorization={}\u0026#34;, SaHolder.getRequest().getRequestPath(), StpUtil.getTokenValue()); // 权限校验 -- 不同模块认证不同权限 //\t这里你可以写和拦截器鉴权同样的代码，不同点在于： // 校验失败后不会进入全局异常组件，而是进入下面的 .setError 函数 // SaRouter.match(\u0026#34;/admin/**\u0026#34;, r -\u0026gt; StpUtil.checkPermission(\u0026#34;admin\u0026#34;)); }) // 异常处理函数：每次认证函数发生异常时执行此函数 .setError(e -\u0026gt; { log.warn(\u0026#34;---------- sa-token全局异常 \u0026#34;); return SaResult.error(e.getMessage()); }) // 前置函数：在每次认证函数之前执行（BeforeAuth 不受 includeList 与 excludeList 的限制，所有请求都会进入） .setBeforeAuth(r -\u0026gt; { // ---------- 设置一些安全响应头 ---------- SaHolder.getResponse() // 服务器名称 .setServer(\u0026#34;sa-server\u0026#34;) // 是否可以在iframe显示视图： DENY=不可以 | SAMEORIGIN=同域下可以 | ALLOW-FROM uri=指定域名下可以 .setHeader(\u0026#34;X-Frame-Options\u0026#34;, \u0026#34;SAMEORIGIN\u0026#34;) // 是否启用浏览器默认XSS防护： 0=禁用 | 1=启用 | 1; mode=block 启用, 并在检查到XSS攻击时，停止渲染页面 .setHeader(\u0026#34;X-XSS-Protection\u0026#34;, \u0026#34;1; mode=block\u0026#34;) // 禁用浏览器内容嗅探 .setHeader(\u0026#34;X-Content-Type-Options\u0026#34;, \u0026#34;nosniff\u0026#34;) ; }) ; } /** * 解决cors跨域 * @return */ @Bean public CorsFilter corsFilter() { //1. 添加 CORS配置信息 CorsConfiguration config = new CorsConfiguration(); //放行哪些原始域 //带上这个会报错 // config.addAllowedOrigin(\u0026#34;localhost:8000\u0026#34;); // When allowCredentials is true, allowedOrigins cannot contain the special value \u0026#34;*\u0026#34; since that cannot be set on the \u0026#34;Access-Control-Allow-Origin\u0026#34; response header. To allow credentials to a set of origins, list them explicitly or consider using \u0026#34;allowedOriginPatterns\u0026#34; instead. config.addAllowedOriginPattern(\u0026#34;*\u0026#34;); //是否发送 Cookie config.setAllowCredentials(true); //放行哪些请求方式 config.addAllowedMethod(\u0026#34;*\u0026#34;); //放行哪些原始请求头部信息 config.addAllowedHeader(\u0026#34;*\u0026#34;); //暴露哪些头部信息 //config.addExposedHeader(\u0026#34;*\u0026#34;); //2. 添加映射路径 UrlBasedCorsConfigurationSource corsConfigurationSource = new UrlBasedCorsConfigurationSource(); corsConfigurationSource.registerCorsConfiguration(\u0026#34;/**\u0026#34;, config); //3. 返回新的CorsFilter return new CorsFilter(corsConfigurationSource); } /** * 解决SaTokenContext 上下文尚未初始化的问题 * 参考: https://gitee.com/dromara/sa-token/issues/IC4XFE * @return */ @Bean public FilterRegistrationBean saTokenContextFilterForJakartaServlet() { FilterRegistrationBean bean = new FilterRegistrationBean\u0026lt;\u0026gt;(new SaTokenContextFilterForJakartaServlet()); // 配置 Filter 拦截的 URL 模式 bean.addUrlPatterns(\u0026#34;/*\u0026#34;); // 设置 Filter 的执行顺序,数值越小越先执行 bean.setOrder(Ordered.HIGHEST_PRECEDENCE); bean.setAsyncSupported(true); bean.setDispatcherTypes(EnumSet.of(DispatcherType.ASYNC, DispatcherType.REQUEST)); return bean; } } 前端 ","date":"2025-06-29T17:48:00Z","permalink":"/zh-cn/post/2025/06/%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%9A%84%E7%94%A8%E6%88%B7%E7%99%BB%E5%BD%95%E7%8E%A9%E6%B3%95sa-token%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8/","title":"前后端分离场景下的用户登录玩法\u0026Sa-token框架使用"},{"content":"分布式事务\n1.4 w字，25 张图让你彻底掌握分布式事务原理\n单数据源事务\n单数据源事务也可以叫做单机事务或本地事务。可利用数据库提供的事务机制ACID即可保证事务一致性。 在分布式场景下，一个系统有多个子系统构成， 每个子系统有自己的数据源，此时仅靠本地事务机制就无法保证全局的事务一致性，也就无法保证数据一致性。 分布式事务模型\n事务参与者：例如每个数据库就是一个事务参与者 事务协调者：访问多个数据源的服务程序，例如 shopping-service 就是事务协调者 资源管理器（Resource Manager, RM）：通常与事务参与者同义 事务管理器（Transaction Manager, TM）：通常与事务协调者同义 在分布式事务模型中，一个 TM 管理多个 RM，即一个服务程序访问多个数据源；TM 是一个全局事务管理器，协调多方本地事务的进度，使其共同提交或回滚，最终达成一种全局的 ACID 特性。 二将军问题\nB军队被两支A军队围在山谷，A1军队将军要通知另一支A2军队，约定时间攻击B军队，因此A1军队要派信使去传递消息，但是信使要经过山谷，可能被B军队俘虏。那么如果信使没有返回A1，如下两种情况：1.信使还没到A2，被俘虏，此时A2不知道进攻时间，A1也不知道A2有没有收到通知。2.信使已告知A2，但是返回途中被俘虏，A1还是不知道A2有没有收到通知。 类似的问题在计算机网络中普遍存在，例如发送者给接受者发送一个 HTTP 请求，或者 MySQL 客户端向 MySQL 服务器发送一条插入语句，然后超时了没有得到响应。请问服务器是写入成功了还是失败了？消息发送者不知道，因此往往要重复发送消息直到收到响应。例如电商系统中订单模块调用支付模块扣款的时候，如果网络故障导致二将军问题出现，扣款请求重复发送，产生的重复扣款结果显然是不能被接受的。因此要保证一次事务中的扣款请求无论被发送多少次，接收方有且只执行一次扣款动作，这种保证机制叫做接收方的幂等性。 分布式事务解决方案 2PC 2pc是解决分布式事务的最简单的模型，分为2个阶段：\n准备阶段：事务协调者向各个事务参与者发询问请求，通知即将执行全局事务，各自做好资源准备，即各自执行本地事务到待提交阶段。各个事务参与者准备好后响应ACK或no或协调者等待超时。 提交/回滚阶段：如果所有事务参与者响应ACK，则由事务协调者通知进行全局事务最终的提交阶段。如果有一个参与者no或协调者等待超时，则要回滚阶段。 要实现 2PC，所有的参与者都要实现三个接口：\nPrepare()：TM 调用该接口询问各个本地事务是否就绪\nCommit()：TM 调用该接口要求各个本地事务提交\nRollback()：TM 调用该接口要求各个本地事务回滚\n可以将这三个接口简单地（但不严谨地）理解成 XA 协议。\nXA 协议是 X/Open 提出的分布式事务处理标准。MySQL、Oracle、DB2 这些主流数据库都实现了 XA 协议，因此都能被用于实现 2PC 事务模型。 2PC存在问题\n性能差：准备阶段要等所有事务参与者响应才能进入提交回滚阶段，这期间参与者的相关资源会被锁住，影响各个参与者的本地事务并发度； 如果准备阶段完成，协调者挂了，那么所有参与者都收不到提交或回滚指令，导致所有参与者会一直阻塞直到协调者恢复，参与者没有超时机制，导致长时间资源锁定。 3PC 3PC 的出现是为了解决 2PC 的一些问题，相比于 2PC 它在参与者中也引入了超时机制 ，并且新增了一个阶段使得参与者可以利用这一个阶段统一各自的状态。3pc分为三个阶段：\n准备阶段：协调者只是询问参与者的自身状况。 预提交阶段：和2pc的准备阶段一样。 提交阶段：提交阶段和 2PC 的一样。 TCC TCC 就是一种解决多个微服务之间的分布式事务问题的方案。TCC 是 Try、Confirm、Cancel 三个词的缩写，其本质是一个应用层面上的 2PC，分为两个阶段：\n准备阶段：协调者调用所有的每个微服务提供的 try 接口，将整个全局事务涉及到的资源锁定住，若锁定成功 try 接口向协调者返回 yes。 提交阶段：若所有的服务的 try 接口在阶段一都返回 yes，则进入提交阶段，协调者调用所有服务的 confirm 接口，各个服务进行事务提交。如果有任何一个服务的 try 接口在阶段一返回 no 或者超时，则协调者调用所有服务的 cancel 接口。 TCC有两个问题：\n既然 TCC 是一种服务层面上的 2PC，它是如何解决 2PC 无法应对宕机 问题的缺陷的呢？答案是不断重试。由于 try 操作锁住了全局事务涉及的所有资源，保证了业务操作的所有前置条件得到满足，因此无论是 confirm 阶段失败还是 cancel 阶段失败都能通过不断重试直至 confirm 或 cancel 成功（所谓成功就是所有的服务都对 confirm 或者 cancel 返回了 ACK）。 在不断重试 confirm 和 cancel 的过程中（二将军问题）有可能重复进行了 confirm 或 cancel，因此还要再保证 confirm 和 cancel 操作具有幂等性，也就是整个全局事务中，每个参与者只进行一次 confirm 或者 cancel。实现 confirm 和 cancel 操作的幂等性，有很多解决方案，例如每个参与者可以维护一个去重表（可以利用数据库表实现也可以使用内存型 KV 组件实现），记录每个全局事务（以全局事务标记 XID 区分）是否进行过 confirm 或 cancel 操作，若已经进行过，则不再重复执行。 事务状态表 类似 TCC 的事务解决方案，借助事务状态表来实现。假设要在一个分布式事务中实现调用 repo-service 扣减库存、调用 order-service 生成订单两个过程。在这种方案中，协调者 shopping-service 维护一张如下的事务状态表，初始状态为 1，每成功调用一个服务则更新一次状态，最后所有的服务调用成功，状态更新到 3\n基于消息中间件的解决方案 无论是 2PC \u0026amp; 3PC 还是 TCC、事务状态表，基本都遵守 XA 协议的思想，即这些方案本质上都是事务协调者协调各个事务参与者的本地事务的进度，使所有本地事务共同提交或回滚，最终达成一种全局的 ACID 特性。在协调的过程中，协调者需要收集各个本地事务的当前状态，并根据这些状态发出下一阶段的操作指令。\n但是这些全局事务方案由于操作繁琐、时间跨度大，或者在全局事务期间会排他地锁住相关资源，使得整个分布式系统的全局事务的并发度不会太高。这很难满足电商等高并发场景对事务吞吐量的要求。\n消息队列 RocketMQ 就支持消息事务，RocketMQ 的发送方会提供一个反查事务状态接口，如果一段时间内半消息没有收到任何操作请求，那么 Broker 会通过反查接口得知发送方事务是否执行成功，然后执行 Commit 或者 RollBack 命令。\nSeata --- ","date":"2025-03-14T12:00:00Z","permalink":"/zh-cn/post/2025/03/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/","title":"分布式事务"},{"content":" 往期推荐\nsynchronized如何实现可重入，和Lock区别-CSDN博客\nMysql索引失效的几种场景、回表、索引覆盖、索引下推-CSDN博客\n为何String不可变，String的运算符重载-CSDN博客\nString.intern()-CSDN博客\nStringBuffer和StringBuilder 两者都是继承自AbstractStringBuilder，在AbstractStringBuilder中使用了byte\n\\[ \\]实现（jdk8之前由char\n\\[ \\]实现），这里的byte[]没有使用final修饰，所以是可变的，StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，线程安全。StringBuilder没加锁，不安全。\nStringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用，相比StringBuilder性能不高。\n字符串拼接 Java 语言本身并不支持运算符重载，\u0026quot;+\u0026ldquo;和\u0026rdquo;+=\u0026ldquo;是专门为 String 类重载过的运算符，也是 Java 中仅有的两个重载过的运算符。\n在\u0026quot;abc\u0026rdquo;+\u0026ldquo;abc\u0026quot;时，实际是通过StringBuilder 调用 append() 方法实现的，拼接完成之后调用 toString() 得到一个 String 对象 ，但是在循环内使用\u0026rdquo;+\u0026ldquo;进行字符串的拼接的话，存在比较明显的缺陷：编译器不会创建单个 StringBuilder 以复用，而是每循环一次就创建过一个 StringBuilder 对象 。而如果直接使用 StringBuilder 对象进行字符串拼接的话，就不会存在这个问题了。\n--- ","date":"2025-02-24T17:52:47Z","permalink":"/zh-cn/post/2025/02/stringbuffer%E5%92%8Cstringbuilder/","title":"StringBuffer和StringBuilder"},{"content":" spring bean作用域：\nhttps://blog.csdn.net/qq_73181349/article/details/144837669\n如何记忆Spring Bean的生命周期 - 草捏子\n大致分为四个阶段：实例化 \u0026mdash;\u0026gt; 属性赋值 \u0026mdash;\u0026gt; 初始化 \u0026mdash;\u0026gt; 销毁。\n创建 Bean 的实例 ：Bean 容器首先会找到配置文件中的 Bean 定义，然后使用 Java 反射来创建 Bean 的实例。 Bean 属性赋值/填充 ：为 Bean 设置相关属性和依赖，例如@Autowired 等注解注入的对象、@Value 注入的值、setter方法或构造函数注入依赖和值、@Resource注入的各种资源。 Bean 初始化 ： 检查 Aware 相关接口， 如果实现了某个aware接口就调用相应的方法。 比如实现了 BeanFactoryAware 接口，就调用 setBeanFactory()方法，传入 BeanFactory对象的实例。 BeanPostProcessor 前置处理，执行postProcessBeforeInitialization() 方法 如果 Bean 实现了InitializingBean接口，执行afterPropertiesSet()方法。 如果 Bean 在配置文件中的定义包含 init-method 属性，执行指定的方法。 BeanPostProcessor 后置处理，执行postProcessAfterInitialization() 方法。 销毁 Bean ：销毁并不是立马把 Bean 给销毁掉，而是把 Bean 的销毁方法先记录下来，将来需要销毁 Bean 或者销毁容器的时候，就调用这些方法去释放 Bean 所持有的资源。 如果 Bean 实现了 DisposableBean 接口，执行 destroy() 方法。 如果 Bean 在配置文件中的定义包含 destroy-method 属性，执行指定的 Bean 销毁方法。或者，也可以直接通过@PreDestroy 注解标记 Bean 销毁之前执行的方法。 doCreateBean\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // AbstractAutowireCapableBeanFactory.java protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final @Nullable Object[] args) throws BeanCreationException { // 1. 实例化 BeanWrapper instanceWrapper = null; if (instanceWrapper == null) { instanceWrapper = createBeanInstance(beanName, mbd, args); } Object exposedObject = bean; try { // 2. 属性赋值 populateBean(beanName, mbd, instanceWrapper); // 3. 初始化 exposedObject = initializeBean(beanName, exposedObject, mbd); } // 4. 销毁-注册回调接口 try { registerDisposableBeanIfNecessary(beanName, bean, mbd); } return exposedObject; } initializeBean\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 // AbstractAutowireCapableBeanFactory.java protected Object initializeBean(final String beanName, final Object bean, @Nullable RootBeanDefinition mbd) { // 3. 检查 Aware 相关接口并设置相关依赖 if (System.getSecurityManager() != null) { AccessController.doPrivileged((PrivilegedAction\u0026lt;Object\u0026gt;) () -\u0026gt; { invokeAwareMethods(beanName, bean); return null; }, getAccessControlContext()); } else { invokeAwareMethods(beanName, bean); } // 4. BeanPostProcessor 前置处理 Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) { wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); } // 5. 若实现 InitializingBean 接口，调用 afterPropertiesSet() 方法 // 6. 若配置自定义的 init-method方法，则执行 try { invokeInitMethods(beanName, wrappedBean, mbd); } catch (Throwable ex) { throw new BeanCreationException( (mbd != null ? mbd.getResourceDescription() : null), beanName, \u0026#34;Invocation of init method failed\u0026#34;, ex); } // 7. BeanPostProceesor 后置处理 if (mbd == null || !mbd.isSynthetic()) { wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); } return wrappedBean; } --- ","date":"2025-02-19T08:11:22Z","permalink":"/zh-cn/post/2025/02/spring%E7%9A%84bean%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BF%87%E7%A8%8B%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/","title":"Spring的bean初始化过程、生命周期"},{"content":"jdk7的ConcurrentHashMap 在JDK1.7中用的是Segment数组+链表实现的。Segment是一种可重入锁(ReentrantLock)，链表则用于存储键值对数据。一个ConcurrentHashMap里包含一个Segment数组，一个Segment里包含一个链表。\nJDK1.7 的ConcurrentHashMap给每一段数据配一把锁，当一个线程访问其中该段数据的时候，会锁住该链表头节点，那么其他段的数据也能被其他线程访问，能够实现真正的并发访问。\nSegment 默认个数是 16，一旦初始化就不能改变。每个链表默认长度也是16。\nrehash ConcurrentHashMap 的扩容只会扩容到原来的两倍。老数组里的数据移动到新的数组时，位置要么不变，要么变为 index+ oldSize\njdk8的ConcurrentHashMap JDK1.7的ConcurrentHashMap底层实现是Segment数组+链表的形式，在数据量大时要遍历链表，效率低。而JDK1.8则使用了Node数组+链表/红黑树的形式。\nNode 是类似于一个 HashEntry 的结构。它的冲突再达到一定大小时会转化成红黑树，在冲突小于一定数量时又退回链表。\nConcurrentHashMap的实现(和HashMap结构一样)\nhttps://blog.csdn.net/qq_73181349/article/details/144943507\n具体实现结构如下：\nJDK1.8 ConcurrentHashMap JDK1.8 ConcurrentHashMap主要通过volatile+CAS或者\nsynchronized来实现的线程安全的。put元素时先根据key的hashcode判断对应Node容器是否为空：\n如果为空则使用volatile+CAS来初始化 如果容器不为空，则根据存储的元素计算该位置是否为空。 如果根据存储的元素计算结果为空 ，则利用CAS设置该节点； 如果根据存储的元素计算结果不为空 ，则使用synchronized ，然后，遍历桶中的数据，并替换或新增节点 到桶中，最后再判断是否需要转为红黑树，这样就能保证并发访问时的线程安全了。 也就是访问Node数组时用CAS+volatile，访问HashEntry时用CAS或synchronized。\n为什么ConcurrentHashMap的key和value不能为null ConcurrentHashMap 的 key 和 value 不能为 null 主要是为了避免二义性。null 是一个特殊的值，表示没有对象或没有引用 。如果用 null 作为键，就无法通过containsKey(key)区分这个键是否存在于 ConcurrentHashMap 中，还是根本没有这个键。同样，如果用 null 作为值，就无法区分这个值是否是真正存储在 ConcurrentHashMap 中的，还是因为找不到对应的键而返回的。\n与此形成对比的是，HashMap 可以存储 null 的 key 和 value，但 null 作为键只能有一个，null 作为值可以有多个。如果传入 null 作为参数，就会返回 hash 值为 0 的位置的值 。单线程环境下，不存在一个线程操作该 HashMap 时，其他的线程将该 HashMap 修改的情况，所以可以通过 contains(key)来做判断是否存在这个键值对，也就不存在二义性问题。\nConcurrentHashMap如何保证复合操作的原子性 复合操作是指由多个基本操作(如put、get、remove、containsKey等)组成 的操作，例如先判断某个键是否存在containsKey(key)，然后根据结果进行插入或更新put(key, value)。这种操作在执行过程中可能会被其他线程打断，导致结果不符合预期。\nConcurrentHashMap 提供了一些原子性的复合操作，如 putIfAbsent、compute、computeIfAbsent 、computeIfPresent、merge等。这些方法都可以接受一个函数作为参数，根据给定的 key 和 value 来计算一个新的 value，并且将其更新到 map 中 。\n--- ","date":"2025-02-18T08:54:49Z","permalink":"/zh-cn/post/2025/02/concurrenthashmap/","title":"ConcurrentHashMap"},{"content":"\n官方教程：https://github.com/anuraghazra/github-readme-stats/blob/master/docs/readme_cn.md\n根据官方教程，有两种玩法，第一种最简单的，不用部署项目，步骤如下：\n1.建仓库\n创建一个仓库，仓库名和自己的github账户名相同，仓库里添加一个readme.md文件，这个文件就是用来美化个人主页的，效果如上图所示。\n2.编辑readme.md文件\nreadme.md文件添加如下内容，把username=xxxxx换成github的账户名即可，然后预览md文件，应该就有效果了，具体主题等配置项可以看官方教程。\n1 ​​​​​​​![Anurag\u0026#39;s GitHub stats](https://github-readme-stats.vercel.app/api?username=xxxxx\u0026amp;show_icons=true) readme.md中还可以使用\u0026lt;img\u0026gt;、\u0026lt;p\u0026gt;等基础标签，参考：​​​​​​​​​​ ","date":"2025-02-17T10:02:16Z","permalink":"/zh-cn/post/2025/02/%E7%BE%8E%E5%8C%96%E4%B8%AA%E4%BA%BAgithub%E4%B8%BB%E9%A1%B5%E9%83%A8%E7%BD%B2github-readme-stats/","title":"美化个人github主页，部署github-readme-stats"},{"content":" 往期推荐\n符号引用和直接引用、强引用、软引用、弱引用、虚引用-CSDN博客\n已老实！再学消息队列、死信队列-CSDN博客\nsynchronized如何实现可重入，和Lock区别-CSDN博客\nMysql索引失效的几种场景、回表、索引覆盖、索引下推-CSDN博客\n1. 未启用Spring事务管理功能或bean没有被spring管理\n2. @Transactional修饰的方法非public或被final、static修饰\n3. 同类的方法A直接调用同类的事务方法B\nspring事务是通过Spring AOP实现的， 对需要spring管理事务的bean生成了代理对象，然后通过代理对象拦截了目标方法的执行，在方法前后 添加了事务的功能，所以必须通过代理对象调用 目标方法的时候，事务才会起效。如果方法A直接调用方法B则是this调用，即该类的类对象，就不是代理对象。可以通过Service 类中注入自己，或者通过AopContext.currentProxy()获取代理对象来解决。\n4. 抛出的异常类型错误\n在业务方法进行异常抛出，spring会自动对事务进行回滚，那么问题来了，抛出哪些异常spring会回滚事务呢？默认情况下，spring遇到RuntimeException和Error的事务才会回滚。因为spring认为RuntimeException和Error是不可预期的错误，而受检异常是可预期的错误，可以通过业务逻辑即可解决。\n当然也可以自定义回滚异常类型@Transactional(rollbackFor = {异常类型列表})\n5. 异常被捕获处理\nspring感知到指定异常被抛出才会进行回滚，如果在方法内部捕获处理掉异常，事务就不会回滚\n1 2 3 4 5 6 7 8 9 @Transactional public void m1(){ 事务操作1 try{ 事务操作2，内部抛出了异常 }catch(Exception e){ log.error(....) } } 正确做法是捕获处理掉异常后抛出来，如下：\n1 2 3 4 5 6 7 8 9 10 @Transactional public void m1(){ 事务操作1 try{ 事务操作2，内部抛出了异常 }catch(Exception e){ log.error(....) throw e } } 6. 事务操作和@Transactional方法不在同一线程\n1 2 3 4 5 6 @Transactional public void m1() { new Thread() { 一系列事务操作 }.start(); } 7. 事务传播行为设置不对\nspring默认事务传播行为默认是**required，事务方法A内部调用事务方法B，**如果方法A存在事务则方法B加入方法A的事务，否则创建新事务。如果非事务方法A调用事务方法B，事务传播级别为NOT_SUPPORT，因为方法A当前不存在事务，则方法B虽然有@Transactional注解，但仍然以非事务去执行。\n--- ","date":"2025-02-16T12:38:15Z","permalink":"/zh-cn/post/2025/02/spring%E4%BA%8B%E5%8A%A1%E5%A4%B1%E6%95%88%E7%9A%84%E5%87%A0%E7%A7%8D%E5%9C%BA%E6%99%AF/","title":"Spring事务失效的几种场景"},{"content":" 往期推荐\nMySQL三大日志_mysql 大事务 回滚 记录binlog吗-CSDN博客\n符号引用和直接引用、强引用、软引用、弱引用、虚引用-CSDN博客\n已老实！再学消息队列、死信队列-CSDN博客\nsynchronized如何实现可重入，和Lock区别-CSDN博客\n如何设计一个能根据任务优先级来执行的线程池-CSDN博客\n聚簇索引和二级索引 innodb使用b+树作为索引数据结构。在创建表时，InnoDB 默认会创建一个主键索引（primary key），也就是聚簇索引，而其它索引都属于二级索引。\n如果没有指明主键索引，就自动在后台创建 隐藏的 6 字节 row_id 列 作为主键索引。\n值得一提的是，InnoDB和MyISAM都支持B+树索引，但是它们数据的存储结构实现方式不同。InnoDB存储擎的B+树索引的叶子节点保存数据本身（图1），MylSAM存储引擎的B+树索引的叶子节点保存数据的物理地址（图2）；\nInnoDB存储引擎根据索引类型不同，分为聚簇索引（图1）和二级索引。区别在于，聚簇索引的叶子节点存放的是实际数据，所有完整的用户数据都存放在聚簇索引的叶子节点，而二级索引的叶子节点存放的是主键值，而不是实际数据 。如果将 name 字段设置为普通索引，那么这个二级索引长下图这样：\n回表和索引覆盖 如果使用主键索引作为条件查询 ，查询聚簇索引的叶子节点数据（图1），那么就直接在叶子节点读取到要查询的数据，比如select * from user where id=1 (id是主键索引) 如果使用二级索引字段作为条件查询 ，查询聚簇索引 的叶子节点数据，那么需要检索两颗B+树：\n先在二级索引的B+树找到对应的叶子节点，获取主键值（图3），然后用获取的主键值，在聚簇索引中的B+树检索到对应的叶子节点（图1），然后获取要查询的数据。这个过程叫做回表，如select * from user where name=\u0026ldquo;林某\u0026rdquo;（name是二级索引） 如果使用二级索引字段作为条件查询 ，查询二级索引的叶子节点数据 （图3），那么只需在二级索引的 B+ 树找到对应的叶子节点，然后读取要查询的数据，不需要用到主键索引，这个过程叫做覆盖索引。如select id from user where name=\u0026ldquo;林某\u0026rdquo;（name是二级索引，id正好存在于二级索引中） 索引失效场景 3.1 like %xx或like %xx% 因为索引 B+ 树是按照「索引值」有序排列存储的，只能根据前缀进行比较。 对索引使用左或左右模糊匹配，此时会走全表扫描\n3.2 对索引使用函数 select * from user where length(name)=3（name是二级索引），因为索引保存的是索引字段原始值，而不是经过函数计算后的值，自然就没办法走索引了而是全表扫描。\n不过，从MySQL8.0开始，索引特性增加了函数索引，可以针对函数计算后的值建立一个索引，也就是说该索引的值是函数计算后的值，所以就可以通过扫描索引来查询数据。\n3.3 对索引表达式计算 select * from from where id +1=10会走全表扫描，因为索引保存的是索引字段的原始值，而不是 id + 1 表达式计算后的值，而select * from from where id = 10 -1则会走索引查询。\n3.4 对索引隐式类型转换 如果索引字段是字符串类型，但是在条件查询中，输入的参数是整型的话就会走全表扫描，而如果反过来，索引字段是整型，查询参数是字符串，此时会走索引，**因为mysql在字符串和整型比较时会自动把字符串变成数字，**所以字符串类型的索引，在使用整型参数查询时，还得把字符串索引变成整型才行，也就相当于调用了函数。\n3.5 联合索引不满足最左匹配 对主键字段建立的索引叫做聚簇索引，对普通字段建立的索引叫做二级索引。那么**多个普通字段组合在一起创建的索引就叫做联合索引（组合索引），**在使用联合索引时要遵循最左匹配，比如创建联合索引（a,b,c），查询时where b=1；where c=3；where b=2 and c=3；这三种情况都会使联合索引失效。\n有一个比较特殊的查询条件：where a \u0026gt; 1 and c = 3 ，这属于索引截断 ，不同版本处理方式也不一样。MySQL5.5的话，前面a会走索引，在联合索引找到主键值后，开始回表，到主键索引读取数据行交给Server层，在Server层再比对c字段的值。从MySQL5.6之后，有一个索引下推 ，即在存储引擎层进行索引遍历时，对索引中包含的字段先做判断（a和c都在索引中），直接过滤掉不满足条件的记录，再返还给Server层，从而减少回表次数。\n当然回表只发生在用二级索引查询聚簇索引的数据，如果用主键索引查聚簇索引的数据就不存在回表了。 索引下推原理\n截断的字段不会在Server层进行条件判断，而是会被下推到「存储引擎层」进行条件判断（因为c字段的值是在(a,b,c)联合索引里的)，然后过滤出符合条件的数据后再返回给Server层。由于在引擎层就过滤掉大量的数据，无需再回表读取数据来进行判断，减少回表次数，从而提升了性能。\n没索引下推：存储引擎先定位到第一条a\u0026gt;1的数据，然后拿着其主键去回表，读取出数据给server层，然后server层判断是否满足c=3，来决定是否给客户端，然后存储引擎重复上面操作，反复回表。\n有索引下推: 就直接在存储引擎层过滤，减少回表操作。\n联合索引的匹配遵循 最左前缀原则 ，且 从最左列开始按顺序匹配 。当遇到第一个范围查询时，后续列的索引将不再生效；而等值查询则允许后续列继续匹配索引，直到遇到范围查询为止。\n3.6 where中使用or 在 WHERE 子句中，如果 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效。因为 OR 的含义就是两个只要满足一个即可，因此只有一个条件列是索引列是没有意义的，只要有条件列不是索引列，就会进行全表扫描\n3.7 两个索引列做比较 MySQL的索引（如B+Tree索引）是按列值单独排序的。每个索引独立存储某列的值及其行位置（ROWID）。当比较两列时：\n若使用 column1 的索引，只能快速定位到 column1 的特定值，但无法直接关联到 column2 的值。同理，column2 的索引也无法关联到 column1 的值。优化器无法通过索引直接找到满足 column1 = column2 的行，只能通过全表扫描逐行比较。\n3.8 不等于比较 3.9 is not null 3.10 not in和not exists 查询条件使用not in时，如果是主键索引则走索引，如果是普通索引，则索引失效。\n3.11 order by 对索引order by导致全表排序\n","date":"2025-02-15T10:33:28Z","permalink":"/zh-cn/post/2025/02/mysql%E7%B4%A2%E5%BC%95%E5%A4%B1%E6%95%88%E7%9A%84%E5%87%A0%E7%A7%8D%E5%9C%BA%E6%99%AF%E5%9B%9E%E8%A1%A8%E7%B4%A2%E5%BC%95%E8%A6%86%E7%9B%96%E7%B4%A2%E5%BC%95%E4%B8%8B%E6%8E%A8/","title":"Mysql索引失效的几种场景、回表、索引覆盖、索引下推"},{"content":" 往期推荐\n字符串常量池-CSDN博客\nAQS\u0026mdash;抽象队列同步器、CLH锁队列-CSDN博客\n符号引用和直接引用、强引用、软引用、弱引用、虚引用-CSDN博客\n已老实！再学消息队列、死信队列-CSDN博客\n可重入 synchronized底层是利用计算机系统mutex Lock实现的。每一个可重入锁都会关联一个线程ID和一个锁状态status。\n当一个线程请求方法时，会检查锁状态。\n如果锁状态是0，代表该锁没有被占用，使用CAS操作获取锁，将线程ID替换成自己的线程D。\n如果锁状态不是0，代表有线程在访问该方法。此时，如果线程D是自己的线程D,如果是可重入锁，会将status自增1，然后获取到该锁，进而执行相应的方法；如果是非重入锁，就会进入阻塞队列等待。\n在释放锁时，\n如果是可重入锁的，每一次退出方法，就会将status减1，直至status的值为0，最后释放该锁。\n如果非可重入锁的，线程退出方法，直接就会释放该锁。\n和lock的区别 Synchronized内置的Java关键字，Lock是一个Java接口 Synchronized无法判断获取锁的状态，Lock可以判断是否获取到了锁 Synchronized会自动释放锁，Lock必须要手动释放锁！如果不释放锁，死锁 Synchronized线程1（获得锁，阻塞）、线程2（等待，傻傻的等）；Lock锁就不一定会等待下去； Synchronized可重入锁，不可以中断的，非公平；Lock，可重入锁，可以判断锁，非公平（可以自己设置）； Synchronized适合锁少量的代码同步问题，Lock适合锁大量的同步代码！ --- ","date":"2025-02-15T10:30:25Z","permalink":"/zh-cn/post/2025/02/synchronized%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%8F%AF%E9%87%8D%E5%85%A5%E5%92%8Clock%E5%8C%BA%E5%88%AB/","title":"synchronized如何实现可重入，和Lock区别"},{"content":" 往期推荐\nJava内存模型（Memory Model）-CSDN博客\nArrayList、LinkedList、HashMap、HashTable、HashSet、TreeSet-CSDN博客\nAQS\u0026mdash;抽象队列同步器-CSDN博客\nKafka入门到入土\u0026mdash;\u0026mdash;万字详解，图文并茂_图解kafka-CSDN博客\nKafka系列第三篇！10 分钟学会如何在 Spring Boot 程序中使用 Kafka 作为消息队列?\n消息队列的使用场景是什么样的？\n目录{#main-toc}\n消息队列作用{#%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%BD%9C%E7%94%A8-toc}\nJMS和AMQP协议{#JMS%E5%92%8CAMQP%E5%8D%8F%E8%AE%AE-toc}\nJMS{#JMS-toc}\n五种消息类型{#%E4%BA%94%E7%A7%8D%E6%B6%88%E6%81%AF%E7%B1%BB%E5%9E%8B-toc}\n两种消息模型{#%E4%B8%A4%E7%A7%8D%E6%B6%88%E6%81%AF%E6%A8%A1%E5%9E%8B-toc}\nAMQP{#AMQP-toc}\n消息类型{#%E6%B6%88%E6%81%AF%E7%B1%BB%E5%9E%8B-toc}\n五种消息模型{#%E4%BA%94%E7%A7%8D%E6%B6%88%E6%81%AF%E6%A8%A1%E5%9E%8B-toc}\n几种消息队列{#%E5%87%A0%E7%A7%8D%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97-toc}\nkafka{#kafka-toc}\nkafka的优势{#kafka%E7%9A%84%E4%BC%98%E5%8A%BF-toc}\nkafka 为什么性能比 RocketMQ 好{#kafka%20%E4%B8%BA%E4%BB%80%E4%B9%88%E6%80%A7%E8%83%BD%E6%AF%94%20RocketMQ%20%E5%A5%BD-toc}\nkafka如何保证消息有序{#kafka%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E6%9C%89%E5%BA%8F-toc}\nkafka如何保证消息不丢失{#kafka%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E4%B8%8D%E4%B8%A2%E5%A4%B1-toc}\n生产者丢失消息{#%E7%94%9F%E4%BA%A7%E8%80%85%E4%B8%A2%E5%A4%B1%E6%B6%88%E6%81%AF-toc}\n消费者丢失消息{#%E6%B6%88%E8%B4%B9%E8%80%85%E4%B8%A2%E5%A4%B1%E6%B6%88%E6%81%AF%C2%A0-toc}\nkafka丢消息{#kafka%E4%B8%A2%E6%B6%88%E6%81%AF-toc}\nkafka如何保证消息不重复消费{#kafka%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E4%B8%8D%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9-toc}\nRocketMQ{#RocketMQ-toc}\nRocketMQ架构{#RocketMQ%E6%9E%B6%E6%9E%84-toc}\nRabbitMQ{#RabbitMQ-toc}\nexchange类型{#exchange%E7%B1%BB%E5%9E%8B%C2%A0-toc}\n死信队列{#%E6%AD%BB%E4%BF%A1%E9%98%9F%E5%88%97%C2%A0-toc}\n消息队列作用 {#%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%BD%9C%E7%94%A8} 老生常谈，削峰限流、异步解耦、分布式事务、顺序保证、延时定时、数据流处理比如大数据、即时通讯（物联网）\n消息队列（基础篇）-4 如何利用事务消息实现分布式事务？_事务消息可以实现分布式事务-CSDN博客\n当然引入组件带来的副作用往往是数据一致性、系统复杂性、系统可用性稳定性，毕竟越复杂的东西越容易出问题。\nJMS和AMQP协议 {#JMS%E5%92%8CAMQP%E5%8D%8F%E8%AE%AE} JMS JAVA Message Service，一个java消息服务的规范，类似jdbc，有点对点、发布订阅两种模型：\n五种消息类型 {#%E4%BA%94%E7%A7%8D%E6%B6%88%E6%81%AF%E7%B1%BB%E5%9E%8B} StreamMessage：Java 原始值的数据流 MapMessage：一套名称-值对 TextMessage：一个字符串对象 ObjectMessage：一个序列化的 Java 对象 BytesMessage：一个字节的数据流 两种消息模型 {#%E4%B8%A4%E7%A7%8D%E6%B6%88%E6%81%AF%E6%A8%A1%E5%9E%8B} 点对点\n一个消息只有一个消费者，未被消费的消息在queue中保留直到被消费或超时。消费者在成功接收消息之后需向队列应答成功，以便消息队列删除当前接收的消息 发布订阅\n生产者把消息广播到一个topic，该topic可以有多个消费者消费。\n在kafka中，一个topic可以有多个分区partition，单个分区的消息是有序的，而全局的topic的多个分区的消息是无序的。这就是为什么kafka一条消息只能被同一个消费者组里面的一个消费者消费，这样就某种程度上保证了消息的不重复消费和乱序消费。 AMQP Advanced Message Queuing Protocol，一个提供统一消息服务的应用层标准 高级消息队列协议 （二进制应用层协议），是应用层协议的一个开放标准，为面向消息的中间件设计，兼容 JMS。RabbitMQ 就是基于 AMQP 协议实现的。\n消息类型 {#%E6%B6%88%E6%81%AF%E7%B1%BB%E5%9E%8B} 二进制字节数组\n五种消息模型 {#%E4%BA%94%E7%A7%8D%E6%B6%88%E6%81%AF%E6%A8%A1%E5%9E%8B} ①direct exchange；\n②fanout exchange；\n③topic change；\n④headers exchange；\n⑤system exchange;\n本质来讲，后四种和 JMS 的 pub/sub 模型没有太大差别，仅是在路由机制上做了更详细的划分；\n几种消息队列 {#%E5%87%A0%E7%A7%8D%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97} 消息中间件：Kafka、RabbitMQ、RocketMQ、ActiveMQ 四个分布式消息队列 综合对比_kafuka tcp 与 mq 队列是否类似-CSDN博客\nkafka 具体看这个：Kafka入门到入土\u0026mdash;\u0026mdash;万字详解，图文并茂_图解kafka-CSDN博客\nkafka的优势 {#kafka%E7%9A%84%E4%BC%98%E5%8A%BF} 极致的性能：\n基于 Scala 和 Java 语言开发，设计中大量使用了批量处理和异步的思想，最高可以每秒处理千万级别的消息。 生态系统兼容性：\nKafka 与 周边生态系统的兼容性是最好的没有之一，尤其在大数据(数据吞吐量大)和流计算领域。 kafka 为什么性能比 RocketMQ 好 {#kafka%20%E4%B8%BA%E4%BB%80%E4%B9%88%E6%80%A7%E8%83%BD%E6%AF%94%20RocketMQ%20%E5%A5%BD} 面试官：RocketMQ 和 Kafka 有什么区别？\n这里性能主要指吞吐量，kafka使用了sendfile零拷贝，RocketMQ 使用的是 mmap 零拷贝技术，具体可以看这个\n用户态和内核态、进程、协程及线程几种状态、DMA、零拷贝_进程和线程 用户态和内核态-CSDN博客\n为什么RocketMQ不使用sendfile呢？\n1 2 3 4 5 6 ssize_t sendfile(int out_fd, int in_fd, off_t* offset, size_t count); // num = sendfile(xxx); void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset); // buf = mmap(xxx) sendfile返回的是发送成功了几个字节数 ，具体发了什么内容，应用层根本不知道 。mmap 返回的是数据的具体内容，应用层能获取到消息内容并进行一些逻辑处理。而 RocketMQ 的一些功能，却需要了解具体这个消息内容，方便二次投递等，比如将消费失败的消息重新投递到死信队列中，如果 RocketMQ 使用 sendfile，那根本没机会获取到消息内容长什么样子，也就没办法实现一些好用的功能了。\n一句话总结就是：和 Kafka 相比，RocketMQ 在架构上做了减法，在功能上做了加法\u0026quot;\nkafka如何保证消息有序 {#kafka%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E6%9C%89%E5%BA%8F} kafka的消息存储在topic的Partition中，整体来看topic的消息是无序的，但是单个Partition的消息是有序的，每次添加消息到 Partition的时候都会采用尾加法，并为其分配一个特定的offset。因此为保证 Kafka 中消息消费的顺序，有了下面两种方法：\n1 个 Topic 只对应一个 Partition。 （推荐）发送消息的时候指定 key/Partition。 kafka如何保证消息不丢失 {#kafka%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E4%B8%8D%E4%B8%A2%E5%A4%B1} 生产者丢失消息 {#%E7%94%9F%E4%BA%A7%E8%80%85%E4%B8%A2%E5%A4%B1%E6%B6%88%E6%81%AF} 生产者(Producer) 调用send方法发送消息之后，消息可能因为网络问题并没有发送过去。\n所以，我们不能默认在调用send方法发送消息之后消息发送成功了。为了确定消息是发送成功，我们要判断消息发送的结果。但是要注意的是 Kafka 生产者(Producer) 使用 send 方法发送消息实际上是异步的操作，我们可以通过 get()方法获取调用结果，但是这样也让它变为了同步操作，示例代码如下：\n1 2 3 4 5 SendResult\u0026lt;String, Object\u0026gt; sendResult = kafkaTemplate.send(topic, o).get(); if (sendResult.getRecordMetadata() != null) { logger.info(\u0026#34;生产者成功发送消息到\u0026#34; + sendResult.getProducerRecord().topic() + \u0026#34;-\u0026gt; \u0026#34; + sendRe sult.getProducerRecord().value().toString()); } 但是一般不推荐这么做！可以采用为其添加回调函数的形式，示例代码如下：\n1 2 3 ListenableFuture\u0026lt;SendResult\u0026lt;String, Object\u0026gt;\u0026gt; future = kafkaTemplate.send(topic, o); future.addCallback(result -\u0026gt; logger.info(\u0026#34;生产者成功发送消息到topic:{} partition:{}的消息\u0026#34;, result.getRecordMetadata().topic(), result.getRecordMetadata().partition()), ex -\u0026gt; logger.error(\u0026#34;生产者发送消失败，原因：{}\u0026#34;, ex.getMessage())); 另外可以为producer设置失败的重试次数和重试的时间间隔。如果多次重试失败，可以把消息加入死信队列\n消费者丢失消息 {#%E6%B6%88%E8%B4%B9%E8%80%85%E4%B8%A2%E5%A4%B1%E6%B6%88%E6%81%AF%C2%A0} 消息加入分区后会有一个offset，如果消费者拉取到了分区的某个消息之后，消费者会自动提交了 offset，此时如果消费者挂掉，那么消息并没有被消费，offset却自动提交了。一种解决方法是关闭自动提交，开启手动提交，但是如果消费完消息还没手动提交offset，消费者挂掉，那么该消息会被消费第二次甚至更多次，直到offset提交。\nkafka丢消息 {#kafka%E4%B8%A2%E6%B6%88%E6%81%AF} kafka的分区有一个多副本机制，副本之间有一个leader副本，其他副本是follower，如果leader副本突然挂掉，有些数据还未来得及同步到follower中，会消息丢失。\n解决方法有以下几种：\n设置acks=all，表示只有所有 ISR 列表（所有的可用副本）的副本全部收到消息时，生产者才会接收到来自服务器的响应。acks 的默认值即为 1，代表我们的消息被 leader 副本接收之后就算被成功发送。 设置min.insync.replicas \u0026gt; 1， 代表消息至少要被写入到 2 个副本才算是被成功发送。min.insync.replicas 的默认值为 1 ，在实际生产中应尽量避免默认值 1。 **设置 unclean.leader.election.enable = false，**各个follower的同步情况不一样，当 leader 副本发生故障时就不会从 follower 副本中和 leader 同步程度达不到要求的副本中选择出 leader ，这样降低了消息丢失的可能性。 kafka如何保证消息不重复消费 {#kafka%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E4%B8%8D%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9} 消息重复消费的根本原因是已消费的数据没有成功提交offset，在上面消费者丢失消息中已讲到。\nRocketMQ RocketMQ常见问题总结 | JavaGuide\nrocketMQ是阿里开源的消息队列，由java开发，和kafka的topic中是partition，rocketMQ的topic中是queue，queue可以分布在不同的borker中来容灾。和kafka相同，对于一个topic的一个queue，同个消费者组有一个消费者消费\nRocketMQ架构 {#RocketMQ%E6%9E%B6%E6%9E%84} NameServer、Broker、Producer、Consumer ，NameServer作用和kafka的zk相同，用来维护rocketmq的元信息，注册发现borker和路由信息管理，但是相对zk来说更为轻量\nRabbitMQ RabbitMQ常见问题总结 | JavaGuide\n基于AMQP实现，由erlang编写，在 RabbitMQ 中，消息并不是直接被投递到queue中，中间还必须经过 Exchange(交换器) 这一层，Exchange会把消息分配到对应的queue。\n生产者将消息发送给交换器时，需要一个 RoutingKey，当 BindingKey 和 RoutingKey 相匹配时，消息会被路由到对应的队列中。\nexchange类型 {#exchange%E7%B1%BB%E5%9E%8B%C2%A0} RabbitMQ 的 Exchange 有 4 种类型，不同的类型对应着不同的路由策略：direct(默认)，fanout, topic, 和 headers。\nfanout 把所有发送到该 Exchange 的消息路由到所有与它绑定的 Queue 中，不需要做任何判断操作，所以 fanout 类型是所有的交换机类型里面速度最快 的。fanout 类型常用来广播消息。 direct 把消息路由到那些 Bindingkey 与 RoutingKey 完全匹配的 Queue 中，常用在处理有优先级的任务，根据任务的优先级把消息发送到对应的队列，这样可以指派更多的资源去处理高优先级的队列。\ntopic把可以把一条消息发送到匹配的多个queue中，BindingKey和RoutingKey 为一个点号\u0026quot;．\u0026ldquo;分隔的字符串，如a.b.c，BindingKey 中还可以存在\u0026rdquo;*\u0026ldquo;和\u0026rdquo;#\u0026ldquo;做模糊匹配，\u0026rdquo;*\u0026ldquo;用于匹配1个单词，\u0026rdquo;#\u0026ldquo;匹配0或多个单词。 headers 类型的交换器不依赖路由键的匹配规则来路由消息 ，而是根据发送的消息内容中的 headers 属性进行完全匹配。 死信队列 {#%E6%AD%BB%E4%BF%A1%E9%98%9F%E5%88%97%C2%A0} 当消息在一个队列中变成死信之后，它能被重新发送到另一个交换器中，这个交换器就是 DLX，绑定 DLX 的队列就称之为死信队列。\n导致的死信的几种原因：\n消息被拒（Basic.Reject /Basic.Nack) 且 requeue = false。 消息 TTL 过期。 队列满了，无法再添加。 ","date":"2025-02-14T12:00:00Z","permalink":"/zh-cn/post/2025/02/%E5%B7%B2%E8%80%81%E5%AE%9E%E5%86%8D%E5%AD%A6%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E6%AD%BB%E4%BF%A1%E9%98%9F%E5%88%97/","title":"已老实！再学消息队列、死信队列"},{"content":" 往期推荐\nKafka入门到入土\u0026mdash;\u0026mdash;万字详解，图文并茂_图解kafka-CSDN博客\nJava内存模型（Memory Model）-CSDN博客\nArrayList、LinkedList、HashMap、HashTable、HashSet、TreeSet-CSDN博客\nAQS\u0026mdash;抽象队列同步器、CLH锁队列-CSDN博客\n参考：https://segmentfault.com/a/1190000042313862\n符号引用 以一组符号来描述所引用的目标。 符号引用可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可，符号引用和虚拟机的布局无关。 为什么要有符号引用？ java类被编译成class文件时，虚拟机并不知道所引用类的地址，所以就用符号引用来代替 ，而在链接的解析阶段 就是为了把这个符号引用转化成为真正的地址即直接引用。\n直接引用 直接引用和虚拟机的布局是相关的，不同的虚拟机对于相同的符号引用所翻译出来的直接引用一般是不同的。 直接引用可以是指向对象，类变量和类方法的指针、相对偏移量、一个间接定位到对象的句柄。 强引用 把一个对象赋给一个引用变量，如 MikeChen mikechen=new MikeChen();\n在一个方法的内部有一个强引用，这个引用保存在Java栈中，而真正的引用内容(MikeChen)保存在Java堆中。\n如果一个对象具有强引用，即使OOM垃圾回收器不会回收该对象，如果强引用对象不使用时，需要弱化从而使GC能够回收，如 mikechen=null;\n软引用（Soft） 1 2 3 软引用是一种相对强引用弱化了一些的引用，需要用java.lang.ref.SoftReference 类来实现： String str=new String(\u0026#34;abc\u0026#34;); // 强引用 SoftReference\u0026lt;String\u0026gt; softRef=new SoftReference\u0026lt;String\u0026gt;(str); 内存不足时就会回收该对象内存，gc不一定回收。\n弱引用（Weak） 不管内存是否足够，只要发生 GC，都会被回收。\n调用System.gc()方法只是起通知作用，不一定立刻gc，JVM的gc时机由JVM自己的状态决定。\n比如ThreadLocal的静态内部类Entry就继承了弱引用，ThreadLocalMap使用ThreadLocal的弱引用作为key，GC时这个ThreadLocal势必会被回收，这样一来，ThreadLocalMap中就会出现key为null的Entry，就没有办法访问这些key为null的Entry的value，也就造成内存泄漏。\n虚引用（Phantom） 虚引用（Phantom Reference）是Java中最弱的引用类型，无法通过引用直接获取到对象实例 。虚引用主要用于跟踪对象被垃圾回收的状态。当一个对象只被虚引用关联时，其实际上并不影响对象的生命周期，也就是说，垃圾回收器随时可能回收被虚引用关联的对象。\n--- ","date":"2025-02-13T14:03:21Z","permalink":"/zh-cn/post/2025/02/%E7%AC%A6%E5%8F%B7%E5%BC%95%E7%94%A8%E5%92%8C%E7%9B%B4%E6%8E%A5%E5%BC%95%E7%94%A8%E5%BC%BA%E5%BC%95%E7%94%A8%E8%BD%AF%E5%BC%95%E7%94%A8%E5%BC%B1%E5%BC%95%E7%94%A8%E8%99%9A%E5%BC%95%E7%94%A8/","title":"符号引用和直接引用、强引用、软引用、弱引用、虚引用"},{"content":" 往期推荐\nJava内存模型（Memory Model）-CSDN博客\n扫盲，CRM、ERP、OA、MVP \u0026hellip;-CSDN博客\nArrayList、LinkedList、HashMap、HashTable、HashSet、TreeSet-CSDN博客\n参考：Java AQS 核心数据结构-CLH 锁\n什么是AQS AbstractQueuedSynchronizer，一个抽象类，用来构建锁和同步器，定义了资源获取和释放的通用流程，ReentrantLock、Semaphore皆是基于AQS实现的。\n核心思想\n线程请求的共享资源已被占用，那么该请求线程进入AQS的CLH队列进行等待，否则把请求线程设置为有效的工作线程，并将共享资源设置为占用状态，即该线程占用了共享资源。\nAQS的CLH锁队列 自旋锁即线程不断对一个原子变量CAS来尝试获取锁，若多线程同时竞争同一个原子变量，可能造成某个线程的 CAS 操作长时间失败，从而导致 \u0026ldquo;饥饿\u0026quot;问题 ，而CLH锁对自旋锁进行了改进，通过引入一个单向队列来让线程排队确保公平性，避免饥饿。\nAQS 在 CLH 锁的基础上进一步优化，形成了其内部的 CLH 队列变体，优化点如下：\n自旋+阻塞：普通的CLH锁使用纯自旋等待锁释放，大量自旋会占用CPU资源，AQS的CLH锁则会短暂自旋，失败后进入阻塞状态，等待被唤醒，减少CPU占用。 双向队列 ：普通的CLH锁是单向的，节点只知道前驱节点的状态，而当某个节点释放锁时，需要通过队列唤醒后续节点。AQS 将队列改为 双向队列 ，新增了 next 指针，使得节点不仅知道前驱节点，也可以直接唤醒后继节点，从而简化了队列操作，提高了唤醒效率。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 /** * Head of the wait queue, lazily initialized. Except for * initialization, it is modified only via method setHead. Note: * If head exists, its waitStatus is guaranteed not to be * CANCELLED. */ private transient volatile Node head; /** * Tail of the wait queue, lazily initialized. Modified only via * method enq to add new wait node. */ private transient volatile Node tail; /** * The synchronization state. */ private volatile int state; Node节点各个状态含义 为什么state要用volatile修饰 使用 volatile 修饰state不是为了利用 volatile 的内存可见性，因为state本来就只会被持有线程写入，只会被队列中该线程的后驱节点对应的线程读，而且后者会轮询读取。因此，可见性问题不会影响锁的正确性。\n但要实现一个可以在多线程程序中正确执行的锁，还需要解决重排序问题 。\n在《Java 并发编程实战》一书对于重排序问题是这么描述的：在没有同步的情况下，编译器、处理器以及运行时等都可能对操作的执行顺序进行一些意想不到的调整。在缺乏足够同步的多线程程序中，要想对内存操作的执行顺序进行判断，几乎无法得到正确的结论。对于 Java synchronized 关键字提供的内置锁(又叫监视器) ，Java内存模型规范中有一条 Happens-Before（先行发生）规则：\u0026ldquo;一个监视器锁上的解锁应该发生在该监视器锁的后续锁定之前\u0026quot;也就是后面的锁在锁定之前得知道前面的锁有没有解锁，而自定义互斥锁就需要自己保证这一规则的成立，因此上述代码通过 volatile 的 Happens-Before（先行发生）规则来解决重排序问题。JMM 的 Happens-Before（先行发生）规则有一条针对 volatile 关键字的规则：\u0026ldquo;volatile 变量的写操作发生在该变量的后续读之前\u0026rdquo;。\nAQS的独占和共享 Exclusive（独占，如ReentrantLock）和Share（共享，如Semaphore/CountDownLatch）。\n一般来说，自定义同步器的共享方式要么是独占，要么是共享，他们也只需实现tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但 AQS 也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock。\nacquire()和release() 6.1 acquire() 1 2 3 4 5 6 7 8 9 10 public final void acquire(int arg) { if (!tryAcquire(arg) \u0026amp;\u0026amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } tryAcquire() ：尝试获取锁（模板方法），AQS 不提供具体实现，由子类实现。 acquireQueued() ：对线程进行阻塞、唤醒，并调用 tryAcquire() 方法让队列中的线程尝试获取锁。 addWaiter() ：如果获取锁失败，会将当前线程封装为 Node 节点加入到 AQS 的 CLH 变体队列中等待获取锁。 6.2 release() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public final boolean release(int arg) { // 1、尝试释放锁 if (tryRelease(arg)) { Node h = head; // 2、唤醒后继节点 if (h != null \u0026amp;\u0026amp; h.waitStatus != 0) unparkSuccessor(h); return true; } return false; } tryRelease() 方法尝试释放锁，该方法为模板方法，由自定义同步器实现。 如果 tryRelease() 返回 true ，表明线程已经没有重入次数了，锁已经被完全释放，因此需要唤醒后继节点。 以ReentrantLock讲解AQS原理 假设有 3 个线程尝试抢占 获取锁，线程分别为 T1 、 T2 和 T3。\n假设线程 T1 先获取到锁，线程 T2 排队等待获取锁。但是在线程 T2 进入队列之前，需要初始化AQS的CLH锁队列。head 节点在初始化后状态为 0 。AQS 内部初始化后的队列如下\n由于线程 T1 持有锁，因此线程 T2 会获取失败并进入队列中等待获取锁。同时会将前继节点（ head 节点）的状态由 0 更新为 SIGNAL ，表示需要对 head 节点的后继节点进行唤醒。此时，AQS 内部队列如下图所示：\n由于线程 T1 持有锁，因此线程 T3 也获取锁失败，会进入队列中等待获取锁。同时会将前继节点（线程 T2 节点）的状态由 0 更新为 SIGNAL ，表示线程 T2 节点需要对后继节点进行唤醒。此时，AQS 内部队列如下图所示：\n此时，假设线程 T1 释放锁，会唤醒后继节点 T2 。线程 T2 被唤醒后获取到锁，并且会从等待队列中退出（不是移除，因为T2还要当head）。\n--- ","date":"2025-02-12T11:02:38Z","permalink":"/zh-cn/post/2025/02/aqs---%E6%8A%BD%E8%B1%A1%E9%98%9F%E5%88%97%E5%90%8C%E6%AD%A5%E5%99%A8clh%E9%94%81%E9%98%9F%E5%88%97/","title":"AQS---抽象队列同步器、CLH锁队列"},{"content":"数组与集合区别 数组固定长度，集合动态改变\n数组可以包含基本数据类型，集合只能包含对象\n数组可以直接访问元素，集合需要通过迭代器或其他方法访问\n集合族谱 在这些集合中，仅有vector和hashtable是线程安全的，其内部方法基本都有synchronized修饰。\nArrayList 底层采用Object数组实现，实现了RandomAccess接口因此支持随机访问。插入删除操作效率慢。\nArrayList需要一份连续的内存空间。\nArrayList扩容机制 ArrayList添加元素时，若达到了内部数组指定的数量上限，会自动进行扩容：\n计算新容量，一般是原容量的1.5倍（1.5 倍，是因为 1.5 可以充分利用移位操作，减少浮点数或者运算时间和运算次数） 根据新容量创建新数组 把原来的数据拷贝到新数组中 更新ArrayList内部指向原数组的引用，指向新数组 ArrayList哪里不安全 首先，对arraylist添加一个元素，分为3步\n判断数组是否需要扩容，如果需要就调用grow方法扩容； 将数组的size位置设置值（因为数组的下标是从0开始的）； 将当前集合的大小+1 多线程插入删除下，ArrayList会暴露三个问题：\n出现null值：\n假设arraylist容量为10，线程1检查当前size=4，不需要扩容，于是在index=4进行插入，但是还没有size++，线程2又来进行插入，检查不需要扩容且size=4，于是也在index=4执行插入，然后两个线程同时执行size++，就导致实际size=6，两次插入都在index=4，而index=5的地方并没有插入数据。 索引越界异常\n还是上述例子，假设线程1检查size=9，没有到10，无需扩容，于是在index=9的地方插入，但还没有size++，线程2来检查size=9，也在index=9的地方插入，然后两个线程同时++，导致size=11。 集合的size()和实际add数量不符\nsize++不 ","date":"2025-02-11T12:00:00Z","permalink":"/zh-cn/post/2025/02/arraylistlinkedlisthashmaphashtablehashsettreeset/","title":"ArrayList、LinkedList、HashMap、HashTable、HashSet、TreeSet"},{"content":" 往期推荐\nsynchronized锁升级-CSDN博客\n字符串常量池-CSDN博客\nhttps://segmentfault.com/a/1190000045398760\n总结 JMM是jvm定义的一套规范，用来规范多线程并发时对共享资源的访问规则，如何保证多线程的可见性、原子性、有序性。JMM把内存分为线程的工作内存和主内存。\nCPU缓存 我们知道CPU是有缓存的，CPU缓存是为了解决主内存和CPU处理速度不对等的问题。其工作方式是先复制一份数据到 CPU 缓存中，当 CPU 需要用到的时候就可以直接从 CPU 缓存中读取数据，当运算完成后，再将运算得到的数据写回主内存中。但是，这样存在 内存缓存不一致性的问题 ！比如执行一个 i++ 操作的话，如果两个线程同时执行的话，假设两个线程从 CPU 缓存中读取的 i=1，两个线程做了 i++ 运算完之后再写回 Main Memory 之后 i=2，而正确结果应该是 i=3。\n那么CPU 为了解决内存缓存不一致性问题就需要定制协议规范，即内存模型，无论是 Windows 系统，还是 Linux 系统，它们都有特定的内存模型。\n指令重排 为了提升执行速度/性能，计算机在执行程序代码的时候，会对指令进行重排序，即执行代码的顺序和实际代码编写顺序不一定相同。常见的指令重排序有下面 2 种情况：\n编译器优化重排：编译器（包括 JVM、JIT 编译器等）在不改变单线程程序语义的前提下，重新安排语句的执行顺序。 指令并行重排：现代处理器采用了指令级并行技术(Instruction-Level Parallelism，ILP)来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 例：a = b + c; d = e - f ; 先加载 b、c（注意，有可能先加载 b，也有可能先加载 c ），但是在执行 add(b,c) 的时候，需要等待 b、c 装载结束才能继续执行，也就是需要增加停顿，那么后面的指令（加载 e 和 f）也会有停顿，这就降低了计算机的执行效率。为了减少停顿，我们可以在加载完 b 和 c 后把 e 和 f 也加载了，然后再去执行 add(b,c)，这样做对程序（串行）是没有影响的，但却减少了停顿。提高了效率！\n然而指令重排序可以保证串行语义一致，但无法保证多线程间的语义也一致 ，在多线程下指令重排序可能会导致一些问题。\n对于编译器优化重排和处理器的指令重排序（指令并行重排和内存系统重排都属于是处理器级别的指令重排序），处理该问题的方式不一样。\n对于编译器，通过禁止特定类型的编译器重排序的方式来禁止重排序。\n对于处理器，通过插入内存屏障或内存栅栏的方式来禁止特定类型的处理器重排序。\nJMM 一般来说，编程语言也可以直接复用操作系统层面的内存模型。不过，不同的操作系统内存模型不同。如果直接复用操作系统层面的内存模型，就可能会导致同样一套代码换了一个操作系统就无法执行了。Java 语言是跨平台的，它需要自己提供一套内存模型以屏蔽系统差异。\n所以Java 线程之间的通信由 Java 内存模型控制，同时保证了java的跨平台，定义了并发编程的规范，抽象了线程和主内存的关系，避免出现像CPU指令重排导致的多线程问题。\nJMM核心概念：\n**内存分区：**JMM 将内存分为 主内存 和 线程工作内存， 主内存就是 所有线程共享的内存区域，包括堆内存和方法区。线程工作内存就是每个线程独有的内存区域，包括局部变量、操作栈、寄存器等。 **可见性：**一个线程对共享变量的修改，其他线程能够立即看到。 **原子性：**一个操作要么全部完成，要么全部不完成，不会出现中间状态。 顺序性：程序中代码的执行顺序与代码的书写顺序一致。 3.1 JMM如何抽象线程和主内存的关系 Java 内存模型（JMM） 抽象了线程和主内存之间的关系，就比如说线程之间的共享变量必须存储在主内存中。\n在 JDK 2之前，Java 的内存模型实现是从 主存 （即共享内存）读取变量，而在当前的 Java 内存模型下，线程可以把变量保存 本地内存 （比如机器的寄存器）中，而不是直接在主存中进行读写。这就可能造成一个线程在主存中修改了一个变量的值，而另外一个线程还继续使用它在寄存器中的变量值的拷贝，造成数据的不一致。 什么是主内存、本地内存\n主内存：所有线程创建的实例对象都存放在主内存中，不管该实例对象是成员变量，还是局部变量，类信息、常量、静态变量都是放在主内存中。为了获取更好的运行速度，虚拟机及硬件系统可能会让工作内存优先存储于寄存器和高速缓存中。 本地内存 ：每个线程都有一个私有的本地内存，本地内存存储了该线程以读 / 写共享变量的副本。每个线程只能操作自己本地内存中的变量，无法直接访问其他线程的本地内存。如果线程间需要通信，必须通过主内存来进行。本地内存是 JMM 抽象出来的一个概念，并不真实存在，它涵盖了缓存、写缓冲区、寄存器以及其他的硬件和编译器优化。 从上图来看，线程 1 与线程 2 之间如果要进行通信的话，必须要经历下面 2 个步骤：\n线程 1 把本地内存中修改过的共享变量副本的值同步到主内存中去。 线程 2 到主存中读取对应的共享变量的值。 也就是说，JMM 为共享变量提供了可见性的保障。不过多线程操作主内存的共享变量也是有线程安全问题的\n3.2 happens-before 原则 前面提到了指令重排可能会引发多线程的执行问题，为此JMM 抽象了 happens-before 原则来解决这个指令重排序问题。\nhappens-before 原则表达的意义其实并不是一个操作发生在另外一个操作的前面，它更想表达的意义是前一个操作的结果对于后一个操作是可见的，无论这两个操作是否在同一个线程里。\n举个例子：操作 1 happens-before 操作 2，即使操作 1 和操作 2 不在同一个线程内，JMM 也会保证操作 1 的结果对操作 2 是可见的。\n并发编程的三个重要特性 原子性\n一次操作或者多次操作，要么所有的操作全部都得到执行并且不会受到任何因素的干扰而中断，要么都不执行。\n在 Java 中，可以借助synchronized、各种 Lock 以及各种原子类实现原子性。\nsynchronized还可以保证可见性！\n可见性\n当一个线程对共享变量进行了修改，那么另外的线程都是立即可以看到修改后的最新值。\n在 Java 中，可以借助synchronized、volatile 以及各种 Lock 实现可见性。\n如果我们将变量声明为 volatile ，这就指示 JVM，这个变量是共享且不稳定的，每次使用它都到主存中进行读取。\n有序性\n由于指令重排序问题，代码的执行顺序未必就是编写代码时候的顺序。\n我们上面讲重排序的时候也提到过，指令重排序可以保证串行语义一致，但是没有义务保证多线程间的语义也一致 ，所以在多线程下，指令重排序可能会导致一些问题。\n在 Java 中，volatile 关键字可以禁止指令进行重排序优化\n--- ","date":"2025-02-10T12:00:00Z","permalink":"/zh-cn/post/2025/02/java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8Bmemory-model/","title":"Java内存模型（Memory Model）"},{"content":" 往期推荐\nsynchronized锁升级-CSDN博客\n【排坑】云服务器docker部署前后端分离项目+域名解析+OSS-CSDN博客\n【大坑！已解决】docker容器jar包连不上另一个容器的mysql_docker启动jar包时无法连接mysql-CSDN博客\nJVM图文入门-CSDN博客\n【已解决】OSS配置问题_keyuewenhua.oss-cn-beijing.aliyuncs-CSDN博客\nString s=new String(\u0026ldquo;abc\u0026rdquo;)时， 虚拟机会先去字符串常量池查找有无abc这个字符串对象，如果有就不在字符串常量池创建了，直接在堆中创建一个abc字符串对象，然后将堆中这个abc的对象地址 返回赋值给变量，如果没有，则先在字符串常量池创建字符串abc，然后在堆中创建abc的字符串对象，然后将堆中这个abc的对象地址返回赋值给变量。\njava的栈上存储的是基本数据类型的变量和对象的引用，而对象本身则存储在堆上。 为什么要先在字符串常量池中创建对象，然后再在堆上创建呢？\n通常我们会用双引号的方式创建字符串对象，而不是new关键字，此时虚拟机会先在字符串常量池中查找有没有\u0026quot;abc\u0026quot;这个字符串对象，如果有，则不创建任何对象，直接将字符串常量池中这个\u0026quot;abc\u0026quot;的对象地址返回，赋给变量 s；如果没有，在字符串常量池中创建\u0026quot;abc\u0026quot;这个对象，然后将其地址返回，赋给变量 s。此时就不用在堆中创建对象了 String s = new String(\u0026ldquo;abc\u0026rdquo;); String s1 = new String(\u0026ldquo;abc\u0026rdquo;);\n这两行代码会创建三个对象，字符串常量池一个、堆上两个。\nString s = \u0026ldquo;abc\u0026rdquo;; String s1 = \u0026ldquo;abc\u0026rdquo;;\n这两行代码只会创建一个对象，就是字符串常量池中的那个。\n","date":"2025-02-07T12:00:00Z","permalink":"/zh-cn/post/2025/02/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%B8%B8%E9%87%8F%E6%B1%A0/","title":"字符串常量池"},{"content":" 往期推荐{#tableOfContents}\n【已解决】redisCache注解失效，没写cacheConfig_com.howbuy.cachemanagement.client.redisclient#incr-CSDN博客{#tableOfContents}\n【已解决】OSS配置问题_keyuewenhua.oss-cn-beijing.aliyuncs-CSDN博客{#tableOfContents}\n【排坑】云服务器docker部署前后端分离项目+域名解析+OSS-CSDN博客{#tableOfContents}\n微服务概念入门：Nacos、OpenFeign、Sentinel、GateWay、Seata-CSDN博客{#tableOfContents}\n字符串常量池-CSDN博客{#tableOfContents}\n目录{#main-toc}\n1. JVM8结构图{#main-toc-toc}\n2. Java性能低的主要原因{#Java%E6%80%A7%E8%83%BD%E4%BD%8E%E7%9A%84%E8%80%8C%E4%B8%BB%E8%A6%81%E5%8E%9F%E5%9B%A0-toc}\n3. 字节码文件{#%E5%AD%97%E8%8A%82%E7%A0%81%E6%96%87%E4%BB%B6-toc}\n3.1 字节码文件的组成{#%E5%AD%97%E8%8A%82%E7%A0%81%E6%96%87%E4%BB%B6%E7%9A%84%E7%BB%84%E6%88%90-toc}\n4. JVM架构{#5.%20JVM%E6%9E%B6%E6%9E%84-toc}\n4.1 类加载器ClassLoader{#%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8ClassLoader-toc}\n4.2 运行时数据区{#%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA-toc}\n程序计数器{#%E7%A8%8B%E5%BA%8F%E8%AE%A1%E6%95%B0%E5%99%A8-toc}\nJava虚拟机栈（方法栈）{#Java%E6%96%B9%E6%B3%95%E6%A0%88%EF%BC%88%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%A0%88%EF%BC%89-toc}\n本地方法栈{#%E6%9C%AC%E5%9C%B0%E6%96%B9%E6%B3%95%E6%A0%88%C2%A0-toc}\n堆{#Heap%E5%A0%86-toc}\n4.3 执行引擎{#%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E-toc}\n5. 双亲委派{#%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE-toc}\n5.1 破坏双亲委派{#%E7%A0%B4%E5%9D%8F%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE-toc}\nTomcat破坏{#Tomcat%E7%A0%B4%E5%9D%8F-toc}\nJDBC破坏{#JDBC%E7%A0%B4%E5%9D%8F%C2%A0-toc}\nJVM8结构图 {#main-toc} Java性能低的主要原因 {#Java%E6%80%A7%E8%83%BD%E4%BD%8E%E7%9A%84%E8%80%8C%E4%B8%BB%E8%A6%81%E5%8E%9F%E5%9B%A0} Java语言如果不做任何的优化，性能其实是不如C和C++语言的。主要原因是：\n在程序运行过程中，Java虚拟机需要将字节码指令实时 地解释成计算机能识别的机器码，这个过程在运行时可能会反复执行，所以效率较低。\nC和C++语言在执行过程中，只需将源代码编译成可执行文件，就包含了计算机能识别的机器码，无需在运行过程中再实时地解释，所以性能较高。\nJava为什么要选择一条执行效率比较低的方式呢？主要是为了实现跨平台的特性。Java的字节码指令，如果希望在不同平台（操作系统+硬件架构），比如在windows或者linux上运行。可以使用同一份字节码指令，交给windows和linux上的Java虚拟机进行解释，这样就可以获得不同平台上的机器码了。这样就实现了Write Once，Run Anywhere 编写一次，到处运行。\n字节码文件 {#%E5%AD%97%E8%8A%82%E7%A0%81%E6%96%87%E4%BB%B6} 我们java中说的字节码文件即 java代码编译后的.class文件，class文件可以跨平台运行在不同操作系统的JVM上。\n3.1 字节码文件的组成 {#%E5%AD%97%E8%8A%82%E7%A0%81%E6%96%87%E4%BB%B6%E7%9A%84%E7%BB%84%E6%88%90} 字节码文件总共可以分为以下几个部分：\n基础信息：魔数、字节码文件对应的Java版本号、访问标识(public final等等)、父类和接口信息\n常量池 **：**保存了字符串常量、类或接口名、字段名，主要在字节码指令中使用\n字段： 当前类或接口声明的字段信息\n**方法：**当前类或接口声明的方法信息，核心内容为方法的字节码指令\n**属性：**类的属性，比如源码的文件名、内部类的列表等\nJVM架构 {#5.%20JVM%E6%9E%B6%E6%9E%84} 根据上面的JVM图，JVM大致可分为三块： 类加载器ClassLoader、运行时数据区 、执行引擎\n4.1 类加载器ClassLoader {#%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8ClassLoader} 类加载器会通过二进制流的方式获取到字节码文件并交给Java虚拟机，虚拟机会在方法区和堆上生成对应的对象保存字节码信息。\n根加载器（启动类加载器）：\n默认加载Java安装目录/jre/lib下的类文件，比如rt.jar，tools.jar，resources.jar等。\n扩展类加载器：\n默认加载Java安装目录/jre/lib/ext下的类文件\n应用程序类加载器（系统类加载器）：\n默认加载的是项目中的类以及通过maven引入的第三方jar包中的类。\n用户自定义类加载器\n输出为null是因为根加载器的具体实现是由C或C++编写，不在java范围内。\n4.2 运行时数据区 {#%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA} 运行时数据可以划分为以下5块\n程序计数器 {#%E7%A8%8B%E5%BA%8F%E8%AE%A1%E6%95%B0%E5%99%A8} 每个线程都有一个私有的程序计数器，也就是一个指针，指向方法区中的方法字节码（用来存储指向指令的地址）。解释器会在工作的时候改变这个计数器的值来选取下一条需要执行的字节码指令。如果线程执行的是非本地方法，则程序计数器中保存的是当前需要执行的指令地址；如果线程执行的是本地方法，则程序计数器中的值是 undefined。\nJava虚拟机栈（方法栈） {#Java%E6%96%B9%E6%B3%95%E6%A0%88%EF%BC%88%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%A0%88%EF%BC%89} 栈中没有垃圾回收的，线程结束后内存会自动释放。栈主管程序运行、生命周期、线程同步。\nJava 虚拟机栈中是一个个栈帧，每个栈帧对应一个被调用的方法，当线程执行一个方法时，会创建一个对应的栈帧，并将栈帧压入栈中。当方法执行完毕后，将栈帧从栈中弹出。\n栈帧及组成{#%E6%A0%88%E5%B8%A7}\n局部变量表，局部变量表的作用是在运行过程中存放所有的局部变量\n操作数栈，操作数栈是栈帧中虚拟机在执行指令过程中用来存放临时数据的一块区域\n帧数据，帧数据主要包含动态链接、方法出口、异常表的引用\n​\nstack1的方法结束后要弹出栈，此时需要通过stack1返回下面的stack2的方法。\n本地方法栈 {#%E6%9C%AC%E5%9C%B0%E6%96%B9%E6%B3%95%E6%A0%88%C2%A0} Java虚拟机栈存储了Java方法调用时的栈帧，而本地方法栈存储的是native本地方法的栈帧\n堆 {#Heap%E5%A0%86} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 public class Test { public static void main(String[] args) { Student s1 = new Student(); s1.name = \u0026#34;张三\u0026#34;; s1.age = 18; s1.id = 1; s1.printTotalScore(); s1.printAverageScore(); Student s2 = new Student(); s2.name = \u0026#34;李四\u0026#34;; s2.age = 19; s2.id= 2; s2.printTotalScore(); s2.printAverageScore(); } } 这段代码中通过new关键字创建了两个Student类的对象，这两个对象会被存放在堆上。在栈上通过s1和s2两个局部变量保存堆上两个对象的地址，从而实现了引用关系的建立。\n​\n以前的Java 中\u0026quot;几乎\u0026quot;所有的对象都会在堆中分配，但随着JIT编译器的发展和逃逸技术的逐渐成熟，所有的对象都分配到堆上渐渐变得不那么\u0026quot;绝对\u0026quot;了。从 JDK 7 开始，Java 虚拟机已经默认开启逃逸分析了，意味着如果某些方法中的对象引用没有被返回或者未被外面使用（也就是未逃逸出去），那么对象可以直接在栈上分配内存。垃圾指JVM中没有任何引用指向它的对象\n逃逸分析\n逃逸分析是一种编译器优化技术，用于判断对象的作用域和生命周期。如果编译器确定一个对象不会逃逸出方法或线程的范围，它可以选择在栈上分配这个对象，而不是在堆上。这样做可以减少垃圾回收的压力，并提高性能。\n一个JVM实例只有一个堆内存，堆内存大小可以调节，类加载器读取类文件后要把类、方法、常变量放到堆内存中，保存所有引用类型的真实信息，堆内存在逻辑上分为三部分：\n新生代：伊甸区、幸存0区 from、幸存1区 to 老年代 永久代 4.3 执行引擎 {#%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E} 略\n双亲委派 {#%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE} 应用程序类加载器（又叫系统类加载器）收到类的加载请求先检查自己是否加载过该类，如果没有，将请求向上委托给自己的父类加载器（extensionLoader），如果父类加载器也没有加载过该类，该父类加载器继续向上委托给自己的父类加载器（bootstrapLoader，又叫根加载器、启动类加载器）若启动类加载器也没有加载过该类，则会根据要加载的类的全限定名尝试加载该类，若加载成功，则返回引用，若加载失败，则抛出异常，并反向委托给扩展类加载器，若仍加载失败，则继续抛出异常，并反向委托给应用程序类加载器，若仍加载失败，则报异常ClassNotFound。\n​\n​\n​\n安全性和沙箱机制\n由于java核心库和扩展库由根加载器加载，这些库中的类有更高的安全级别，而应用程序类由应用程序类加载器加载，安全级别低，双亲向上委派可以防止核心API被篡改，提高了程序安全性。\n什么是沙箱？\njava安全模型的核心就是java沙箱，沙箱是一个限制程序运行的环境，沙箱机制就是把java代码限定在jvm的特定运行范围内，严格限制代码对本地系统资源的访问（CPU、内存、文件系统、网络等），通过这样来保证代码的有效隔离，防止对本地系统造成破坏。\n避免类重复加载\n由于父类加载器加载类时会优先尝试加载，若类已经被加载过，就不会再次加载，避免了类重复加载。\n5.1 破坏双亲委派 {#%E7%A0%B4%E5%9D%8F%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE} 打破双亲委派机制历史上有三种方式，但本质上只有第一种算是真正的打破了双亲委派机制：\n自定义类加载器并且重写loadClass方法。Tomcat通过这种方式实现应用之间类隔离。\n线程上下文类加载器。利用上下文类加载器加载类，比如JDBC和JNDI等。\nOsgi框架的类加载器。历史上Osgi框架实现了一套新的类加载器机制，允许同级之间委托进行类的加载，目前很少使用。\nTomcat破坏 {#Tomcat%E7%A0%B4%E5%9D%8F} ​\nJDBC破坏 {#JDBC%E7%A0%B4%E5%9D%8F%C2%A0} JDBC中使用了DriverManager来管理项目中引入的不同数据库的驱动，比如mysql驱动、oracle驱动。DriverManager类位于rt.jar包中，由启动类加载器加载。依赖中的mysql驱动对应的类，由应用程序类加载器来加载。DriverManager属于rt.jar是启动类加载器加载的。而用户jar包中的驱动需要由应用类加载器加载，这就违反了双亲委派机制 。存疑\n​\nJDBC案例中真的打破了双亲委派机制吗？{#JDBC%E6%A1%88%E4%BE%8B%E4%B8%AD%E7%9C%9F%E7%9A%84%E6%89%93%E7%A0%B4%E4%BA%86%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE%E6%9C%BA%E5%88%B6%E5%90%97%EF%BC%9F}\n最早这个论点提出是在周志明《深入理解Java虚拟机》中，他认为打破了双亲委派机制，这种由启动类加载器加载的类，委派应用程序类加载器去加载类的方式，所以打破了双亲委派机制。\n但是如果我们分别从DriverManager以及驱动类的加载流程上分析，JDBC只是在DriverManager加载完之后，通过初始化阶段触发了驱动类的加载，类的加载依然遵循双亲委派机制。\n所以我认为这里没有打破双亲委派机制，只是用一种巧妙的方法让启动类加载器加载的类，去引发的其他类的加载。\n","date":"2025-02-06T12:00:00Z","permalink":"/zh-cn/post/2025/02/jvm%E5%9B%BE%E6%96%87%E5%85%A5%E9%97%A8/","title":"JVM图文入门"},{"content":"往期推荐\n浅谈云原生\u0026ndash;微服务、CICD、Serverless、服务网格_cicd 云原生-CSDN博客\n【排坑】云服务器docker部署前后端分离项目+域名解析+OSS-CSDN博客\n练习两年半，我的全栈博客出生了-CSDN博客\nhttps://blog.csdn.net/qq_73181349/article/details/145311064\n1.分布式基础 1.1 微服务 所谓微服务，就是把传统的单体项目的各个服务拆分出来单独部署，每个小服务 运行在自己 的进程中，他们之间通过HTTP调用进行通信。提高服务弹性、可维护性。\n1.2 ","date":"2025-02-05T16:56:07Z","permalink":"/zh-cn/post/2025/02/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8nacosopenfeignsentinelgatewayseata/","title":"微服务概念入门：Nacos、OpenFeign、Sentinel、GateWay、Seata"},{"content":"登陆注册页 首页 文章详情页 分类专栏 搜索结果页 可以根据类别、标签或者搜索框文本搜索\n创作页 这里参考了csdn的创作页\n支持多种皮肤，可以上传粘贴图片 AI页 引入了讯飞星火大模型lite版本，token不限量\n个人主页 主页也参考了csdn\n管理页 ","date":"2025-02-04T14:53:44Z","permalink":"/zh-cn/post/2025/02/%E7%BB%83%E4%B9%A0%E4%B8%A4%E5%B9%B4%E5%8D%8A%E6%88%91%E7%9A%84%E5%85%A8%E6%A0%88%E5%8D%9A%E5%AE%A2%E5%87%BA%E7%94%9F%E4%BA%86/","title":"练习两年半，我的全栈博客出生了"},{"content":" 踩坑推荐\n【大坑！已解决】docker容器jar包连不上另一个容器的mysql-CSDN博客\n【排坑】程序包jdk.nashorn.internal.ir.debug不存在-CSDN博客\n【已解决】 \\[ org.apache.catalina.core.StandardService : 173 \\] - Stopping service \\[Tomcat\\]-CSDN博客\n【已解决】redisCache注解失效，没写cacheConfig-CSDN博客\n【已解决】OSS配置问题-CSDN博客\n环境工具 阿里云服务器 Alibaba Cloud Linux 3.2104 LTS 64位 OSS 域名解析（具体就不写了，怕被DDOS🤡） 后端jar包 jdk11、springboot 2.6.13 前端umimax+antd系列 docker nginx latest redis7.2.4 etcd3.5.15 mysql8.0.35 SSH客户端 final shell mysql、redis、nginx等所有项目依赖环境全部部署在docker容器中。 mysql、redis这些，部署完记得在本地连一下看有没有问题，有问题就用docker logs看日志或者进容器改配置，服务器记得放开对应端口 ！！\n每部署完一个容器就用docker ps看一下status和port，如果是status是restarting或者port没映射上，大概率有问题，另外注意cpu和内存占用 （final shell看的很方便），有几次启动容器后cpu和内存占用爆满了，服务器都登不上了🤣\n拉取镜像慢的话可以改docker镜像配置，具体看\n","date":"2025-01-26T12:00:00Z","permalink":"/zh-cn/post/2025/01/%E6%8E%92%E5%9D%91%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8docker%E9%83%A8%E7%BD%B2%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB%E9%A1%B9%E7%9B%AE-%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90-oss/","title":"【排坑】云服务器docker部署前后端分离项目+域名解析+OSS"},{"content":"\nOSS SDK快速入门_对象存储(OSS)-阿里云帮助中心\n阿里官方的SDK使用方法还得配置环境变量access Key、access Secret ，我没有配置，仅把access Key和access Secret写到了yml文件读取，结果上传图片时还是出现下面的问题。\n[ ERROR ] [ com.serein.exception.GlobalExceptionHandler : 31 ] - RuntimeException com.aliyun.oss.common.auth.InvalidCredentialsException: Access key id should not be null or empty. at com.aliyun.oss.common.auth.DefaultCredentialProvider.checkCredentials(DefaultCredentialProvider.java:63) at com.aliyun.oss.common.auth.DefaultCredentialProvider.\u0026lt;init\u0026gt;(DefaultCredentialProvider.java:38) at com.aliyun.oss.common.auth.DefaultCredentialProvider.\u0026lt;init\u0026gt;(DefaultCredentialProvider.java:34) at com.aliyun.oss.OSSClientBuilder.getDefaultCredentialProvider(OSSClientBuilder.java:83) at com.aliyun.oss.OSSClientBuilder.build(OSSClientBuilder.java:38 解决方法是把BUCKET_NAME、END_POINT 、ACCESS_KEY_ID 、ACCESS_KEY_SECRET直接写死到代码中，如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public static String uploadImageOSS(MultipartFile img) { OSS ossClient= new OSSClientBuilder().build(END_POINT, ACCESS_KEY_ID, ACCESS_KEY_SECRET); if (!ossClient.doesBucketExist(BUCKET_NAME)) { ossClient.createBucket(BUCKET_NAME); CreateBucketRequest createBucketRequest = new CreateBucketRequest(BUCKET_NAME); createBucketRequest.setCannedACL(CannedAccessControlList.PublicRead); ossClient.createBucket(createBucketRequest); } //filePath是存到oss的文件名，fileUrl是访问的路径 String filePath = createOSSFileName(img.getOriginalFilename()); PutObjectRequest putObjectRequest = null; try { putObjectRequest = new PutObjectRequest(BUCKET_NAME, filePath, img.getInputStream()); } catch (IOException e) { throw new RuntimeException(e); } PutObjectResult result = ossClient.putObject(putObjectRequest); return \u0026#34;https://\u0026#34; + BUCKET_NAME + \u0026#34;.\u0026#34; + END_POINT + \u0026#34;/\u0026#34; + filePath; } 然后再次上传图片就出现了下面的问题，因为前面填的是OSS内网oss-cn-beijing-internal.aliyuncs.com ，换成oss-cn-beijing.aliyuncs.com外网就ok了\n[ WARN ] [ com.aliyun.oss : 70 ] - [Client]Unable to execute HTTP request: Connect to xxxx.oss-cn-beijing-internal.aliyuncs.com:80 failed: Connection timed out: connect [ErrorCode]: SocketException [RequestId]: Unknown 然后图片成功上传到OSS了，但是外部并不能访问，如果拿着url在浏览器查看，会这样\n\u0026lt;Error\u0026gt; \u0026lt;Code\u0026gt;AccessDenied\u0026lt;/Code\u0026gt; \u0026lt;Message\u0026gt;You have no right to access this object because of bucket acl.\u0026lt;/Message\u0026gt; \u0026lt;RequestId\u0026gt;679391585E3414373911E999\u0026lt;/RequestId\u0026gt; \u0026lt;HostId\u0026gt;blog-backend.oss-cn-beijing.aliyuncs.com\u0026lt;/HostId\u0026gt; \u0026lt;EC\u0026gt;0003-00000001\u0026lt;/EC\u0026gt; \u0026lt;RecommendDoc\u0026gt;https://api.aliyun.com/troubleshoot?q=0003-00000001\u0026lt;/RecommendDoc\u0026gt; \u0026lt;/Error\u0026gt; 解决方法就是关闭公共访问，并且设置读写权限为公共读，这样编辑器就可以根据url显示出图片了\n成功渲染出图片！\n","date":"2025-01-25T16:12:01Z","permalink":"/zh-cn/post/2025/01/%E5%B7%B2%E8%A7%A3%E5%86%B3oss%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98/","title":"【已解决】OSS配置问题"},{"content":"环境配置 jdk11 springboot 2.6.13 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-cache\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; redis: host: 192.168.2.129 port: 6379 username: default password: 'linux02' database: 1 # 默认使用lettuce lettuce: pool: # 最大连接数，最大空闲数，最小空闲数 max-active: 5 max-idle: 2 min-idle: 2 # 缓存10min，允许缓存null值防止缓存穿透 cache: type: redis redis: time-to-live: 600 cache-null-values: true 代码 1 2 3 4 5 6 7 @Cacheable(cacheNames = BLOG_CACHE_PREFIX + \u0026#34;otherPassages\u0026#34;, key = \u0026#34;#userId\u0026#34;) @Override public List\u0026lt;PassageTitleVO\u0026gt; getOtherPassagesByUserId(Long userId) { .... .... return passageTitleVOS; } 解决 网上找了一些案例，有的不需要写cacheConfig，有些需要写，我之前就用过cacheable的注解，当时就是上面的配置，没写配置类也有效果，这次我最开始就没写，然后Cacheable注解就没效果了，最后写了个cacheConfig才解决。\n1 2 3 4 5 6 7 8 9 10 11 @Configuration @EnableCaching public class CacheConfig extends CachingConfigurerSupport { @Bean public RedisCacheManager cacheManager(RedisConnectionFactory redisConnectionFactory) { return RedisCacheManager.builder(redisConnectionFactory).build(); } } 然后又发现yml设置的过期时间没有生效，存到redis的是永不过期，又在 cacheConfig配置了过期时间，600s\n1 2 3 4 5 6 7 8 9 10 11 12 13 @Configuration @EnableCaching public class CacheConfig extends CachingConfigurerSupport { @Bean public RedisCacheManager cacheManager(RedisConnectionFactory redisConnectionFactory) { RedisCacheConfiguration redisCacheConfiguration = RedisCacheConfiguration.defaultCacheConfig() .entryTtl(Duration.ofSeconds(600)); return RedisCacheManager.builder(redisConnectionFactory).cacheDefaults(redisCacheConfiguration) .build(); } } ","date":"2025-01-25T12:00:00Z","permalink":"/zh-cn/post/2025/01/%E5%B7%B2%E8%A7%A3%E5%86%B3rediscache%E6%B3%A8%E8%A7%A3%E5%A4%B1%E6%95%88%E6%B2%A1%E5%86%99cacheconfig/","title":"【已解决】redisCache注解失效，没写cacheConfig"},{"content":" 常见的mysql日志有二进制日志 binlog（归档日志）和重做日志 redo log（事务日志）和 undo log（回滚日志）。\nredo log MySQL 中数据是以页为单位，查询一条记录会从硬盘把一页的数据加载出来 ，加载出来的数据叫数据页 ，会放入到**Buffer Pool**中。\n后续的查询都是先从 Buffer Pool 中找，没有命中再去硬盘加载，减少硬盘 IO 开销，提升性能。\n更新表数据的时候，也是先更新Buffer Pool 的数据，如果没有则先把数据读到Buffer Pool。 然后会把\u0026quot;在某个数据页上做了什么修改\u0026quot;记录到redo log buffer，接着刷盘到 redo log文件里。\n那么什么时候会进行刷盘呢？\n为什么需要redo log buffer pool基于内存，提高了mysql性能，但是内存的数据没有持久化到磁盘，mysql宕机后会数据丢失，为了防止断电导致数据丢失的问题，当有一条记录需要更新的时候，InnoDB引擎就会先更新内存（同时标记为脏页），然后将本次对这个页的修改以redo log的形式记录下\n来，这个时候更新就算完成\n","date":"2025-01-24T21:03:52Z","permalink":"/zh-cn/post/2025/01/mysql%E4%B8%89%E5%A4%A7%E6%97%A5%E5%BF%97/","title":"MySQL三大日志"},{"content":"\n1 2 @Update(\u0026#34;update blog.user set mail=#{mail} where userId=#{userId}\u0026#34;) void updateEncrypt(String mail, Long userId); org.mybatis.spring.MyBatisSystemException: nested exception is org.apache.ibatis.binding.BindingException: Parameter \u0026lsquo;mail\u0026rsquo; not found. Available parameters are \\[arg1, arg0, param1, param2\\]。\nMyBatis 没有找到 mail 参数的映射，无法正确绑定参数。\nMyBatis 默认的行为是通过位置来传递参数 （例如 arg0, arg1）。如果使用了具名参数（如 mail 和 userId），需要确保方法参数名与注解中的名称一致。有如下两种解决方法：\n1 2 3 //使用 @Param 注解来明确指定参数名 @Update(\u0026#34;UPDATE blog.user SET mail=#{mail} WHERE userId=#{userId}\u0026#34;) void updateEncrypt(@Param(\u0026#34;mail\u0026#34;) String mail, @Param(\u0026#34;userId\u0026#34;) Long userId); 1 2 3 //如果方法有多个参数，可以将这些参数封装成一个对象，然后通过该对象传递。 @Update(\u0026#34;UPDATE blog.user SET mail=#{mail} WHERE userId=#{userId}\u0026#34;) void updateEncrypt(UserUpdateRequest request); --- ","date":"2025-01-24T12:00:00Z","permalink":"/zh-cn/post/2025/01/mybatisplus-mapper%E5%B1%82%E7%BB%91%E5%AE%9A%E5%8F%82%E6%95%B0%E9%94%99%E8%AF%AF/","title":"mybatisplus Mapper层绑定参数错误"},{"content":"问题 环境：jdk11，springboot 2.6.13\n本地打好的jar包传到云服务器上，用docker build打成镜像后，docker run运行不起来，用docker logs查看日志如下，最后一行stopping service \\[Tomcat\\]，因为我run的时候还加上了restart=always参数，所以导致这个jar包一直失败又重启，cpu都干到100%🤣\n原因分析 \\[ org.apache.catalina.core.StandardService : 173 \\] - Stopping service \\[Tomcat\\]当时就去网上查了一些解决方法，包括但不限于pom文件引入日志依赖、修改日志级别为debug以得到更多报错信息都没法解决，最后打算排除内嵌tomcat换成war包试试，偶然看到idea打jar包时输出的这样的警告（之前一直没注意过🤣🤣）：\n因为我有hotkey-client的jar包是自己打的然后在pom中引进来的，并且用了**systemPath引入**，这样写在本地idea是可以运行起来，但是项目打成jar包就会stopping service。\n解决方法也简单，上面的黄色警告已经说了依赖的 systemPath 不应指向项目目录内的文件，那么就把hotkey-client jar 包install到本地仓库，不用 systemPath.，这样项目打出来的jar包就可以正常运行，效果如下：\n另外本地打好的jar包最好先在本地环境先跑一下，本地跑不起来云服务器大概率也不行\n","date":"2025-01-23T12:00:00Z","permalink":"/zh-cn/post/2025/01/%E5%B7%B2%E8%A7%A3%E5%86%B3-org.apache.catalina.core.standardservice-173-stopping-service-tomcat/","title":"【已解决】 [ org.apache.catalina.core.StandardService : 173 ] - Stopping service [Tomcat]"},{"content":"今天项目打包的时候报这样的错误，jdk8，在idea项目中明明可以找到该文件路径和代码，但是打包打不进去，\n参考了一些博客文章，\n【Maven问题】 错误: 程序包xxx 不存在 - 简书\nMaven错误：程序包java.nashorn.XXX不存在_程序包jdk.nashorn.api.scripting不存在-CSDN博客\n有一定借鉴意义但是没解决问题，idea的classpath中已经包含了nashorn，且可以运行项目，但是maven就是打不成jar包，试来试去都没怀疑maven问题🤓，没想到还真是maven的问题。\nmaven的runner配置，jre原来是jdk17，改成1.8就ok了\n","date":"2025-01-22T12:00:00Z","permalink":"/zh-cn/post/2025/01/%E6%8E%92%E5%9D%91%E7%A8%8B%E5%BA%8F%E5%8C%85jdk.nashorn.internal.ir.debug%E4%B8%8D%E5%AD%98%E5%9C%A8/","title":"【排坑】程序包jdk.nashorn.internal.ir.debug不存在"},{"content":"最终解决方案 mysq版本l8.0.35，驱动是8.0.26，com.mysql.cj.jdbc.Driver\njdbc的url：\njdbc:mysql://云服务器的主机地址（127.0.0.1和localhost都不行）:3306/hotkey_db?useUnicode=true\u0026amp;characterEncoding=utf-8\u0026amp;useSSL=true\u0026amp;autoReconnect=true\u0026amp;failOverReadOnly=false\u0026amp;serverTimezone=GMT\u0026amp;useTimezone=true\n（有些参数应该是没用的，但是懒得改了，能跑就行）\n问题场景 这两天在云服务器的docker上部署了jar包，jar包要连另一个容器的mysql，在本地测试过本地运行的jar包可以连接云服务器的mysql，但是云服务器的docker的jar包连不上。具体展示如下：jar包成功运行后登录时就会报这样的错误，执行sql查询失败，说是连不上mysql ，以下是jar包的部分日志。\n报错日志 登录时执行mysql查询失败，因为连不上mysql，用docker network 查看了网络信息，这些个容器启动时候已经在同一个docker网络了，并且在jar包的容器中可以ping通mysql的网关\nmysql连接报错\ncommunications link failure，很经典的错误，去网上查了很多解决办法，包括但不限于修改jar包的jdbc的url连接参数，比如useSSL=false、时区serverTimeZone、localhost和127.0.0.1，甚至是mysql的配置文件my.cnf，说是因为连接池连接过期的，要修改wait_timeout。结果没一个能解决问题的，后来问了一个朋友，他正好遇见过同样的问题，之前还给我说过但是我没记住🤣，他的解决方案就是把jdbc的url换成云服务器的主机地址，而非127.0.0.4或localhost。至此问题解决。\n分析 docker的每个容器之间是进程隔离的，每个容器都相当于一个独立的主机，我的jar和mysql不在同一个容器，相当于mysql和jar包不在一个主机上，那么jar包里的127.0.0.1:3306显然是连不到mysql的。\n最后又试了试，如果jar包和mysql不在同一个docker网络，也是可以正常运行的，因为jar已经通过主机地址访问到mysql了\n","date":"2025-01-21T16:35:43Z","permalink":"/zh-cn/post/2025/01/%E5%A4%A7%E5%9D%91%E5%B7%B2%E8%A7%A3%E5%86%B3docker%E5%AE%B9%E5%99%A8jar%E5%8C%85%E8%BF%9E%E4%B8%8D%E4%B8%8A%E5%8F%A6%E4%B8%80%E4%B8%AA%E5%AE%B9%E5%99%A8%E7%9A%84mysql/","title":"【大坑！已解决】docker容器jar包连不上另一个容器的mysql"},{"content":"用户态和内核态 根据进程访问资源的特点，我们可以把进程在系统上的运行分为两个级别：\n用户态(User Mode) : 用户态运行的进程可以直接读取用户程序的数据 ，拥有较低的权限。当应用程序需要执行某些需要特殊权限的操作 ，例如读写磁盘、网络通信等，就需要向操作系统发起系统调用请求，进入内核态。 内核态(Kernel Mode) ：内核态运行的进程几乎可以访问计算机的任何资源包括系统的内存空间、设备、驱动程序等，不受限制，拥有非常高的权限。当操作系统接收到进程的系统调用请求时，就会从用户态切换到内核态，执行相应的系统调用，并将结果返回给进程，最后再从内核态切换回用户态。 不过，由于切换内核态需要付出较高的开销（需要进行一系列的上下文切换和权限检查），应该尽量减少进入内核态的次数，以提高系统的性能和稳定性。\n用户态和内核态如何切换 用户态切换到内核态的 3 种方式：\n系统调用（Trap） ：用户态进程 主动 要求切换到内核态的一种方式，主要是为了使用内核态才能做的事情比如读取磁盘资源。系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现。\n中断（Interrupt） ：当外围设备完成用户请求的操作后，会向 CPU 发出相应的中断信号，这时 CPU 会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序，如果先前执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了由用户态到内核态的切换。比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等。\n异常（Exception）：当 CPU 在执行运行在用户态下的程序时，发生了某些事先不可知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常。\n进程和线程 进程（Process） 是指计算机中正在运行的一个程序实例。比如打开某个应用。\n线程（Thread） 轻量级进程，多个线程可以在同一个进程中同时执行，并且共享进程的资源比如内存空间、文件句柄、网络连接等。举例：你打开的微信里就有一个线程专门用来拉取别人发你的最新的消息。\n协程 是一种用户态 的轻量级线程 ，其调度完全由用户程序控制，而不需要内核的参与。协程拥有自己的寄存器上下文和栈，但与其他协程共享堆内存。协程的切换开销非常小，因为只需要保存和恢复协程的上下文，而无需进行内核级的上下文切换。这使得协程在处理大量并发任务时具有非常高的效率。然而，协程需要程序员显式地进行调度和管理，相对于线程和进程来说，其编程模型更为复杂。\n进程的几种状态 和线程的状态很像\n创建状态(new)：进程正在被创建，尚未到就绪状态。 就绪状态(ready)：进程已处于准备运行状态，即进程获得了除了处理器之外的一切所需资源，一旦得到处理器资源(处理器分配的时间片)即可运行。 运行状态(running)：进程正在处理器上运行(单核 CPU 下任意时刻只有一个进程处于运行状态)。 阻塞状态(waiting)：又称为等待状态，进程正在等待某一事件而暂停运行如等待某资源为可用或等待 IO 操作完成。即使处理器空闲，该进程也不能运行。 结束状态(terminated)：进程正在从系统中消失。可能是进程正常结束或其他原因中断退出运行。 创建状态和就绪状态的区别：\n状态 通俗理解 线程内部发生了什么 是否能被调度 创建（NEW） 只是\u0026quot;生成了车票\u0026quot;，还没进站 已 new Thread()，但未调用 start() ，操作系统线程尚未真正创建 ❌ 调度器看不见，进不了就绪队列 就绪（RUNNABLE） 已进站，在候车大厅排队 调用了 start()，操作系统线程已建立，所有资源到位，等待 CPU 时间片 ✅ 随时可能被调度器挑中执行 DMA（direct memory access） 在没有DMA技术之前，I/O过程是这样的：\n用户进程发起read调用，切换用户态到内核态\nCPU发出I/O请求给磁盘控制器，然后返回\n磁盘控制器收到请求开始准备数据，把数据放到磁盘控制器的内部缓冲区，然后产生一个中断\nCPU收到中断信号，停下手头的工作，把磁盘控制器缓冲区的数据一次一个字节地读进自己的寄存器，然后再把寄存器里的数据写入到内存，而在数据传输的期间 CPU 是无法执行其他任务的\n由上可见整个数据传输过程中，都需要CPU亲自参与，期间cpu不能做其他事，非常拉低性能。于是有了DMA。\n传统的文件传输 进程文件传输，最简单的方式就是把磁盘文件读取出来，通过网络协议发出去。\n期间发生四次 用户态和内核态的切换，因为发生了**两次系统调用，一次是 read() ，一次是 write()，**每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。\n如何优化文件传输性能 在读取磁盘数据的时候之所以要发生上下文切换，是因为用户空间没有权限操作磁盘或网卡，需要切换到内核态来完成，而一次系统调用必然发生两次上下文切换：用户到内核、内核到用户，所以要减少上下文切换次数，就要减少系统调用次数！\n在前面我们知道了，传统的文件传输方式会历经4次数据拷贝，而且这里面，从内核\n的读缓冲区拷贝到用户的缓冲区里，再从用户的缓冲区里拷贝到socket的缓冲区里，\n这个过程是没有必要的。因为文件传输的应用场景中，在用户空间我们并不会对数据再加工，所以数据实际上可以不用搬运到用户空间，因此用户的缓冲区是没有必要存在的。\n如何实现零拷贝 mmap + write 在前面我们知道，read()系统调用的过程中会把内核缓冲区的数据拷贝到用户的缓冲区里，于是为了减少这一步开销，可以用mmap()替换read()系统调用函数。\nmmap() 系统调用函数会直接把内核缓冲区里的数据「映射」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。\n这种方式仍然是两次系统调用，四次上下文切换，只不过少了1次拷贝。\nsendfile linux内核2.1中的sendfile是专门发送文件的系统调用函数，仅有一次系统调用，可以代替read和write两次系统调用。该系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，这样就只有 2 次上下文切换，和 3 次数据拷贝。\nlinux 内核 2.4 ，在网卡支持SG-DMA的情况下，可以用sendfile直接把文件从内核缓冲区拷贝到网卡。这才是真正的零拷贝，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。\nPageCache 回顾前面说道文件传输过程，其中第一步都是先需要先把磁盘文件数据拷贝「内核缓冲\n区」里，这个「内核缓冲区」实际上是磁盘高速缓存(PageCache)。\nPageCache会缓存最近被访问的数据 （把磁盘数据读到缓存中），来提高读写性能。同时会进行预读，比如实际要从磁盘读取32kb数据，那么内核会把32-64kb的数据也提前读到缓存。\n","date":"2025-01-18T12:00:00Z","permalink":"/zh-cn/post/2025/01/%E7%94%A8%E6%88%B7%E6%80%81%E5%92%8C%E5%86%85%E6%A0%B8%E6%80%81%E8%BF%9B%E7%A8%8B%E5%8D%8F%E7%A8%8B%E5%8F%8A%E7%BA%BF%E7%A8%8B%E5%87%A0%E7%A7%8D%E7%8A%B6%E6%80%81dma%E9%9B%B6%E6%8B%B7%E8%B4%9D/","title":"用户态和内核态、进程、协程及线程几种状态、DMA、零拷贝"},{"content":" 往期推荐\nJava io模型-CSDN博客\n如何设计一个能根据任务优先级来执行的线程池-CSDN博客\nWeb实时消息推送的几种方案_setmessageinnerhtml is not defined-CSDN博客\nyum、dnf、apt包管理工具-CSDN博客\nJava反射、静态代理、动态代理-CSDN博客 在jdk1.5版本（包含）之前，锁的状态只有两种状态：无锁状态和重量级锁状态，只要有线程访问共享资源对象，则锁直接成为重量级锁，jdk1.6版本后，对synchronized锁进行了优化，新加了\u0026quot;偏向锁\u0026quot;和\u0026quot;轻量级锁\u0026quot;，用来减少上下文的切换以提高性能，所以锁就有了4种状态。\n无锁\n对于共享资源，不涉及多线程的竞争访问。在DK1.6之后偏向锁的默认开启的，但是有一个偏向延迟，需要在VM启动之后的多少秒之后才能开启，这个可以通过VM参数进行设置，同时是否开启偏向锁也可以通过VM参数设置。 偏向锁\n共享资源首次被访问时 ，JVM会对该共享资源对象做一些设置，比如将对象头中是否偏向锁标志位置为1，对象头中的线程ID设置为当前线程ID（注意：这里是操作系统的线程ID），后续当前线程再次访问这个共享资源时，会根据偏向锁标识跟线程ID进行比对是否相同，比对成功则直接获取到锁（锁偏向于这个线程），进入临界区域 （就是被锁保护，线程间只能串行访问的代码），这也是synchronized锁的可重入功能。 轻量级锁\n当多个线程同时申请共享资源锁的访问时，这就产生了竞争，JVM会先尝试使用轻量级锁，以CAS方式 来获取锁（一般就是自旋加锁，不阻塞线程采用循环等待 的方式），成功则获取到锁，状态为轻量级锁，轻量级锁竞争失败（达到一定的自旋次数还未成功）则锁升级到重量级锁。 重量级锁\n如果共享资源锁已经被某个线程持有，此时是偏向锁状态，**未释放锁前，再有其他线程来竞争时，则会升级到重量级锁，把竞争线程挂起，**重量级锁由操作系统来实现，所以性能消耗相对较高。 synchronized 和 volatile区别 synchronized 关键字和 volatile 关键字是两个互补的而非对立的\nvolatile 关键字是线程同步的轻量级 实现，所以 volatile性能肯定比synchronized关键字要好 。但是 volatile 关键字只能用于变量 而 synchronized 关键字可以修饰方法以及代码块 。 volatile 关键字能保证数据的可见性，但不能保证数据的原子性 。synchronized 关键字两者都能保证。 volatile关键字主要用于解决变量在多个线程之间的可见性，同时防止指令重排序。而 synchronized 关键字解决的是多个线程之间访问资源的同步性。 --- ","date":"2025-01-16T12:00:00Z","permalink":"/zh-cn/post/2025/01/synchronized%E9%94%81%E5%8D%87%E7%BA%A7/","title":"synchronized锁升级"},{"content":" 往期推荐\nJava io模型-CSDN博客\n如何设计一个能根据任务优先级来执行的线程池-CSDN博客\nWeb实时消息推送的几种方案_setmessageinnerhtml is not defined-CSDN博客\nyum、dnf、apt包管理工具-CSDN博客\n概述 反射机制是在运行状态中，对于任意一个类，都能够知道这个类中的所有属性和方法，对于任意一个对象，都能够调用它的任意一个方法和属性，这种动态获取的信息以及动态调用对象的方法的功能称为Java语言的反射机制。\nSpring、mybatis、动态代理、注解都是使用了反射。 优点：可以让咱们的代码更加灵活、为各种框架提供开箱即用的功能提供了便利。\n缺点：让我们在运行时有了分析操作类的能力，这同样也增加了安全问题。比如可以无视泛型参数的安全检查（泛型参数的安全检查发生在编译时）。另外，反射的性能也要稍差点，不过，对于框架来说实际是影响不大的。\n获取Class对象的4种方式 如果我们动态获取到这些信息，我们需要依靠 Class 对象。Class 类对象将一个类的方法、变量等信息告诉运行的程序。\n知道具体类名，直接类名.class 通过对象实例instance.getClass()获取 通过 Class.forName()传入类的全路径获取 通过类加载器xxxClassLoader.loadClass()传入类路径获取 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 //定义反射类 public class TargetObject { private String value; public TargetObject() { value = \u0026#34;QingQiu\u0026#34;; } public void publicMethod(String s) { System.out.println(\u0026#34;I love \u0026#34; + s); } private void privateMethod() { System.out.println(\u0026#34;value is \u0026#34; + value); } } //使用反射操作上面的类方法及属性 public class Main { public static void main(String[] args) throws ClassNotFoundException, NoSuchMethodException, IllegalAccessException, InstantiationException, InvocationTargetException, NoSuchFieldException { /** * 获取 TargetObject 类的 Class 对象并且创建 TargetObject 类实例 */ Class\u0026lt;?\u0026gt; targetClass = Class.forName(\u0026#34;com.serein.TargetObject\u0026#34;); TargetObject targetObject = (TargetObject) targetClass.newInstance(); /** * 获取 TargetObject 类中定义的所有方法 */ Method[] methods = targetClass.getDeclaredMethods(); for (Method method : methods) { System.out.println(method.getName()); } /** * 获取指定方法并调用 */ Method publicMethod = targetClass.getDeclaredMethod(\u0026#34;publicMethod\u0026#34;, String.class); publicMethod.invoke(targetObject, \u0026#34;QingQiu\u0026#34;); /** * 获取指定参数并对参数进行修改 */ Field field = targetClass.getDeclaredField(\u0026#34;value\u0026#34;); //为了对类中的参数进行修改我们取消安全检查 field.setAccessible(true); field.set(targetObject, \u0026#34;serein\u0026#34;); /** * 调用 private 方法 */ Method privateMethod = targetClass.getDeclaredMethod(\u0026#34;privateMethod\u0026#34;); //为了调用private方法我们取消安全检查 privateMethod.setAccessible(true); privateMethod.invoke(targetObject); } } 常见的反射应用场景 加载数据库驱动 ，Class.forName(com.mysql.cj.jdbc.Driver) 加载配置文件，Spring通过xml装载Bean的过程： 将xml配置文件加载入内存 java类里面解析xml的内容，得到对应实体类的字节码字符串以及相关的属性信息 使用反射机制，根据这个字符串获得某个类的Class实例动态配置实例的属性 代理 通过代理对象来代替对真实对象的访问，这样就可以在不修改原目标对象的前提下，提供额外的功能操作，扩展目标对象的功能。比喻：活动方要请明星出席，不会直接去找明星（真实对象），而是去找其经纪人（代理对象）\n静态代理 静态代理实现步骤 定义一个接口及其实现类 创建一个代理类同样实现这个接口 将目标对象注入进代理类，然后在代理类的对应方法调用目标类中的对应方法。这样的话，我们就可以通过代理类屏蔽对目标对象的访问，并且可以在目标方法执行前后做一些自己想做的事情。 代码演示 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 //定义发送短信的接口 public interface SmsService { String send(String message); } //实现发送短信的接口 public class SmsServiceImpl implements SmsService { public String send(String message) { System.out.println(\u0026#34;send message:\u0026#34; + message); return message; } } //创建代理类并同样实现发送短信的接口 public class SmsProxy implements SmsService { private final SmsService smsService; public SmsProxy(SmsService smsService) { this.smsService = smsService; } @Override public String send(String message) { //调用方法之前，我们可以添加自己的操作 System.out.println(\u0026#34;before method send()\u0026#34;); smsService.send(message); //调用方法之后，我们同样可以添加自己的操作 System.out.println(\u0026#34;after method send()\u0026#34;); return null; } } //实际使用 public class Main { public static void main(String[] args) { SmsService smsService = new SmsServiceImpl(); SmsProxy smsProxy = new SmsProxy(smsService); smsProxy.send(\u0026#34;java\u0026#34;); } } 动态代理 JDK动态代理 在 Java 动态代理机制中 InvocationHandler 接口和 Proxy 类是核心。\nProxy 类中使用频率最高的方法是：newProxyInstance() ，这个方法主要用来生成一个代理对象。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // 常用 // loader :类加载器，用于加载代理对象。 // interfaces : 被代理类实现的一些接口； // h : 实现了 InvocationHandler 接口的对象； public static Object newProxyInstance (ClassLoader loader, Class\u0026lt;?\u0026gt;[] interfaces, InvocationHandler h) //这个私有方法通常是在实现代理类时，由 JVM 或框架内部的机制调用，来处理更复杂的代理类生成逻辑。 private static Object newProxyInstance(Class\u0026lt;?\u0026gt; caller,Constructor\u0026lt;?\u0026gt; cons, InvocationHandler h) 要实现动态代理的话，还必须需要实现InvocationHandler 来自定义处理逻辑。 当我们的动态代理对象调用一个方法时，这个方法的调用就会被转发到实现InvocationHandler 接口类的 invoke 方法来调用。\n1 2 3 4 5 6 7 8 9 10 11 12 public interface InvocationHandler { /** * 当你使用代理对象调用方法的时候实际会调用到这个方法 */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable; // proxy :动态生成的代理类 // method : 与代理类对象调用的方法相对应 // args : 当前 method 方法的参数 } JDK动态代理实现步骤 定义一个接口及其实现类； 自定义 InvocationHandler 并重写invoke方法，在 invoke 方法中我们会调用原生方法（目标类的方法）并自定义一些处理逻辑； 通过 Proxy.newProxyInstance(ClassLoader loader,Class\u0026lt;?\u0026gt;[] interfaces,InvocationHandler h) 方法创建代理对象； 代码演示 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 //定义发送短信的接口 public interface SmsService { String send(String message); } //实现发送短信的接口 public class SmsServiceImpl implements SmsService { public String send(String message) { System.out.println(\u0026#34;send message:\u0026#34; + message); return message; } } //定义JDK动态代理类 public class DebugInvocationHandler implements InvocationHandler { /** * 代理类中的真实对象 */ private final Object target; public DebugInvocationHandler(Object target) { this.target = target; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws InvocationTargetException, IllegalAccessException { //调用方法之前，我们可以添加自己的操作 System.out.println(\u0026#34;before method \u0026#34; + method.getName()); //当我们的动态代理对象调用原生方法的时候，最终实际上调用到的是 invoke() 方法，然后 invoke() 方法代替我们去调用了被代理对象的原生方法。 Object result = method.invoke(target, args); //调用方法之后，我们同样可以添加自己的操作 System.out.println(\u0026#34;after method \u0026#34; + method.getName()); return result; } } //获取代理对象的工厂类 public class JdkProxyFactory { public static Object getProxy(Object target) { return Proxy.newProxyInstance( target.getClass().getClassLoader(), // 目标类的类加载器 target.getClass().getInterfaces(), // 代理需要实现的接口，可指定多个 new DebugInvocationHandler(target) // 代理对象对应的自定义 InvocationHandler ); } } //实际使用 SmsService smsService = (SmsService) JdkProxyFactory.getProxy(new SmsServiceImpl()); smsService.send(\u0026#34;java\u0026#34;); cglib动态代理 JDK动态代理通过实现接口实现代理，而CGLIB 通过继承方式实现代理。很多知名的开源框架都使用到了CGLIB， 例如 Spring 中的 AOP 模块中：如果目标对象实现了接口，则默认采用 JDK 动态代理，否则采用 CGLIB 动态代理。 在 CGLIB 动态代理机制中 MethodInterceptor 接口和 Enhancer 类是核心。\n自定义 MethodInterceptor 并重写 intercept 方法，intercept 用于拦截增强被代理类的方法。Enhancer类来动态获取被代理类，当代理类调用方法的时候，实际调用的是 MethodInterceptor 中的 intercept 方法。\n1 2 3 4 5 6 7 8 9 10 // obj : 被代理的对象（需要增强的对象） // method : 被拦截的方法（需要增强的方法） // args : 方法入参 // proxy : 用于调用原始方法 public interface MethodInterceptor extends Callback{ // 拦截被代理类中的方法 public Object intercept(Object obj, java.lang.reflect.Method method, Object[] args,MethodProxy proxy) throws Throwable; } CGLIB动态代理类使用步骤 定义一个类； 自定义 MethodInterceptor 并重写 intercept 方法，intercept 用于拦截增强被代理类的方法，和 JDK 动态代理中的 invoke 方法类似； 通过 Enhancer 类的 create()创建代理类； 代码演示 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;cglib\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;cglib\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; //实现一个使用阿里云发送短信的类 public class AliSmsService { public String send(String message) { System.out.println(\u0026#34;send message:\u0026#34; + message); return message; } } //自定义 MethodInterceptor（方法拦截器） public class DebugMethodInterceptor implements MethodInterceptor { /** * @param o 被代理的对象（需要增强的对象） * @param method 被拦截的方法（需要增强的方法） * @param args 方法入参 * @param methodProxy 用于调用原始方法 */ @Override public Object intercept(Object o, Method method, Object[] args, MethodProxy methodProxy) throws Throwable { //调用方法之前，我们可以添加自己的操作 System.out.println(\u0026#34;before method \u0026#34; + method.getName()); Object object = methodProxy.invokeSuper(o, args); //调用方法之后，我们同样可以添加自己的操作 System.out.println(\u0026#34;after method \u0026#34; + method.getName()); return object; } } //获取代理类 public class CglibProxyFactory { public static Object getProxy(Class\u0026lt;?\u0026gt; clazz) { // 创建动态代理增强类 Enhancer enhancer = new Enhancer(); // 设置类加载器 enhancer.setClassLoader(clazz.getClassLoader()); // 设置被代理类 enhancer.setSuperclass(clazz); // 设置方法拦截器 enhancer.setCallback(new DebugMethodInterceptor()); // 创建代理类 return enhancer.create(); } } //实际使用 AliSmsService aliSmsService = (AliSmsService) CglibProxyFactory.getProxy(AliSmsService.class); aliSmsService.send(\u0026#34;java\u0026#34;); 看到这里似乎发现cglib也是实现了一个接口重写intercept()来增强的，那么继承 体现在哪里呢？其实在使用 CGLIB 进行代理时，CGLIB 会通过继承目标类，生成一个新的子类 Target$Proxy，并重写目标类的非 final 方法。CGLIB 会在这些重写的方法中加入代理逻辑。\n静态代理和动态代理对比 灵活性：动态代理更加灵活，不需要必须实现接口，可以直接代理实现类（cglib），并且可以不需要针对每个目标类都创建一个代理类。另外，静态代理中，接口一旦新增加方法，目标对象和代理对象都要进行修改，这是非常麻烦的！ JVM 层面：静态代理在编译时就将接口、实现类、代理类这些都变成了一个个实际的 class 文件。而动态代理是在运行时动态生成类字节码，并加载到 JVM 中的。 JDK代理和CGLIB代理对比 JDK 动态代理只能代理实现了接口的类或者直接代理接口，而 CGLIB 可以代理未实现任何接口的类。 另外， CGLIB 动态代理是通过生成一个被代理类的子类来拦截被代理类的方法调用，因此不能代理声明为 final 类型的类和方法。 就二者的效率来说，大部分情况都是 JDK 动态代理更优秀，随着 JDK 版本的升级，这个优势更加明显 ","date":"2025-01-15T15:55:11Z","permalink":"/zh-cn/post/2025/01/java%E5%8F%8D%E5%B0%84%E9%9D%99%E6%80%81%E4%BB%A3%E7%90%86%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/","title":"Java反射、静态代理、动态代理"},{"content":"Red Hat 系列 rpm (Red Hat Package Manager) Red Hat 系列 Linux 发行版（如 CentOS、RHEL、Fedora）上的软件包管理。\n支持平台：Red Hat 系列 Linux（RHEL、CentOS、Fedora）。 依赖关系 ：不自动处理依赖关系，需要用户手动管理或借助其他工具（如 yum 或 dnf）。 功能 ：rpm 是一个低级 包管理工具，主要用于安装、查询、卸载和管理 .rpm 包。它本身不处理依赖关系，通常由其他工具（如 yum 或 dnf）解决 yum (Yellowdog Updater, Modified) Red Hat 系列 Linux 发行版（如 CentOS、RHEL、Fedora）的软件包管理。\n支持平台：Red Hat 系列 Linux（RHEL、CentOS、Fedora）。 依赖关系：自动处理软件包依赖。 功能 ：yum 是一个高级的包管理工具，可以从软件仓库中自动下载和安装软件包，自动处理依赖关系和升级。它是基于 RPM 包的管理工 ","date":"2025-01-13T12:00:00Z","permalink":"/zh-cn/post/2025/01/yumdnfapt%E5%8C%85%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/","title":"yum、dnf、apt包管理工具"},{"content":" 往期推荐\n如何设计一个能根据任务优先级来执行的线程池-CSDN博客\n【泛型擦除】通过反射向List中添加不同类型的元素-CSDN博客\nJava io模型-CSDN博客\nJava异常族谱-CSDN博客 消息推送一般分为 Web 端消息推送和移动端消息推送。，分为推和拉两种形式。\n短轮询 拉的方式，最简单的实现方式就是前端写个定时器，每隔一段时间向后台请求未读消息。然而如果推送数据不会频繁变更，无论后端此时是否有新的消息产生，客户端都会进行请求，势必会对服务端造成很大压力，浪费带宽和服务器资源。而如果数据频繁变更，比如：每10s请求一次配置，如果在第11s时配置更新了，那么推送将会延迟9s，等待下一次请求，又造成数据延迟。\n长轮询 长轮询是对上边短轮询的一种改进版本，在尽可能减少对服务器资源浪费的同时，保证消息的相对实时性。长轮询在中间件中应用的很广泛，比如 Nacos 和 Apollo 配置中心，消息队列 Kafka、RocketMQ 中都有用到长轮询。\n原理：客户端发起请求后，服务端不会立即返回请求结果，而是将请求挂起等待一段时间，如果此段时间内服务端数据变更，立即响应客户端请求，若是一直无变化则等到指定的超时时间后响应请求，客户端重新发起长链接。\niframe流 在页面中插入一个隐藏的\u0026lt;iframe\u0026gt;标签，通过在src中请求消息数量 API 接口，由此在服务端和客户端之间创建一条长连接，服务端持续向iframe传输数据。\n缺点：rame 流的服务器开销很大，还不如短轮询，而且 IE、Chrome 等浏览器一直会处于 loading 状态，很不推荐！！！\n这种方式实现简单，前端只要一个\u0026lt;iframe\u0026gt;标签搞定了\n1 \u0026lt;iframe src=\u0026#34;/iframe/message\u0026#34; style=\u0026#34;display:none\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt; 后端直接组装HTML、JS返回即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @Controller @RequestMapping(\u0026#34;/iframe\u0026#34;) public class IframeController { @GetMapping(path = \u0026#34;message\u0026#34;) public void message(HttpServletResponse response) throws IOException, InterruptedException { while (true) { response.setHeader(\u0026#34;Pragma\u0026#34;, \u0026#34;no-cache\u0026#34;); response.setDateHeader(\u0026#34;Expires\u0026#34;, 0); response.setHeader(\u0026#34;Cache-Control\u0026#34;, \u0026#34;no-cache,no-store\u0026#34;); response.setStatus(HttpServletResponse.SC_OK); response.getWriter().print(\u0026#34; \u0026lt;script type=\\\u0026#34;text/javascript\\\u0026#34;\u0026gt;\\n\u0026#34; + \u0026#34;parent.document.getElementById(\u0026#39;clock\u0026#39;).innerHTML = \\\u0026#34;\u0026#34; + count.get() + \u0026#34;\\\u0026#34;;\u0026#34; + \u0026#34;parent.document.getElementById(\u0026#39;count\u0026#39;).innerHTML = \\\u0026#34;\u0026#34; + count.get() + \u0026#34;\\\u0026#34;;\u0026#34; + \u0026#34;\u0026lt;/script\u0026gt;\u0026#34;); } } } SSE 服务端向客户端推送消息，其实除了可以用WebSocket（长连接、双向）这种耳熟能详的机制外，还有一种服务器发送事件(Server-Sent Events)，简称 SSE。这是一种服务器端到客户端(浏览器)的单向消息推送。 ChatGPT 就是采用的 SSE。对于需要长时间等待响应的对话场景，ChatGPT 采用了一种巧妙的策略：它会将已经计算出的数据\u0026quot;推送\u0026quot;给用户，并利用 SSE 技术在计算过程中持续返回数据。这样做的好处是可以避免用户因等待时间过长而选择关闭页面。 **SSE基于HTTP协议，**在服务器和客户端之间打开一个单向通道，服务端响应的不再是一次性的数据包而是text/event-stream类型的数据流信息，在有数据变更时从服务器流式传输到客户端，有点类似于在线视频播放，视频流会连续不断的推送到浏览器\n前端实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \u0026lt;script\u0026gt; let source = null; let userId = 7777 if (window.EventSource) { // 建立连接 source = new EventSource(\u0026#39;http://localhost:7777/sse/sub/\u0026#39;+userId); setMessageInnerHTML(\u0026#34;连接用户=\u0026#34; + userId); /** * 连接一旦建立，就会触发open事件 * 另一种写法：source.onopen = function (event) {} */ source.addEventListener(\u0026#39;open\u0026#39;, function (e) { setMessageInnerHTML(\u0026#34;建立连接。。。\u0026#34;); }, false); /** * 客户端收到服务器发来的数据 * 另一种写法：source.onmessage = function (event) {} */ source.addEventListener(\u0026#39;message\u0026#39;, function (e) { setMessageInnerHTML(e.data); }); } else { setMessageInnerHTML(\u0026#34;你的浏览器不支持SSE\u0026#34;); } \u0026lt;/script\u0026gt; 后端实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 private static Map\u0026lt;String, SseEmitter\u0026gt; sseEmitterMap = new ConcurrentHashMap\u0026lt;\u0026gt;(); /** * 创建连接 */ public static SseEmitter connect(String userId) { try { // 设置超时时间，0表示不过期。默认30秒 SseEmitter sseEmitter = new SseEmitter(0L); // 注册回调 sseEmitter.onCompletion(completionCallBack(userId)); sseEmitter.onError(errorCallBack(userId)); sseEmitter.onTimeout(timeoutCallBack(userId)); sseEmitterMap.put(userId, sseEmitter); count.getAndIncrement(); return sseEmitter; } catch (Exception e) { log.info(\u0026#34;创建新的sse连接异常，当前用户：{}\u0026#34;, userId); } return null; } /** * 给指定用户发送消息 */ public static void sendMessage(String userId, String message) { if (sseEmitterMap.containsKey(userId)) { try { sseEmitterMap.get(userId).send(message); } catch (IOException e) { log.error(\u0026#34;用户[{}]推送异常:{}\u0026#34;, userId, e.getMessage()); removeUser(userId); } } } WebSocket 基于TCP连接的双全工协议，工作流程如下：\n客户端向服务器发送一个 HTTP 请求，请求头中包含 Upgrade: websocket 和 Sec-WebSocket-Key 等字段，表示要求升级协议为 WebSocket； 服务器收到这个请求后，会进行升级协议的操作，如果支持 WebSocket，它将回复一个 HTTP 101 状态码，响应头中包含 ，Connection: Upgrade和 Sec-WebSocket-Accept: xxx 等字段、表示成功升级到 WebSocket 协议。 客户端和服务器之间建立了一个 WebSocket 连接 ，可以进行双向的数据传输，通过心跳机制 来保持 WebSocket 连接的稳定性和活跃性。数据以帧（frames）的形式进行传送，而不是传统的 HTTP 请求和响应。WebSocket 的每条消息可能会被切分成多个数据帧（最小单位）。发送端会将消息切割成多个帧发送给接收端，接收端接收消息帧，并将关联的帧重新组装成完整的消息。 **客户端或服务器可以主动发送一个关闭帧，表示要断开连接。**另一方收到后，也会回复一个关闭帧，然后双方关闭 TCP 连接。 MQTT 基于发布/订阅模式的轻量级通讯协议，通过订阅相应的主题来获取消息，是物联网中的一个标准传输协议。 TCP 协议位于传输层，MQTT 协议位于应用层，MQTT 协议构建于 TCP/IP 协议上，也就是说只要支持 TCP/IP 协议栈的地方，都可以使用 MQTT 协议。 ","date":"2025-01-09T12:00:00Z","permalink":"/zh-cn/post/2025/01/web%E5%AE%9E%E6%97%B6%E6%B6%88%E6%81%AF%E6%8E%A8%E9%80%81%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%A1%88/","title":"Web实时消息推送的几种方案"},{"content":" 不同的线程池会选用不同的阻塞队列作为任务队列，比如FixedThreadPool 使用的是LinkedBlockingQueue（有界队列），默认构造器初始的队列长度为 Integer.MAX_VALUE ，由于队列永远不会被放满，因此FixedThreadPool最多只能创建核心线程数的线程。 假如需要实现一个优先级任务线程池的话，那可以考虑使用 PriorityBlockingQueue （优先级阻塞队列）作为任务队列（ThreadPoolExecutor 的构造函数有一个 workQueue 参数可以传入任务队列）。 要想让 PriorityBlockingQueue 实现对任务的排序，传入其中的任务必须是具备排序能力的，方式有两种：\n实现 Comparable 接口 提交到线程池的任务实现 Comparable 接口，并重写 compareTo 方法来指定任务之间的优先级比较规则。\n缺点：1.任务类必须实现 Comparable 接口，硬编码不够灵活。2.如果需要多种优先级规则，任务类代码会变得复杂。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import java.util.concurrent.*; public class PriorityTask implements Runnable, Comparable\u0026lt;PriorityTask\u0026gt; { private final int priority; private final String name; public PriorityTask(int priority, String name) { this.priority = priority; this.name = name; } @Override public void run() { System.out.println(\u0026#34;Executing task: \u0026#34; + name + \u0026#34; with priority: \u0026#34; + priority); } @Override public int compareTo(PriorityTask other) { return Integer.compare(this.priority, other.priority); // 优先级值越小，优先级越高 } } public class PriorityThreadPoolExecutor extends ThreadPoolExecutor { public PriorityThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit) { super(corePoolSize, maximumPoolSize, keepAliveTime, unit, new PriorityBlockingQueue\u0026lt;Runnable\u0026gt;()); } } //使用示例 public class Main { public static void main(String[] args) { PriorityThreadPoolExecutor executor = new PriorityThreadPoolExecutor(2, 4, 1, TimeUnit.MINUTES); executor.execute(new PriorityTask(10, \u0026#34;Low priority task\u0026#34;)); executor.execute(new PriorityTask(1, \u0026#34;High priority task\u0026#34;)); executor.execute(new PriorityTask(5, \u0026#34;Medium priority task\u0026#34;)); executor.shutdown(); } } Comparator 创建 PriorityBlockingQueue 时传入一个 Comparator 对象来指定任务之间的排序规则(推荐)。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 import java.util.concurrent.*; public class Task implements Runnable { private final int priority; private final String name; public Task(int priority, String name) { this.priority = priority; this.name = name; } @Override public void run() { System.out.println(\u0026#34;Executing task: \u0026#34; + name + \u0026#34; with priority: \u0026#34; + priority); } public int getPriority() { return priority; } } public class PriorityThreadPoolExecutor extends ThreadPoolExecutor { public PriorityThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit) { super(corePoolSize, maximumPoolSize, keepAliveTime, unit, new PriorityBlockingQueue\u0026lt;\u0026gt;(11, Comparator.comparingInt(Task::getPriority))); } } //使用示例 public class Main { public static void main(String[] args) { PriorityThreadPoolExecutor executor = new PriorityThreadPoolExecutor(2, 4, 1, TimeUnit.MINUTES); executor.execute(new Task(10, \u0026#34;Low priority task\u0026#34;)); executor.execute(new Task(1, \u0026#34;High priority task\u0026#34;)); executor.execute(new Task(5, \u0026#34;Medium priority task\u0026#34;)); executor.shutdown(); } } ","date":"2025-01-08T11:45:00Z","permalink":"/zh-cn/post/2025/01/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%83%BD%E6%A0%B9%E6%8D%AE%E4%BB%BB%E5%8A%A1%E4%BC%98%E5%85%88%E7%BA%A7%E6%9D%A5%E6%89%A7%E8%A1%8C%E7%9A%84%E7%BA%BF%E7%A8%8B%E6%B1%A0/","title":"如何设计一个能根据任务优先级来执行的线程池"},{"content":"\n为了保证操作系统的稳定性和安全性，一个进程的地址空间划分为 用户空间（User space） 和 内核空间（Kernel space ） 。\n像我们平常运行的应用程序都是运行在用户空间，只有内核空间才能进行系统态级别的资源有关的操作，比如文件管理、进程通信、内存管理等等。也就是说，我们想要进行 IO 操作，一定是要依赖内核空间的能力。并且，用户空间的程序不能直接访问内核空间。当想要执行 IO 操作时，由于没有执行这些操作的权限，只能发起系统调用请求操作系统帮忙完成。\n因此，用户进程想要执行 IO 操作的话，必须通过 系统调用 来间接访问内核空间\n我们在平常开发过程中接触最多的就是 磁盘 IO（读写文件） 和 网络 IO（网络请求和响应）。\n从应用程序的视角来看的话，我们的应用程序对操作系统的内核发起 IO 调用（系统调用），操作系统负责的内核执行具体的 IO 操作。也就是说，我们的应用程序实际上只是发起了 IO 操作的调用而已，具体 IO 的执行是由操作系统的内核来完成的。\n同步/异步、阻塞/非阻塞 同步/异步关注的是任务执行的顺序性 ，阻塞/非阻塞关注的是线程是否被挂起，有无回调。\nJava常见IO模型 同步阻塞BIO 同步阻塞 IO 模型中，应用程序发起 read 调用后，会一直阻塞，直到内核把数据拷贝到用户空间。\n同步非阻塞NIO jdk4引入，同步非阻塞 IO 模型中，应用程序会一直发起 read 调用，等待数据从内核空间拷贝到用户空间的这段时间里，线程依然是阻塞的，直到在内核把数据拷贝到用户空间。\n但是，这种 IO 模型同样存在问题：应用程序不断进行 I/O 系统调用轮询数据是否已经准备好的过程是十分消耗 CPU 资源的。\nIO多路复用 IO 多路复用模型中，线程首先发起 select 调用 ，询问内核数据是否准备就绪，等内核把数据准备好了，用户线程再发起 read 调用 。read 调用的过程（数据从内核空间 -\u0026gt; 用户空间）还是阻塞的 。IO多路复用减少无效的系统调用，减少了对 CPU 资源的消耗。\nJava 中的 NIO ，有一个非常重要的选择器 ( Selector ) 的概念，也可以被称为 多路复用器。通过它，只需要一个线程便可以管理多个客户端连接。当客户端数据到了之后，才会为其服务。\n注意！ 使用 NIO 并不一定意味着高性能，它的性能优势主要体现在高并发和高延迟的网络环境下。当连接数较少、并发程度较低或者网络传输速度较快时，NIO 的性能并不一定优于传统的 BIO 。\n到此我们发现，上述几种都不是异步IO，他们区别在于发起read请求时是否阻塞，但是在从内核空间拷贝数据到用户空间的过程中始终是阻塞的。\nBuffer 在传统的 BIO 中，数据的读写是面向流的， 分为字节流和字符流。在 Java 1.4 的 NIO 库中，所有数据都是用缓冲区处理的\nChannel channel是一个双向通道（双全工），可以同时用于读写，而inputStream流和outputStream流则是单向的\nSelector Selector（选择器） 它允许一个线程处理多个 Channel。Selector 是基于事件驱动的 I/O 多路复用模型。\n主要运作原理是：通过 Selector 注册通道的事件，Selector 会不断地轮询注册在其上的 Channel。当事件发生时，比如：某个 Channel 上面有新的 TCP 连接接入、读和写事件，这个 Channel 就处于就绪状态 ，会被 Selector 轮询出来。Selector 会将相关的 Channel 加入到就绪集合中。通过 SelectionKey 可以获取就绪 Channel 的集合，然后对这些就绪的 Channel 进行相应的 I/O 操作。\nJava NIO 核心知识总结 | JavaGuide\n零拷贝NIO 零拷贝是提升 IO 操作性能的一个常用手段，指计算机执行 IO 操作时，CPU 不需要将数据从一个存储区域复制到另一个存储区域，从而可以减少上下文切换以及 CPU 的拷贝时间。 也就是说，零拷贝主要解决操作系统在处理 I/O 操作时频繁复制数据的问题。零拷贝的常见实现技术有： mmap+write、sendfile和 sendfile + DMA gather copy 。\nJava 对零拷贝的支持：\nMappedByteBuffer 是 NIO 基于内存映射 （mmap）这种零拷⻉⽅式的提供的⼀种实现，底层实际是调用了 Linux 内核的 mmap 系统调用。它可以将一个文件或者文件的一部分映射到内存中，形成一个虚拟内存文件，这样就可以直接操作内存中的数据，而不需要通过系统调用来读写文件。 FileChannel 的transferTo()/transferFrom()是 NIO 基于发送文件（sendfile）这种零拷贝方式的提供的一种实现，底层实际是调用了 Linux 内核的 sendfile系统调用。它可以直接将文件数据从磁盘发送到网络，而不需要经过用户空间的缓冲区。 异步AIO AIO 也就是 NIO 2。Java 7 中引入了 NIO 的**改进版 NIO 2,**它是异步 IO 模型。\n异步 IO 是基于事件和回调机制实现的，也就是应用发起read操作之后会直接返回，不会堵塞在那里，当内核后台完成数据拷贝，会执行回调函数来通知应用。\n--- ","date":"2025-01-07T13:26:47Z","permalink":"/zh-cn/post/2025/01/java-io%E6%A8%A1%E5%9E%8B----bionioaioio%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%E9%9B%B6%E6%8B%B7%E8%B4%9D/","title":"Java io模型----BIO、NIO、AIO、IO多路复用、零拷贝"},{"content":"\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 ArrayList\u0026lt;String\u0026gt; list= new ArrayList\u0026lt;String\u0026gt;(); //泛型擦除,输出true System.out.println(strings.getClass().equals(integers.getClass())); list.add(\u0026#34;xxx\u0026#34;); //编译时错误，无法通过编译 //strings.add(18); Class\u0026lt;? extends ArrayList\u0026gt; clazz= list.getClass(); Method add = clazz.getMethod(\u0026#34;add\u0026#34;, Object.class); //运行时可以通过 add.invoke(list,17); //输出 [xxx,17] System.out.println(strings.toString()); --- ","date":"2025-01-04T19:31:55Z","permalink":"/zh-cn/post/2025/01/%E6%B3%9B%E5%9E%8B%E6%93%A6%E9%99%A4%E9%80%9A%E8%BF%87%E5%8F%8D%E5%B0%84%E5%90%91list%E4%B8%AD%E6%B7%BB%E5%8A%A0%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%85%83%E7%B4%A0/","title":"【泛型擦除】通过反射向List中添加不同类型的元素"},{"content":"概览 在 Exception 异常中，大致又分为checked Exception（非运行时异常、受检异常）和 unchecked Exception（运行时异常、非受检异常）\n受检异常\n在程序编译阶段就要被检查出来并处理掉的（try~catch 或者在方法外面 throws），否则程序无法通过编译，比如 ClassNotFound、FileNotFoundException。\n非受检异常\n可以通过编译，但是会在程序运行过程中爆露出来，比如常见的空指针、参数错误、类型转换错误 。\ntry-catch-finally try块：用于捕获异常。其后可接零个或多个 catch 块，如果没有 catch 块，则必须跟一个 finally 块。 catch块：用于处理 try 捕获到的异常。 finally 块：无论是否捕获或处理异常，finally 块里的语句都会被执行。当在 try 块或 catch 块中遇到 return 语句时，finally 语句块将在方法 return 之前被执行。 finally 的代码在某些情况下也不是必须执行的，比如 JVM 终止、线程死亡。 throws 和 throw throws：用于在方法声明中声明可能抛出的异常类型。如果一个方法可能抛出异常，但不想在方法内部进行处理，可以使用 throws 关键字将异常传递（上抛）给调用者来处理，直至被 catch 或者抛给前端用户。 throw：手动抛出异常。可以根据需要在代码中使用 throw 语句主动抛出特定类型的异常比如自定义异常。 ","date":"2024-12-31T12:00:00Z","permalink":"/zh-cn/post/2024/12/java%E5%BC%82%E5%B8%B8%E6%97%8F%E8%B0%B1%E8%BF%90%E8%A1%8C%E6%97%B6%E5%BC%82%E5%B8%B8%E9%9D%9E%E8%BF%90%E8%A1%8C%E6%97%B6%E5%BC%82%E5%B8%B8error/","title":"Java异常族谱、运行时异常、非运行时异常、ERROR"},{"content":" 编译型： 通过编译器将源代码一次性翻译成 可被该平台执行的机器码。一般情况下，编译语言的执行速度比较快，跨平台性差。常见的编译性语言有 C、C++、Go、Rust 等等。 解释型： 会通过解释器一句一句的将代码解释（interpret）为机器代码后再执行。解释型语言执行速度比较慢。常见的解释性语言有 Python、JavaScript、PHP 等等。 为什么说 Java 语言\u0026quot;编译与解释并存\u0026quot;？\n由 Java 编写的程序需要先经过编译步骤，生成字节码（.class 文件），这种字节码必须由 Java 解释器来解释执行。\n常用的 java 编译模式是JIT（即时编译）即动态编译，在程序执行期间编译，因此支持反射、动态代理等特性。（Spring 生态很多就依赖的 java 的代理、反射特性） JDK 9 引入了一种新的编译模式 AOT 。这种编译模式会在程序被执行前就将其编译成机器码 ，属于静态编译（C、 C++，Rust，Go 等语言就是静态编译）。AOT 避免了 JIT 预热等各方面的开销，可以提高 Java 程序的启动速度，避免预热时间长。并且，AOT 还能减少内存占用和增强 Java 程序的安全性（AOT 编译后的代码不容易被反编译和修改），特别适合云原生场景。 ","date":"2024-12-31T12:00:00Z","permalink":"/zh-cn/post/2024/12/%E7%BC%96%E8%AF%91%E4%B8%8E%E8%A7%A3%E9%87%8A%E5%B9%B6%E5%AD%98%E7%9A%84java/","title":"编译与解释并存的Java"},{"content":" Java 泛型（Generics） 是 JDK 5 中引入的一个新特性。可以在编译时提供更强的类型检查 ，并且在编译后能够保留类型信息，避免了在运行时出现类型转换异常。\n泛型 3 种用法 泛型一般有三种使用方式:泛型类 、泛型接口 、泛型方法。\n泛型类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 //此处T可以随便写为任意标识，常见的如T、E、K、V等形式的参数常用于表示泛型 //在实例化泛型类时，必须指定T的具体类型 public class Generic\u0026lt;T\u0026gt;{ private T key; public Generic(T key) { this.key = key; } public T getKey(){ return key; } } //实例化泛型类 Generic\u0026lt;Integer\u0026gt; genericInteger = new Generic\u0026lt;Integer\u0026gt;(123456); 泛型接口 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 //泛型接口 public interface Generator\u0026lt;T\u0026gt; { public T method(); } //实现接口，不指定类型 class GeneratorImpl\u0026lt;T\u0026gt; implements Generator\u0026lt;T\u0026gt;{ @Override public T method() { return null; } } //实现泛型接口，指定类型 class GeneratorImpl implements Generator\u0026lt;String\u0026gt; { @Override public String method() { return \u0026#34;hello\u0026#34;; } } 泛型方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 //泛型方法 public static \u0026lt; E \u0026gt; void printArray( E[] inputArray ) { for ( E element : inputArray ){ System.out.printf( \u0026#34;%s \u0026#34;, element ); } System.out.println(); } //使用 //创建不同类型数组：Integer, Double 和 Character Integer[] intArray = { 1, 2, 3 }; String[] stringArray = { \u0026#34;Hello\u0026#34;, \u0026#34;World\u0026#34; }; printArray( intArray ); printArray( stringArray ); 案例：通用结果返回类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 public class BaseResponse\u0026lt;T\u0026gt; implements Serializable { private int code; private T data; private String message; public BaseResponse(int code, T data, String message) { this.code = code; this.data = data; this.message = message; } public BaseResponse(int code, T data) { this(code, data, \u0026#34;\u0026#34;); } public BaseResponse(ErrorCode errorCode) { this(errorCode.getCode(), null, errorCode.getMessage()); } } public class ResultUtil { /** * 成功 * * @param data * @param \u0026lt;T\u0026gt; * @return */ public static \u0026lt;T\u0026gt; BaseResponse\u0026lt;T\u0026gt; success(T data) { return new BaseResponse\u0026lt;\u0026gt;(200, data, \u0026#34;ok\u0026#34;); } /** * 失败 * * @param errorCode * @return */ public static BaseResponse error(ErrorCode errorCode) { return new BaseResponse\u0026lt;\u0026gt;(errorCode); } /** * 失败 * * @param errorCode * @return */ public static BaseResponse error(ErrorCode errorCode, String message) { return new BaseResponse(errorCode.getCode(), null, message); } } ","date":"2024-12-31T11:00:00Z","permalink":"/zh-cn/post/2024/12/java%E6%B3%9B%E5%9E%8B/","title":"Java泛型"},{"content":" String.intern() 是一个 native (本地) 方法，用来处理字符串常量池中的字符串对象引用。它的工作流程可以概括为以下两种情况：\n常量池中已有相同内容的字符串对象 ：如果字符串常量池中已经有一个与调用 intern() 方法的字符串内容相同的 String 对象，intern() 方法会直接返回常量池中该对象的引用。 常量池中没有相同内容的字符串对象 ：如果字符串常量池中还没有一个与调用 intern() 方法的字符串内容相同的对象，intern() 方法会将当前字符串对象的引用添加到字符串常量池中，并返回该引用。 总结 intern() 方法的主要作用是确保字符串引用在常量池中的唯一性。 当调用 intern() 时，如果常量池中已经存在相同内容的字符串，则返回常量池中已有对象的引用；否则，将该字符串添加到常量池并返回其引用。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // s1 指向字符串常量池中的 \u0026#34;Java\u0026#34; 对象 String s1 = \u0026#34;Java\u0026#34;; // s2 也指向字符串常量池中的 \u0026#34;Java\u0026#34; 对象，和 s1 是同一个对象 String s2 = s1.intern(); // 在堆中创建一个新的 \u0026#34;Java\u0026#34; 对象，s3 指向它 String s3 = new String(\u0026#34;Java\u0026#34;); // s4 指向字符串常量池中的 \u0026#34;Java\u0026#34; 对象，和 s1 是同一个对象 String s4 = s3.intern(); // s1 和 s2 指向的是同一个常量池中的对象 System.out.println(s1 == s2); // true // s3 指向堆中的对象，s4 指向常量池中的对象，所以不同 System.out.println(s3 == s4); // false // s1 和 s4 都指向常量池中的同一个对象 System.out.println(s1 == s4); // true ","date":"2024-12-30T12:00:00Z","permalink":"/zh-cn/post/2024/12/string.intern/","title":"String.intern()"},{"content":"==和 equals 区别 ==用于基本数据类型，比较的是值，用于引用类型，比较的是对象的内存地址。 java 中只有值传递，因此对于引用类型，实际比较的引用的内存地址的值。 equals 不能用来判断基本数据类型，只能判断引用数据类型，判断两个对象是否相等。 1 2 3 4 5 6 7 8 String a = new String(\u0026#34;ab\u0026#34;); // a 为一个引用 String b = new String(\u0026#34;ab\u0026#34;); // b为另一个引用,对象的内容一样 String aa = \u0026#34;ab\u0026#34;; // 放在常量池中 String bb = \u0026#34;ab\u0026#34;; // 从常量池中查找 System.out.println(aa == bb);// true System.out.println(a == b);// false System.out.println(a.equals(b));// true System.out.println(42 == 42.0);// true hashcode 把输入的值经过哈希算法得出哈希码，即散列码，用来确定对象在哈希表的索引位置。 hashCode() 定义在 JDK 的 Object 类中，这就意味着 Java 中的任何类都包含有 hashCode() 函数。另外需要注意的是：Object 的 hashCode() 方法是本地方法，也就是用 C 语言或 C++ 实现的。 为什么 JDK 同时提供hashCode() 和 equals()方法 hashCode() 和 equals()都是用于比较两个对象是否相等。\n为什么 JDK 还要同时提供这两个方法呢？\n这是因为在一些容器（比如 HashMap、HashSet）中，有了 hashCode() 之后，判断元素是否在对应容器中的效率会更高（参考添加元素进HashSet的过程）！\n我们在前面也提到了添加元素进HashSet的过程，如果 HashSet 在对比的时候，同样的 hashCode 有多个对象，它会继续使用 equals() 来判断是否真的相同。也就是说 hashCode 帮助我们大大缩小了查找成本。 为什么不只提供 hashCode() 方法呢？\n这是因为两个对象的hashCode 值相等并不代表两个对象就相等。 那为什么两个对象有相同的 hashCode 值，它们也不一定是相等的？\n因为 hashCode() 所使用的哈希算法也许刚好会让多个对象传回相同的哈希值。越糟糕的哈希算法越容易碰撞，但这也与数据值域分布的特性有关（所谓哈希碰撞也就是指的是不同的对象得到相同的 hashCode )。 ","date":"2024-12-29T15:23:14Z","permalink":"/zh-cn/post/2024/12/equalshashcode/","title":"==、equals、hashcode"},{"content":"为何 String 不可变 java8 之前，String 的源码中是用字符数组实现的，同时使用了 final 和 private 修饰，被 final 修饰的结果就是变量不可修改、类不可继承、方法不可重写，被 private 修饰就无法对外暴露，这就是为何 String 不可变。 java8 之后，String 改成了用字节数组实现。 新版的 String 其实支持两个编码方案：Latin-1 和 UTF-16。如果字符串中包含的汉字没有超过 Latin-1 可表示范围内的字符，那就会使用 Latin-1 作为编码方案。Latin-1 编码方案下，byte 占一个字节(8 位)，char 占用 2 个字节（16），byte 相较 char 节省一半的内存空间。JDK 官方就说了绝大部分字符串对象只包含 Latin-1 可表示的字符。 String 的运算符重载 Java 语言本身并不支持运算符重载，\u0026quot;+\u0026ldquo;和\u0026rdquo;+=\u0026ldquo;是专门为 String 类重载过的运算符，也是 Java 中仅有的两个重载过的运算符。\n1 2 3 4 String str1 = \u0026#34;he\u0026#34;; String str2 = \u0026#34;llo\u0026#34;; String str3 = \u0026#34;world\u0026#34;; String str4 = str1 + str2 + str3; 可以看出，字符串对象通过\u0026rdquo;+\u0026ldquo;的字符串拼接方式，实际上是通过 StringBuilder 调用 append() 方法实现的，拼接完成之后调用 toString() 得到一个 String 对象 。\n不过，在循环内使用\u0026rdquo;+\u0026ldquo;进行字符串的拼接的话，存在比较明显的缺陷：编译器不会创建单个 StringBuilder 以复用， 意味着每循环一次就会创建一个 StringBuilder 对象，会导致创建过多的 StringBuilder 对象。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 //在循环内使用+号每次都会创建新的StringBuilder String[] arr = {\u0026#34;he\u0026#34;, \u0026#34;llo\u0026#34;, \u0026#34;world\u0026#34;}; String s = \u0026#34;\u0026#34;; for (int i = 0; i \u0026lt; arr.length; i++) { s += arr[i]; } System.out.println(s); //避免了上述问题 String[] arr = {\u0026#34;he\u0026#34;, \u0026#34;llo\u0026#34;, \u0026#34;world\u0026#34;}; StringBuilder s = new StringBuilder(); for (String value : arr) { s.append(value); } System.out.println(s); ","date":"2024-12-29T14:38:34Z","permalink":"/zh-cn/post/2024/12/%E4%B8%BA%E4%BD%95string%E4%B8%8D%E5%8F%AF%E5%8F%98string%E7%9A%84%E8%BF%90%E7%AE%97%E7%AC%A6%E9%87%8D%E8%BD%BD/","title":"为何String不可变，String的运算符重载"},{"content":" 何为事务？ 一言蔽之，事务是逻辑上的一组操作，要么都执行，要么都不执行。 \u0026gt; 原子性 （Atomicity）：事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性 （Consistency）：执行事务前后，数据保持一致，例如转账业务中，无论事务是否成功，转账者和收款人的总额应该是不变的； 隔离性 （Isolation）：并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性 （Durability）：一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 只有保证了事务的持久性、原子性、隔离性之后，一致性才能得到保障。也就是说 A、I、D 是手段，C 是目的！\n周志明的软件架构课软件架构分布式系统基础设施架构演进单体架构_SOA 架构微服务_云原生-极客时间\n并发事务的问题 丢失修改： 在事务 1 读取一个数据时，另外事务 2 也访问了该数据，那么在事务 1 中修改了这个数据后，事务 2 也修改了这个数据。这样事务 1 的修改结果就被丢失，因此称为丢失修改。 **例：**事务 1 读取某表中的数据 A=20，事务 2 也读取 A=20。事务 1 先修改 A=A-1，事务 2 后来也修改 A=A-1，最终结果 A=19，事务 1 的修改被丢失 脏读： 事务 2 读取了事务 1 未提交的数据 **例：**事务 1 读取某表中的数据 A=20 并修改 A=A-1，事务 2 读取到 A = 19,事务 1 回滚导致对 A 的修改并未提交到数据库， A 的值还是 20 不可重复读： 事务 1 多次读同一数据。在事务 1 还没有结束时，事务 2 也访问该数据。那么，在事务 1 中的两次读数据之间，由于事务 2 的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。 **例：**事务 1 读取某表中的数据 A=20，事务 2 也读取 A=20，事务 1 修改 A=A-1，事务 2 再次读取 A =19，此时读取的结果和第一次读取的结果不同 幻读： 幻读与不可重复读类似。它发生在一个事务读取了几行数据，接着另一个并发事务插入了一些数据时。在随后的查询中，第一个事务就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。 **例：**事务 2 读取某个范围的数据，事务 1 在这个范围插入了新的数据，事务 2 再次读取这个范围的数据发现相比于第一次读取的结果多了新的数据 并发事务控制方式 MySQL 中并发事务的控制方式无非就两种：锁 和 MVCC。锁可以看作是悲观控制的模式，多版本并发控制可以看作是乐观控制的模式。\nInnoDB 存储引擎对 MVCC 的实现 | JavaGuide\n锁 控制方式下会通过锁来显式控制共享资源而不是通过调度手段，MySQL 中主要是通过 读写锁 来实现并发控制。\n共享锁（S 锁）：又称读锁，事务在读取记录的时候获取共享锁，允许多个事务同时获取（锁兼容）。 排他锁（X 锁）：又称写锁/独占锁，事务在修改记录的时候获取排他锁，不允许多个事务同时获取。如果一个记录已经被加了排他锁，那其他事务不能再对这条记录加任何类型的锁（锁不兼容）。 MVCC 是多版本并发控制方法，即对一份数据会存储多个版本，通过事务的可见性来保证事务能看到自己应该看到的版本。通常会有一个全局的版本分配器来为每一行数据设置版本号，版本号是唯一的。 MVCC 在 MySQL 中实现所依赖的手段主要是: 隐藏字段、read view、undo log。\n四个事务隔离级别 Innodb 中的事务隔离级别和锁的关系 - 美团技术团队\nMySQL 的隔离级别基于锁和 MVCC 机制共同实现的。 读已提交和 可重复读 隔离级别是基于 MVCC 实现的，可串行化 隔离级别是通过锁来实现的。 读未提交 (Read Uncommitted) 特点：在该隔离级别下，一个事务可以读取另一个事务尚未提交的数据。这就会导致脏读，即一个事务读取到的可能是另一个事务未提交的修改，这些修改可能会被回滚。 问题：事务 B 读取到事务 A 修改的值，但事务 A 回滚后，数据实际上没有变化。发生了脏读。 读已提交 (Read Committed) 特点：在该隔离级别下，一个事务只能读取到已提交事务的数据，因此脏读不会发生。\n问题：事务 A 在读取两次同一数据时，第二次读取的数据可能会因其他事务的提交而发生变化 ，这就是不可重复读 。\n「读已提交」事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致 ，因为可能这期间另外一个事务修改了该记录，并提交了事务。这就是不可重复读的问题\n可重复读 (Repeatable Read) 特点：在该隔离级别下，事务会看到在事务开始时一致的数据快照，不可重复读的问题被解决。 问题：如果事务 A 查询范围的数据（例如某一时间段内的所有交易记录），事务 B插入 的新数据可能会被事务 A 在第二次查询中看到，导致事务 A 两次查询到的数据量不一致，这就是幻读，可重复读仅解决部分幻读。 用 Read View 只能保证\u0026quot;快照读\u0026quot;不幻读；一旦事务里出现\u0026quot;当前读\u0026quot;（UPDATE/DELETE/SELECT \u0026hellip; FOR UPDATE 等），就会重新加 Next-Key Lock，这时若别的事务新插入的记录落在这个锁范围里，就可能被本事务再次看到，于是出现\u0026quot;部分幻读\u0026quot;。 「可重复读」隔离级别是启动事务时生成一个 Read View，然后整个事务期间都在用 这个 Read View ，这样就保证了在事务期间读到的数据都是事务启动前的记录。解决了不可重复读和部分幻读。 串行化 (Serializable) 特点：在该隔离级别下，事务是完全隔离、串行执行。其他事务必须等当前事务完成才能开始执行，这避免了所有并发问题（脏读、不可重复读和幻读），但也大大降低了性能。 幻读 幻读 VS 不可重复读\n幻读重点在于数据是否存在。原本不存在的数据却真实的存在了，这便是幻读。引起幻读的原因在于另一个事务进行了 INSERT 操作。 不可重复读重点在于数据值是否被改变。在一个事务中对同一条记录进行查询，第一次读取到的数据和第二次读取到的数据不一致，这便是可重复读。引起不可重复读的原因在于另一个事务进行了 UPDATE 或者是 DELETE 操作。 简单来说：幻读是说数据的条数发生了变化，原本不存在的数据存在了。不可重复读是说数据的内容发生了变化，原本存在的数据的内容发生了改变。\n可重复读隔离下为什么会产生幻读\n在可重复读隔离级别下，普通的查询是快照读 ，是不会看到别的事务插入的数据的，就没有幻读 。因此，幻读在 当前读（ select \u0026hellip; for update 等语句，使用临键锁**）** 下才会出现。\nMySQL 里除了普通查询是快照读，其他都是当前读，比如 update、insert、delete,这些语句执行前都会查询最新版本的数据，然后再做进一步的操作。\nMySQL 中如何实现可重复读\n当隔离级别为可重复读的时候，事务只在第一次 SELECT 的时候会获取一次 Read View，而后面所有的 SELECT 都会复用这个 Read View。也就是说：不管其他事务怎么修改数据，，对于 A 事务而言，它能看到的数据永远都是第一次 SELECT 时看到的数据。这显然不合理，如果其它事务插入了数据，A 事务却只能看到过去的数据，读取不了当前的数据。\n解决幻读的办法\nMySQL 可重复读隔离级别，完全解决幻读了吗？ | 小林 coding{#解决幻读的方法}\n**解决幻读的核心思想就是事务 A 在操作某张表数据的时候，另外事务 B 不允许新增或者删除这张表中的数据。**解决幻读的方式主要有以下几种：\n将事务隔离级别调整为 SERIALIZABLE 。\n在可重复读的事务级别下，给事务操作的这张表添加表锁。\n在可重复读的事务级别下，给事务操作的这张表添加 临键锁。\n其他情况下的幻读\n在可重复读隔离级别下，事务 A 第一次执行普通的 select 语句时生成一个数据快照，之后事务 B 向表中新插入了一条 id=5 的记录并提交（此时是当前读）。接着，事务 A 对 id=5 这条记录进行了更新操作（看不见但是能更新），在这个时刻这条新记录的 trx id 隐藏列的值就变成了事务 A 的事务 id，之后事务 A 再使用普通 select 语句去查询这条记录时就可以看到这条记录了，于是就发生了幻读。\n事务 A 先快照读，得到数据量为 3，然后事务 B 插入一条数据并提交事务，事务 A 使用当前读得到的数量就是 4 了，前后数据量不对。\n要避免这类特殊场景下发生幻读的现象的话，就是尽量在开启事务之后，马上执行 select for update 这类当前读的语句，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录。\n","date":"2024-12-27T12:00:00Z","permalink":"/zh-cn/post/2024/12/mysql%E5%9B%9B%E7%A7%8D%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E6%9C%BA%E5%88%B6/","title":"MySQL四种事务隔离机制"},{"content":"Java 基本数据类型的包装类型的大部分都用到了缓存机制来提升性能。\nByte,Short,Integer,Long这 4 种包装类默认创建了数值 [-128，127] 的相应类型的缓存数据，Character 创建了数值在 [0,127] 范围的缓存数据，Boolean 直接返回 True or False。\n果超出对应范围仍然会去创建新的对象，缓存的范围区间的大小只是在性能和资源之间的权衡。\n两种浮点数类型的包装类 Float,Double 并没有实现缓存机制。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 public static void main( String[] args ) { Integer i1 = 128; Integer i2 = 128; System.out.println(i1 == i2);// 输出 false Integer i3 = 33; Integer i4 = 33; System.out.println(i3 == i4);// 输出 true Float i11 = 333f; Float i22 = 333f; System.out.println(i11 == i22);// 输出 false Double i5 = 1.2; Double i6 = 1.2; System.out.println(i6 == i5);// 输出 false Integer i7 = 40; Integer i8 = new Integer(40); System.out.println(i7==i8);//输出false，因为i7直接用的缓存，i8则是创建的对象，存在堆 } 装箱其实就是调用了 包装类的valueOf()方法，拆箱其实就是调用了 xxxValue()方法。\n1 2 Integer i = 10 //等价于 Integer i = Integer.valueOf(10) int n = i //等价于 int n = i.intValue(); ","date":"2024-12-26T22:17:09Z","permalink":"/zh-cn/post/2024/12/java%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B%E7%9A%84%E7%BC%93%E5%AD%98/","title":"Java包装类型的缓存"},{"content":" MySQL 是建立在关系模型基础上的关系型数据库，关系模型表明了数据库中所存储的数据之间的联系（一对一、一对多、多对多）。\n字段类型 TINYINT 占 1 字节，INT 占 4 字节，BIGINT 占 8 字节。 CHAR 是定长字符串，VARCHAR 是变长字符串。CHAR 在存储时会在右边填充空格以达到指定的长度，检索时会去掉空格；VARCHAR 在存储时需要使用 1 或 2 个额外字节 记录字符串的长度，检索时不需要处理。CHAR（100）指的是 100 个字符而非字节 MySQL 中字符的存储是与字符集（CHARSET）相关的，具体的字节数取决于你使用的字符集和编码。 latin1 编码（单字节编码），每个字符占用 1 字节 。utf8mb4 编码（变长编码），每个字符可能占用 1 到 4 字节，具体取决于字符的内容。 VARCHAR(100)和 VARCHAR(10)区别 VARCHAR(100)和 VARCHAR(10)都是变长类型，表示能存储最多 100 个字符和 10 个字符。但二者存储相同的字符串，所占用磁盘的存储空间其实是一样的，其所占用的磁盘空间是基于实际存储的字符长度，而不是字段的最大长度。\nDECIMAL 和 FLOAT/DOUBLE 区别 DECIMAL 是定点数，可以存储精确的小数值。\nFLOAT（4 字节），DOUBLE（8 字节） 是浮点数，只能存储近似的小数值。\nDECIMAL 用于存储具有精度要求的小数，例如与货币相关的数据，可以避免浮点数带来的精度损失。\nTEXT 和 BLOB TEXT 可以存储更长的字符串，即长文本数据，例如博客内容。 BLOB 类型主要用于存储二进制大对象，例如图片、音视频等文件。 TEXT 和 BLOB 缺点： 不能有默认值。 检索效率较低。 在使用临时表时无法使用内存临时表，只能在磁盘上创建临时表。 不能直接创建索引，需要指定前缀长度。 \u0026gt; DATETIME 和 TIMESTAMP MySQL 日期类型选择建议 | JavaGuide\nDATETIME 类型没有时区信息，TIMESTAMP 和时区有关。 NULL 和\u0026quot;\u0026ldquo;的区别 NULL 代表一个不确定的值,就算是两个 NULL,它俩也不一定相等。例如，SELECT NULL=NULL的结果为 NULL ''的长度是 0，是不占用空间的，而NULL 是需要占用空间 为什么 MySQL 不建议使用 NULL 作为列默认值? NULL 会影响聚合函数的结果。例如，SUM、AVG、MIN、MAX 等聚合函数会忽略 NULL 值，如果参数是某个字段名(COUNT(列名))，则会忽略 NULL 值，只统计非空值的个数。查询 NULL 值时，必须使用 IS NULL 或 IS NOT NULLl 来判断，而不能使用 =、!=、 \u0026lt;、\u0026gt; 之类的比较运算符。而''是可以使用这些比较运算符的。\nBoolean 类型如何表示 MySQL 中没有专门的布尔类型，而是用 TINYINT(1) 类型来表示布尔值。TINYINT(1) 类型可以存储 0 或 1，分别对应 false 或 true。\nMySQL 基础架构 连接器： 身份认证和权限相关(登录 MySQL 的时候)。 查询缓存： 执行查询语句的时候，会先查询缓存（MySQL 8.0 版本后移除，因为这个功能不太实用）。 分析器： 没有命中缓存的话，SQL 语句就会经过分析器，分析器说白了就是要先看你的 SQL 语句要干嘛，再检查你的 SQL 语句语法是否正确。 优化器： 按照 MySQL 认为最优的方案去执行。 执行器： 执行语句，然后从存储引擎返回数据。 执行语句之前会先判断是否有权限，如果没有权限的话，就会报错。 插件式存储引擎 ：主要负责数据的存储和读取，采用的是插件式架构，支持 InnoDB、MyISAM、Memory 等多种存储引擎。InnoDB 是 MySQL 的默认存储引擎，绝大部分场景使用 InnoDB 就是最好的选择。 MySQL 存储引擎 MySQL :: MySQL 8.0 Reference Manual :: 17 The InnoDB Storage Engine\nMySQL :: MySQL 8.0 Reference Manual :: 18 Alternative Storage Engines\nMySQL 5.5.5 之前，MyISAM 是 MySQL 的默认存储引擎。5.5.5 版本之后，InnoDB 是 MySQL 的默认存储引擎。 在上述所有引擎中，只有 InnoDB 引擎支持事务。 上面还提到过 MySQL 存储引擎采用的是 插件式架构 ，支持多种存储引擎。存储引擎是基于表的，而不是数据库。 MyISAM 和 InnoDB 区别 锁： MyISAM 只有表级锁，而 InnoDB 支持行级锁和表级锁，默认为行级锁。对于并发操作，细粒度的行级锁性能肯定更好！\n事务： MyISAM 不提供事务支持。InnoDB 提供事务支持，实现了四个隔离级别，分别是：读未提交、读已提交、可重复读、可串行化。InnoDB 默认是可重读，隔离级别是可以解决幻读问题发生的（基于 MVCC 和 Next-Key Lock）。\n外键： MyISAM 不支持物理外键，而 InnoDB 支持物理外键。然而外键的维护对数据库性能也有一定影响，特别是分布式、高并发项目，一个字段的更新往往会引起其他字段的更新，极大拉低数据库性能，因此是不建议使用物理外键，而是逻辑外键（在代码中进行约束）\n数据恢复 ：MyISAM 不支持，而 InnoDB 支持。使用 InnoDB 的数据库在异常崩溃后，数据库重新启动的时候依赖redo.log使数据库恢复到崩溃前的状态。\nMVCC： MVCC 可以看作是行级锁的一个升级，可以有效减少加锁操作，提高性能。MyISAM 显然是不支持 MVCC 的，毕竟连行级锁都没有。\n索引： 都使用 B+Tree 作为索引结构，但是两者的实现方式不太一样。InnoDB 引擎中，其数据文件本身就是索引文件。相比 MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按 B+Tree 组织的一个索引结构，树的叶节点 data 域保存了完整的数据记录。\n数据缓存策略和机制： InnoDB 使用缓冲池（Buffer Pool）缓存数据页和索引页，MyISAM 使用键缓存（Key Cache）仅缓存索引页而不缓存数据页。当数据库对数据做修改的时候，需要把数据页从磁盘读到 buffer pool 中，然后在 buffer pool 中进行修改 ，此时 buffer pool 中的数据页就与磁盘上的数据页内容不一致，如果这个时候发生 DB 服务重启，那么这些数据并没有同步到磁盘文件中（同步到磁盘文件是个随机 IO），就会发生数据丢失，如果这个时候，能够在有一个文件，当 buffer pool 中的数据页变更结束后，把相应修改记录记录到这个文件（记录日志是顺序 IO），那么当 DB 服务进行恢复 DB 的时候，可以根据这个文件的记录内容，重新持久化刷新到磁盘文件，保持数据的一致性。\n性能： InnoDB 的性能比 MyISAM 更强大，不管是在读写混合模式下还是只读模式下，随着 CPU 核数的增加，InnoDB 的读写能力呈线性增长。MyISAM 因为读写不能并发，它的处理能力跟核数没关系。\nMySQL 如何存储 IP 地址 可以使用字符串存储，但是存储空间相对较大（每个字符占用 1 字节），每个 IP 占用空间为 7-15 个字节（1.1.1.1 占用 7 字节，100.100.100.100 占用 15 字节）。\n对于 ipv4，其实是 4 字节 32 位的数字，可以将 IP 地址转换成整形数据存储，性能更好，占用空间也更小。\nMySQL 提供了两个方法来处理 ip 地址\nINET_ATON()：把 ip 转为无符号整型 (4-8 位) INET_NTOA() :把整型的 ip 转为地址 插入数据前，先用 INET_ATON() 把 ip 地址转为整型，显示数据时，使用 INET_NTOA() 把整型的 ip 地址转为地址显示即可。\n","date":"2024-12-26T14:43:23Z","permalink":"/zh-cn/post/2024/12/mysql%E5%85%A5%E9%97%A8%E6%A6%82%E8%BF%B0/","title":"MySQL入门概述"},{"content":"JD hotKey 链接\n京东 hotkey 把热点数据默认缓存在了本地缓存 caffeine 中，也可以存到 redis 中，但是京东 hotkey 的 SDK 没有 redis 的实现方法，因此需要自己实现。\n官方目录结构下：分别是 client 客户端（要打包引入到自己的项目）、common 工具包（也打包引入到自己项目），dashboard（hotkey 可视化面板，自己设置端口启动即可）、sample（实现 demo）、worker（也要自己设置端口并且启动，用来和 etcd 交流信息） ​\nclient 是 hotKey 客户端，需要打包引入到我们自己的项目中（在自己项目中建个 lib 目录），刚开始打包报错，说是找不到某些模块，把父模块 clean 然后 install 一下，再打包 client 模块就好了。\nclient 打成 jar 包后，要用的是 with-dependencies 包，并且要改名成 hotkey-client-0.0.4-SNAPSHOT.jar，因为我们自己项目的依赖名字就是 hotkey-client-0.0.4-SNAPSHOT.jar\n另外 client 的 pom 文件会加载不到一个 plugin，这个时候需要设置一下 groupId，去中央仓库看一下就知道了：\n自己的项目引入依赖：\n1 2 3 4 5 6 7 \u0026lt;dependency\u0026gt; \u0026lt;artifactId\u0026gt;hotkey-client\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;com.jd.platform.hotkey\u0026lt;/groupId\u0026gt; \u0026lt;version\u0026gt;0.0.4-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;system\u0026lt;/scope\u0026gt; \u0026lt;systemPath\u0026gt;${project.basedir}/lib/hotkey-client-0.0.4-SNAPSHOT.jar\u0026lt;/systemPath\u0026gt; \u0026lt;/dependency\u0026gt; dashboard 模块，配置启动端口和 etcd 的地址，默认是http://127.0.0.1:2379，另外resources目录下还有db.sql需要建表，是dashboard运行所必须的。\n配置用户：appName 和后端 yml 配置要相同 配置 dashboard 的热 key 规则： worker 模块配置端口直接启动即可。\n项目成功接入 hotKey 监控 ","date":"2024-12-21T11:58:09Z","permalink":"/zh-cn/post/2024/12/etcd-%E4%BA%AC%E4%B8%9Chotkey%E6%8E%A2%E6%B5%8B%E4%BD%BF%E7%94%A8/","title":"etcd+京东hotkey探测使用"},{"content":"\n测试之后发现是因为 Map\u0026lt;Long,List\u0026lt;CommentVO\u0026gt;\u0026gt;的返回值类型的锅，改成 Page\u0026lt;List\u0026lt;CommentVO\u0026gt;\u0026gt;即可解决。\n前端使用的 umiMAX 的 openapi，报错如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 \u0026#39; originalRef: BaseResponse\u0026lt;\u0026lt;boolean\u0026gt;\u0026gt;\\n\u0026#39; + \u0026#39; \u0026#34;401\u0026#34;:\\n\u0026#39; + \u0026#39; description: Unauthorized\\n\u0026#39; + \u0026#39; \u0026#34;403\u0026#34;:\\n\u0026#39; + \u0026#39; description: Forbidden\\n\u0026#39; + \u0026#39; \u0026#34;404\u0026#34;:\\n\u0026#39; + \u0026#39; description: Not Found\\n\u0026#39; + \u0026#39; deprecated: false\\n\u0026#39; + \u0026#39; x-order: \u0026#34;2147483647\u0026#34;\\n\u0026#39; + \u0026#39; \u0026#34;/api/admin/passage/reject/{passageId}\u0026#34;:\\n\u0026#39; + \u0026#39; get:\\n\u0026#39; + \u0026#39; tags:\\n\u0026#39; + \u0026#39; - admin-passage-controller\\n\u0026#39; + \u0026#39; summary: rejectPassage\\n\u0026#39; + \u0026#39; operationId: rejectPassageUsingGET\\n\u0026#39; + \u0026#39; produces:\\n\u0026#39; + \u0026#39; - \u0026#34;*/*\u0026#34;\\n\u0026#39; + \u0026#39; parameters:\\n\u0026#39; + \u0026#39; - name: passageId\\n\u0026#39; + \u0026#39; in: path\\n\u0026#39; + \u0026#39; description: passageId\\n\u0026#39; + \u0026#39; \u0026#39;... 78804 more characters, externals: [], externalRefs: {}, rewriteRefs: true, preserveMiro: true, promise: { resolve: [Function (anonymous)], reject: [Function (anonymous)] }, patches: 0, cache: {}, openapi: { openapi: \u0026#39;3.0.0\u0026#39;, info: { description: \u0026#39;无问青秋博客\u0026#39;, version: \u0026#39;v1.0\u0026#39;, title: \u0026#39;Serein博客API接口测试\u0026#39;, contact: {} }, tags: [ { name: \u0026#39;admin-category-controller\u0026#39;, \u0026#39;x-order\u0026#39;: \u0026#39;2147483647\u0026#39; }, { name: \u0026#39;admin-comment-controller\u0026#39;, \u0026#39;x-order\u0026#39;: \u0026#39;2147483647\u0026#39; }, { name: \u0026#39;admin-passage-controller\u0026#39;, \u0026#39;x-order\u0026#39;: \u0026#39;2147483647\u0026#39; }, { name: \u0026#39;admin-tag-controller\u0026#39;, \u0026#39;x-order\u0026#39;: \u0026#39;2147483647\u0026#39; }, { name: \u0026#39;admin-user-controller\u0026#39;, \u0026#39;x-order\u0026#39;: \u0026#39;2147483647\u0026#39; }, { name: \u0026#39;category-controller\u0026#39;, \u0026#39;x-order\u0026#39;: \u0026#39;2147483647\u0026#39; }, { name: \u0026#39;comment-controller\u0026#39;, \u0026#39;x-order\u0026#39;: \u0026#39;2147483647\u0026#39; }, { name: \u0026#39;passage-controller\u0026#39;, \u0026#39;x-order\u0026#39;: \u0026#39;2147483647\u0026#39; }, { name: \u0026#39;tag-controller\u0026#39;, \u0026#39;x-order\u0026#39;: \u0026#39;2147483647\u0026#39; }, { name: \u0026#39;user-controller\u0026#39;, \u0026#39;x-order\u0026#39;: \u0026#39;2147483647\u0026#39; } ], paths: { \u0026#39;/api/admin/category/addCategory\u0026#39;: { post: { tags: [Array], summary: \u0026#39;addCategory\u0026#39;, operationId: \u0026#39;addCategoryUsingPOST\u0026#39;, requestBody: [Object], responses: [Object], responsesObject: [Object], deprecated: false, \u0026#39;x-order\u0026#39;: \u0026#39;2147483647\u0026#39; } }, 前端openapi生成接口报错 preserveMiro: true, promise: { resolve: [Function (anonymous)], reject: [Function (anonymous)] }, patches: 0, cache: {}, openapi: { openapi: '3.0.0', info: { description: '无问青秋博客', version: 'v1.0', title: 'Serein博客API接口测试', contact: {} }, tags: [ { name: 'admin-category-controller', 'x-order': '2147483647' }, { name: 'admin-comment-controller', 'x-order': '2147483647' }, { name: 'admin-passage-controller', 'x-order': '2147483647' }, { name: 'admin-tag-controller', 'x-order': '2147483647' }, { name: 'admin-user-controller', 'x-order': '2147483647' }, { name: 'category-controller', 'x-order': '2147483647' }, { name: 'comment-controller', 'x-order': '2147483647' }, { name: 'passage-controller', 'x-order': '2147483647' }, { name: 'tag-controller', 'x-order': '2147483647' }, { name: 'user-controller', 'x-order': '2147483647' } ], paths: { '/api/admin/category/addCategory': { post: { tags: [Array], summary: 'addCategory', operationId: 'addCategoryUsingPOST', requestBody: [Object], responses: [Object], responsesObject: [Object], deprecated: false, 'x-order': '2147483647' } }, '/api/admin/category/delete/{categoryId}': { put: { tags: [Array], summary: 'deleteCategory', operationId: 'deleteCategoryUsingPUT', parameters: [Array], responses: [Object], responsesObject: [Object], deprecated: false, 'x-order': '2147483647' } }, '/api/admin/comment/getComments/': { post: { tags: [Array], summary: 'getComments', operationId: 'getCommentsUsingPOST', requestBody: [Object], responses: [Object], responsesObject: [Object], deprecated: false, 'x-order': '2147483647' } }, '/api/user/userInfoData': { get: { tags: [Array], summary: 'getUserInfoData', operationId: 'getUserInfoDataUsingGET', responses: [Object], responsesObject: [Object], deprecated: false, 'x-order': '2147483647' } } }, 'x-openapi': { 'x-markdownFiles': null, 'x-setting': { language: 'zh-CN', enableSwaggerModels: true, swaggerModelName: 'Swagger Models', enableReloadCacheParameter: false, enableAfterScript: true, enableDocumentManage: true, enableVersion: false, enableRequestCache: true, enableFilterMultipartApis: false, enableFilterMultipartApiMethodType: 'POST', enableHost: false, enableHostText: '', enableDynamicParameter: false, enableDebug: true, enableFooter: true, enableFooterCustom: false, enableSearch: true, enableOpenApi: true, enableHomeCustom: false, enableGroup: true, enableResponseCode: true } }, servers: [ { url: '//127.0.0.1:8081/api' } ], components: { requestBodies: { QueryPageRequest: { content: [Object], description: 'queryPageRequest', required: true }, uploadPassageCoverUsingPOST: { content: [Object], required: true } }, schemas: { AddPassageDTO: { type: 'object', properties: [Object], title: 'AddPassageDTO' }, GetUserByIdListRequest: { type: 'object', properties: [Object], title: 'GetUserByIdListRequest' }, LoginRequest: { type: 'object', properties: [Object], title: 'LoginRequest' }, LoginUserVO: { type: 'object', properties: [Object], title: 'LoginUserVO' }, Page_List_UserVO_: { type: 'object', properties: [Object], title: 'Page\u0026lt;\u0026lt;List\u0026lt;\u0026lt;UserVO\u0026gt;\u0026gt;\u0026gt;\u0026gt;' } } } }, fetch: \u0026lt;ref *1\u0026gt; [Function: fetch] { isRedirect: [Function (anonymous)], Promise: [Function: Promise], default: [Circular *1], Headers: [class Headers], Request: [class Request], Response: [class Response], FetchError: [Function: FetchError] }, resolver: { depth: 0, base: undefined, actions: [ [] ] }, refmap: {} } } Node.js v20.10.0 ","date":"2024-12-16T22:39:23Z","permalink":"/zh-cn/post/2024/12/%E5%89%8D%E7%AB%AFopenapi%E6%A0%B9%E6%8D%AE%E5%90%8E%E7%AB%AFswagger%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E5%89%8D%E7%AB%AF%E6%8E%A5%E5%8F%A3%E6%8A%A5%E9%94%99/","title":"前端OpenAPI根据后端Swagger自动生成前端接口报错"},{"content":"网络层概述 网络层提供的两种服务 分类编址的 IPV4 无分类编址的 IPV4\u0026mdash;CIDR IPV4 地址应用规划 使用定长子网掩码划分子网 使用变长子网掩码划分子网 IPV4 和 MAC 地址 地址解析协议 ARP IP 数据报发送转发流程 静态路由配置 默认路由 特定主机路由 路由选择分类 注意这里的内部网关协议 IGP、外部网关协议 EGP 是一个大类别，还分为不同的具体协议\n路由信息协议 RIP 开放最短路径优先 OSPF 路由信息协议 BGP 路由器工作原理 IP 多播地址和多播组 IPV6 ","date":"2024-12-16T12:24:02Z","permalink":"/zh-cn/post/2024/12/%E8%80%83%E5%89%8D%E9%A2%84%E4%B9%A04.%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%BD%91%E7%BB%9C%E5%B1%82/","title":"【考前预习】4.计算机网络—网络层"},{"content":"数据链路层概述 数据链路层属于计算机网络的低层。数据链路层使用的信道主要有以下两种类型：\n点对点信道：使用一对一的点对点通信方式。 广播信道：使用一对多的广播通信方式，因此过程比较复杂。广播信道上连接的主机很多，因此必须使用专用的共享信道协议来协调这些主机的数据发送。 局域网虽然是个网络，但我们并不把局域网放在网络层中讨论。这是因为在网络层要讨论的问题是多个网络互连的问题，是讨论分组怎样从一个网络通过路由器转发到另一个网络。\n数据链路层研究的是在同一个局域网中，分组怎样从一台主机不用路由器传送到另一台主机。 数据链路和帧 链路：从一个节点到相邻节点的一段物理线路。 数据链路：在链路的基础上增加必要的通信协议控制数据传输。 链路层协议数据单元：帧，链路层接收到网络层的 IP 数据报添加首部尾部信息封装成帧作为链路层传输单元。 点对点信道的数据链路通信步骤：\n(1)节点 A 的数据链路层把网络层交下来的 P 数据报添加首部和尾部封装成帧。\n(2)节点 A 把封装好的帧发送给节点 B 的数据链路层。\n(3)若节点 B 的数据链路层收到的帧无差错，则从收到的帧中提取出 P 数据报交给上面的网络层：否则丢弃这个帧。 数据链路层三个基本问题 封装成帧 在 IP 数据报头部尾部添加一些控制信息，使得接收端的数据链路层接收到物理层的比特流后能根据首部尾部标记识别到帧的开始和结束，因此帧的作用之一就是帧定界。\n最大传送单元 MTU 是数据部分长度上限。 \u0026gt; 帧定界 当数据是由可打印的 ASCⅡ 码组成的文本文件时，帧定界可以使用特殊的帧定界符。**ASCⅡ 码是 7 位编码，一共可组合成 128 个不同的 ASCIⅡ 码，其中可打印的有 95 个，而不可打印的控制字符有 33 个。**控制字符 SOH(Start Of Header)放在一帧的最前面，表示帧的首部开始。另一个控制字符 EOT(End Of Transmission)表示帧的结束。SOH 和 EOT 都是控制字符的名称。它们的十六进制编码分别是 01（二进制是 00000001）和 04（二进制是 00000100）。SOH(或 EOT)并不是 S,O,H（或 E,O,T)三个字符。 如果数据传送时中断，那么接收端接收到的数据就只有 SOH，没有 EOT，那么说明帧是不完整的，接后端链路层就会丢弃该帧，因此帧定界有一定的检错作用。 \u0026gt; 透明传输 由于帧的开始和结束的标记使用专门指明的控制字符，因此传输数据中的任何 8 bit 的组合不能和帧定界的控制字符的比特编码一样，否则就会出现帧定界的错误。 当传送的帧是用文本文件组成的帧时(从键盘上输入)，其数据部分一定不会出现像 SOH 或 EOT 这样的帧定界控制字符。可见不管从键盘上输入什么字符都可以放在这样的帧中传输过去，这样的传输就是透明传输。 当数据部分是非 ASCⅡ 码的文本文件时（如二进制代码的计算机程序或图像等)，情况就不同了。如果数据中的某个字节的二进制代码恰好和 SOH 或 EOT 这种控制字符一样，数据链路层就会错误地\u0026quot;找到帧的边界\u0026quot;，把部分帧收下（误认为是个完整的帧)，而把剩下的那部分数据丢弃，这样就不是透明传输。 实现透明传输\u0026mdash;字节填充 设法使数据中出现的控制字符 \u0026ldquo;SOH\u0026quot;和\u0026quot;EOT\u0026quot;在接收端不被解释为控制字符。具体的方法是：发送端的数据链路层在数据中出现控制字符\u0026quot;SOH\u0026quot;或\u0026quot;EOT\u0026quot;的前面插入一个转义字符\u0026quot;ESC\u0026rdquo; (其十六进制编码是 1B,二进制是 00011011)。而在接收端数据链路层在把数据送往网络层之前删除这个插入的转义字符 。这种方法称为字节填充或字符填充 。如果转义字符也出现在数据\n当中，那么解决方法仍然是在转义字符的前面插入一个转义字符 。因此，当接收端收到连续\n的两个转义字符时，就删除其中前面的一个。 差错检测 比特流在传输过程中可能出现比特差错，一段时间内，传输错误的比特占传输总比特的比率叫误码率 BER，这和信噪比有一定关系。\n循环冗余检验 CRC 在发送端，先把数据划分为组，假定每组 k 个比特。现假定待传送的数据 M=101001(k=6)。CRC 运算就是在数据 M 的后面添加供差错检测用的 n 位冗余码，然后构成一个帧发送出去，一共发送(k+n)位。在所要发送的数据后面增加 n 位的冗余码，虽然增大了数据传输的开销，却可以进行差错检测。 冗余码可用以下方法得出：用二进制的模 2 运算进行乘 M 的运算，相当于在 M 后面添加 n 个 0。得到的(k+)位的数除以收发双方事先商定的长度为(n+1)位的除数 P,得出商是 Q 而余数是 R(n 位，比 P 少一位)。M=101001(即 k=6)。假定除数 P=1101(即 n=3)。经模 2 除法运算后的结果是：商 Q=110101(商没有用处)，而余数 R=001 这个余数 R 就作为冗余码拼接在数据 M 的后面发送出去。这种为了进行检错而添加的冗余码常称为帧检验序列 FCS。因此加上 FCS 后发送的帧是 101001001（即M+FCS)，共(k+n)位。 在接收端对收到的每一帧经过 CRC 检验后，有以下两种情况： 若得出的余数 R=0,则判定这个帧没有差错，就接受。 若余数 R≠0，则判定这个帧有差错（但无法确定究竟是哪一位或哪几位出现了差\n错)，就丢弃。 CRC 是一种检错方法，而 FCS 是添加在数据后面的冗余码。\n生成多项式 可靠传输 由上面知，凡是数据链路层接收到的帧，都概率接近 1 的认为帧没有差错，因为差错的帧都被丢弃了。但是我们并不要求链路层向网络层提供可靠传输的服务。\n传输差错可分为两类：一类是前面的比特差错，另一类是帧丢失、重复、失序。所以，CRC 检验实现的无比特差错传输还不是可靠传输。\n点对点信道—点对点协议 PPP 除了能实现可靠传输的高级数据链路控制 HDLC，还有更为简单好用的 PPP 协议。\nPPP 协议制定于 1992 年，是用户计算机和 ISP 进行通信所使用的数据链路层协议。\nPPP 协议组成 PPP 协议有三个组成部分：\n一个将 IP 数据报封装到串行链路的方法。PPP 既支持异步链路（无奇偶检验的 8 bit 数据)，也支持同步链路。IP 数据报在 PPP 帧中就是其信息部分。这个信息部分的长度受最大传送单元 MTU 的限制。 一个用来建立、配置和测试数据链路连接的链路控制协议 LCP(Link Control Protocol)。通信的双方可协商一些选项。 一套网络控制协议 NCP(Network Control Protocol)，其中的每一个协议支持不同的网络层协议。 PPP 协议帧格式 PPP 帧的首部和尾部分别为四个字段和两个字段。\n首部的第一个字段和尾部的第二个字段都是标志字段 F(Flag),规定为 0x7E(符号“0x”，表示它后面的字符是用十六进制表示的。十六进制的 7E 的二进制表示是 01111110)。标志字段表示一个帧的开始或结束。因此标志字段就是 PPP 帧的定界符。 首部中的地址字段 A 规定为 0xFF（即 11111111)，控制字段 C 规定为 0x03(即 00000011)。最初曾考虑以后再对这两个字段的值进行其他定义，但至今也没有给出。可见这两个字段实际上并没有携带 PPP 帧的信息。 PPP 首部的第四个字段是 2 字节的协议字段。当协议字段为 0x0021 时，PPP 帧的信息\n字段就是IP 数据报。若为 0xC021,则信息字段是 PPP 链路控制协议 LCP 的数据，\n0x8021 表示这是网络层的控制数据。 信息字段的长度是可变的，不超过 1500 字节。\n尾部中的第一个字段(2 字节)是使用 CRC 的帧检验序列 FCS。 透明传输实现—字节填充 **PPP 使用异步传输时，**采用字节填充来实现透明传输：\n把信息字段中和标志字段一样的比特组合（0x7E）转义为 0x7D 并使用字节填充。 把信息字段中比特组合为 0x7D 的变为 2 字节序列（0x7D，0x5D）。 把信息字段中的 ASCⅡ 码控制字符前加 0x7D。 透明传输实现—零比特填充 **PPP 在 SONET/SDH 使用同步传输时，**采用零比特填充来实现透明传输：\n发送端先扫描整个信息字段，只要发现有 5 个连续 1，则立即填入一个 0。保证在信息字段中不会出现 6 个连续 1。 接收端在收到一个帧时，先找到标志字段 F 以确定一个帧的边界，接着再用硬件对其中的比特流进行扫描。每当发现 5 个连续 1 时，就把这 5 个连续 1 后的一个 0 删除，以还原成原来的信息比特流。 这样就保证了透明传输：在所传送的数据比特流中可以传送任意组合的比特流，而不会引起对帧边界的错误判断。 广播信道 局域网最主要的特点：网络为一个单位所拥有，且地理范围和站点数目均有限。\n局域网可按网络拓扑进行分类：\n星形网，由于集线器(hub)的出现和双绞线大量用于局域网中，星形以太网以及多级星形结构的以太网获得了非常广泛的应用。 环形网。 总线网，各站直接连在总线上。总线两端的匹配电阻吸收在总线上传播的电磁波信号的能量，避免总线产生有害电磁波反射。\n总线网以传统以太网最为著名，但以太网后来又演变成了星形网。 现在以太网已成为了局域网的同义词，因此本章从本节开始都是讨论以太网技术。 共享信道 共享信道要着重考虑的一个问题就是如何使众多用户能够合理而方便地共享通信媒体\n资源。这在技术上有两种方法：\n静态划分信道，用户只要分配到了信道就不会和其他用户发生冲突。但这种划分信道的方法代价较高，不适合于局域网使用。 动态媒体接入控制，又称为多点接入，特点是信道并非在用户通信时固定分配给用户。这里又分为以下两类： 随机接入随机接入的特点是所有的用户可随机地发送信息。但如果恰巧有两个或更多的用户在同一时刻发送信息，那么在共享媒体上就要产生碰撞（即发生了冲突)，使得这些用户的发送都失败。因此，必须有解决碰撞的网络协议。 受控接入，受控接入的特点是用户不能随机地发送信息而必须服从一定的控制。这类的典型代表有分散控制的令牌环局域网和集中控制的多点线路探询。 适配器的作用 早期的适配器是计算机主箱的网络接口板，称为网卡，用于连接外界局域网，现在的计算机主板镶嵌了这种适配器，不再单独使用网卡。 通信适配器上面装有处理器和存储器（包括 RAM、ROM)。适配器和局域网之间的通信是通过电缆或双绞线以串行传输方式进行的，而适配器和计算机之间的通信则是通过计算机主板上的 I/O 总线以并行传输方式进行的。 由于网络上的数据率和计算机总线上的数据率并不相同，因此在适配器中必须装有对数据进行缓存的存储芯片。在主板上插入适配器时，还必须把管理该适配器的设备驱动程序安装在计算机的操作系统中。 同时适配器还要实现以太网协议。 计算机的硬件地址在适配器 ROM 中，软件地址（IP 地址）在存储器中 \u0026gt; 总线型网络拓扑—CSMA/CD 协议 为了通信简便，以太网有如下两种措施：\n采用无连接，不建立连接直接发数据、不需要 ACK 确认，提供不可靠交付，接收端 CRC 检测到错误帧，对于错误帧是否重传由高层决定，比如 TCP 协议，TCP 协议检测到数据缺失，会让以太网重传。\n然而，总线型拓扑网络中只要有一台计算机发送数据，那么总线传输资源就会被占用，所以在同一时间只能允许一台计算机发送数据，否则会发生冲突。\n为了避免发生冲突，就有了 CSMA/CD 协议，意为载波监听多点接入/碰撞检测 以太网使用曼彻斯特编码 \u0026gt; “多点接入”就是说明这是总线型网络，许多计算机以多点接入的方式连接在一根总\n线上。协议的实质是“载波监听”和“碰撞检测”。\n“载波监听”也就是“边发送边监听”。载波监听就是在发送数据前后，每个站都必须不停地检测信道。如果检测出已经有其他站在发送，则本站就暂时不要发送数据。在发送中检测信道，是为了及时发现如果有其他站也在发送，就立即中断本站的发送。这就称为碰撞检测。\n然而也会出现两个计算机同时发送消息的情况。\n在使用 CSMA/CD 协议时，一个站不可能同时进行发送和接收。因此使用 CSMA/CD 协议的以太网不可能进行全双工通信而只能进行双向交替通信（半双工通信）。\n截断二进制指数、退避算法 以太网使用截断二进制指数退避算法来确定碰撞后重传的时机。这种算法让发生碰撞而停止发送数据的站退避一个随机的时间再发送数据，而不是等待信道变为空闲后就立即再发送数据，因为几个发生碰撞的站将会同时检测到信道变成了空闲。如果大家都同时重传，必然接\n连发生碰撞。如果采用退避算法，生成了最小退避时间的站将最先获发送权。以后其余的站\n的退避时间到了，但发送数据之前监听到信道忙，就不会马上发送数据了。\n星形网络拓扑—集线器 传统以太网使用粗同轴线缆后来演变成便宜灵活的双绞线，这种以太网采用星形拓扑，星形中心增加了一种可靠性非常高的设备，叫做集线器。 双绞线能够传送高速数据的主要措施是把双绞线的绞合度做得非常精确。这样不仅可使特性阻抗均匀以减少失真，而且大大减少了电磁波辐射和无线电频率的干扰。 使用集线器的以太网在逻辑上仍是一个总线网，各站共享逻辑上的总线，使用的还是 CSMA/CD 协议。网络中的各站必须竞争对传输媒体的控制，并且在同一时刻至多只允许一个站发送数据。 一个集线器有许多端口，每个端口通过 RJ-45 插头用两对双绞线与一台计算机上的适配器相连。一个集线器很像一个多端口的转发器。 集线器工作在物理层，每个端口仅转发比特，不进行碰撞检测 \u0026gt; 以太网信道利用率 以太网 MAC 层 MAC 的硬件地址 硬件地址又称为物理地址、MAC 地址，它固化在适配器的 ROM 中，全球唯一，48 比特位。如果计算机更换了适配器，那么其局域网位置就发生改变，因此严格的讲，MAC 地址是每台计算机的唯一名字，而不能确定其位置。\nIEEE 规定地址字段的第一字节的最低有效位为 I/G 位。当 I/G 位为 0 时，地址字段表示一个单个站地址。当 I/G 位为 1 时表示组地址，用来进行多播(组播)。\n在地址记法中有两种标准：第一种记法是把每一字节的最低位(即最低有效位)写在最左边（第一位）。IEEE802.3标准就采用这种记法。例如，十进制数 11 的二进制表示是 1011，最高位写在最左边。但若使用IEEE802.3标准的记法，就应当记为 1101，把最低位写在最左边。\n适配器有过滤功能。当适配器从网络上每收到一个 MAC 帧就先用硬件检查 MAC 帧中的目的地址。如果是发往本站的帧则收下，然后再进行其他的处理。否则将此帧丢弃，\n这里“发往本站的帧”包括以下三种帧：\n单播(unicast)帧（一对一），即收到的帧的 MAC 地址与本站的 MAC 地址相同。 广播(broadcast)帧（一对全体），即发送给本局域网上所有站点的帧**（全 1 地址）**。 多播(multicast)帧（一对多），即发送给本局域网上一部分站点的帧。 以太网适配器还有一种混杂工作方式，可以窃听传输的帧。嗅探器就使用了这种适配器。\nMAC 帧格式 常用的以太网 MAC 帧格式有两种标准：DIX Ethernet V2 标准（以太网 V2 标准)和 IEEE 的 802.3 标准。这里只介绍使用得最多的以太网 V2 的 MAC 帧格式，图中假定网络层使用的是 IP 协议。实际上使用其他的协议也是可以的。 以太网扩展 扩展以太网更常用的方法是在数据链路层进行的。最初人们使用的是网桥(bridge)。网桥对收到的帧根据其 MAC 帧的目的地址进行转发和过滤。当网桥收到一个帧时，并不是向所有的端口转发此帧，而是根据此帧的目的 MAC 地址，查找网桥中的地址表，然后确定将该帧转发到哪一个端口，或者是把它丢弃(即过滤)。\n1990 年问世的交换式集线器(switching hub)代替了网桥。交换式集线器常称为以太网交换机(switch)或第二层交换机(L2 switch)，强调这种交换机工作在数据链路层。\n以太网交换机特点\n交换机实质是多端口网桥，采用全双工方式，可以并行。换句话说，每一个端口和连接到端口的主机构成了一个碰撞域，具有 N 个端口的以太网交换机的碰撞域共有 N 个。 以太网交换机的端口还有存储器，能在输出端口繁忙时把到来的帧进行缓存。 以太网交换机即插即用，内部的帧交换表（又称为地址表）是通过自学习算法自动地逐渐建立起来的。实际上，这种交换表就是一个内容可寻址存储器 CAM。以太网交换机由于使用了专用的交换结构芯片，用硬件转发收到的帧，其转发速率要比使用软件转发的网桥快很多。 以太网交换机自学习\n图示中以太网交换机有四个端口，各连一台计算机，计算机 MAC 地址分别是 ABCD 考虑到主机更换适配器改变了 MAC 地址，那么就需要更改交换表的信息，因此交换表还有第三列用来记录时间，只要超过预设时间信息就会删除，以保证交换表数据符合当前网络的实际状况。\n兜圈子问题 \u0026gt;\n总线以太网使用 CSMA/CD 协议，以半双工方式工作。\n以太网交换机不使用共享总线，没有碰撞问题，不使用 CSMA/CD 协议，以全双工方式工作。\n既然连以太网的重要协议 CSMA/CD 都不使用了（相关的“争用期”也没有了），为什么还叫作以太网呢？原因就是它的帧结构未改变，仍然采用以太网的帧结构。\n虚拟局域网 VLAN 虚拟局域网其实只是局域网给用户提供的一种服务，而并不是一种新型局域网。\nVLAN 标签字段的长度是 4 字节，插入在以太网 MAC 帧的源地址字段和类型字段之间。 VLAN 标签的前两个字节总是设置为 0x8100(即二进制的 1000000100000000)，称为 IEEE802.1Q 标签类型。 VLAN 标签的后两个字节中，前面 4 bit 实际上并没有什么作用**，后面的 12bit**是该虚拟局域网 VLAN 标识符 VID(VLAN ID)，它唯一地标志了 802.1Q 帧属于哪一个 VLAN。12 位的 VID 可识别 4096 个不同的 VLAN。插入 VLAN 标签后，802.1Q 帧最后的帧检验序列 FCS 必须重新计算。 当数据链路层检测到 MAC 帧的源地址字段后面的两个字节的值是 0x8100 时，就知道现在插入了 4 字节的 VLAN 标签。由于用于 VLAN 的以太网帧的首部增加了 4 个字节，因此以太网的最大帧长从原来的 1518 字节(1500 字节的数据加上 18 字节的首部和尾部)变为 1522 字节。 交换机端口类型 ","date":"2024-12-13T11:56:09Z","permalink":"/zh-cn/post/2024/12/%E8%80%83%E5%89%8D%E9%A2%84%E4%B9%A03.%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/","title":"【考前预习】3.计算机网络—数据链路层"},{"content":"物理层基本概念 物理层考虑的是如何在连接各种计算机的传输媒体上传输数据比特流 ，而不是指具体的传输媒体。计算机网络中的硬件设备和传输媒体的种类和通信手段繁多，物理层的作用正是要尽可能地屏蔽掉这些传输媒体和通信手段的差异，使物理层上面的数据链路层感觉不到这些差异，这样就可使数据链路层只需要考虑如何完成本层的协议和服务，而不必考虑网络具体的传输媒体和通信手段是什么 。用于物理层的协议也常称为物理层规程 (procedure)。其实物理层规程就是物理层协议。只是在\u0026quot;协议\u0026quot;这个名词出现之前人们就先使用了\u0026quot;规程\u0026quot;这一名词。\n因此物理层的主要任务可以描述为确定与传输媒体的接口有关的特性：\n机械特性\n指明接口所用接线器的形状和尺寸、引脚数目和排列、固定和锁定装置等。平时常见的各种规格的接插件都有严格的标准化的规定。 电气特性\n指明在接口电缆的各条线上出现的电压的范围。 功能特性\n指明某条线上出现的某一电平的电压的意义。 过程特性\n指明对于不同功能的各种可能事件的出现顺序。 数据在计算机内部一般是并行传输，但是在通信线路上一般是串行传输。\n物理连接的方式很多（例如，可以是点对点的，也可以采用多点连接或广播连接)，而传输媒体的种类也非常之多（如架空明线、双绞线、对称电缆、同轴电缆、光缆，以及各种波段的无线信道等)。因此在学习物理层时，应将重点放在掌握基本概念上。\n数据通信基础知识 数据通信系统可划分为源系统、传输系统、目的系统。通信的目的是传送消息，而数据就是运送消息的实体（使用特定的方式表示消息），信号则是数据的电气或电磁的表现。根据信号中代表消息的参数取值不同，信号可划分为两类：\n模拟信号（连续信号） 数字信号（离散信号） 信道 信道用来表示向某方向传送信息的媒体。从通信双方的信息交互来看，分为：\n单向通信（单工） 双向交替通信（半双工） 双向同时通信（全双工） 来自源系统的信号称为基带信号，基带信号包含较多低频分量、直流分量，信道不能直接传输，因此要对基带信号调制，调制分为：\n基带调制：仅仅对基带信号的波形进行变换，使它能够与信道特性相适应。变换后的信号仍然是基带信号。由于这种基带调制是把数字信号转换为另一种形式的数字信号，因此大家更愿意把这种过程称为编码。 带通调制：使用载波(carrier)进行调制，把基带信号的频率范围搬移到较高的频段，并转换为模拟信号，这样就能够更好地在模拟信道中传输。 信道极限容量 一个码元携带的信息量不固定，由调制方式和编码方式决定。\n码元传输速率越高、信号传输距离越远、噪声干扰越大或媒体传输质量越差，接收端的波形失真越严重。 限制码元传输速率的因素主要有两个：\n信道能通过的频率范围\n奈氏准则：在带宽为 W(Hz)的低通信道中，若不考虑噪声影响，则码元传输的最高速率是 2W(码元/秒)。传输速率超过此上限，就会出现严重的码间串扰的问题，使接收端对码元的判决（即识别）成为不可能。例如，信道的带宽为 4000Hz,那么最高码元传输速率就是每秒 8000 个码元。 信噪比\n噪声随机产生在所有的电子设备和通信信道中，它的瞬时值有时会很大，因此噪声会使接收端对码元的判决产生错误(1 误判为 0 或 0 误判为 1)。但噪声的影响是相对的。如果信号相对较强，那么噪声的影响就相对较小。因此，信噪比就很重要。所谓信噪比就是信号的平均功率和噪声的平均功率之比，常记为 S/N。使用分贝 dB 作为单位。\n\u0026gt; 香农公式\n对于频带宽度确定的信道，信噪比如果不能再提高，码元传输速率也到达上限，此时想要提高信息传输速率，可以使用编码让每个码元携带更多比特信息量。\n物理层的传输媒体 传输媒体也称为传输介质或传输媒介，传输媒体可分为两大类，即导引型传输媒体和非导引型传输媒体。在导引型传输媒体中，电磁波被导引沿着固体媒体（铜线或光纤）传播，而非导引型传输媒体就是指自由空间，在非导引型传输媒体中电磁波的传输常称为无线传输。\n导引型传输媒体 双绞线：两个相互绝缘的铜导线绞合在一起，绞合可以减少一定的电磁干扰。 同轴电缆：由内导体铜质芯线、绝缘层、网状编制的外导体屏蔽层、绝缘保护套组成。 光缆：光纤通信就是利用光导纤维传递光脉冲。一条光纤中可以存在多条不同角度入射的光线，这种光纤就是多模光纤。\n光纤特点\n通信容量非常大的优点。 传输损耗小，中继距离长，对远距离传输特别经济。 抗雷电和电磁干扰性能好。这在有大电流脉冲干扰的环境下尤为重要。 无串音干扰，保密性好，也不易被窃听或截取数据。 体积小，重量轻。这在现有电缆管道己拥塞不堪的情况下特别有利。 \u0026gt; 非导引型传输媒体 紫外线和更高波段目前还不能用于通信。\n信道复用 频分复用 FDM（频分多址） N 路信号在一个信道中传送，使用调制把各路信号分别搬移到合适的频率位置，使彼此不干扰。 时分复用 TDM（时分多址） 把时间划分成一段段等长的时分复用帧，每路信号在每个 TDM 帧中占用固定序号的时隙，使得所有用户在不同时间占用同样的频带宽度。时分复用比频分复用更利于数字信号传输。但是可能造成线路资源浪费 统计时分复用 统计时分复用是时分复用的升级版，能明显提高信道利用率。 2.4.4 波分复用 WDM 光的频分复用。由于光载波的频率很高，因此习惯上用波长而不用频率来表示所使用的光载波。这样就产生了波分复用这一名词。\n2.4.5 码分复用 CDM（码分多址） 另一种共享信道的方法。当码分复用信道为多个不同地址的用户所共享时，就称为码分多址 CDMA\n。每一个用户可以在同样的时间使用同样的频带进行通信。由于各用户使用经过特殊挑选的不同码型，因此各用户之间不会造成干扰。码分复用最初用于军事通信，因为这种系统发送的信号有很强的抗干扰能力，其频谱类似于白噪声，不易被敌人发现。\n宽带接入技术 在计算机网络概述中我们提到了 ISP 和宽带接入技术，这里我们只讨论有线宽带接入\n非对称数字用户线 ADSL 技术 光纤同轴混合网（HFC 网） FTTx 技术 ","date":"2024-12-10T15:09:53Z","permalink":"/zh-cn/post/2024/12/%E8%80%83%E5%89%8D%E9%A2%84%E4%B9%A02.%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%89%A9%E7%90%86%E5%B1%82/","title":"【考前预习】2.计算机网络—物理层"},{"content":"互联网概述 计算机网络（简称网络）：由若干节点和连接这些节点的链路组成，节点可以是计算机、集线器、路由器等。 互连网（internet）：由多个路由器连接组成的范围更大的计算机网络。 互联网（Internet）：当前全球最大的开放的由众多网络连接而成的特定互连网，采用 TCP/IP 协议族作为通信规则，前身是美国的ARPANET（ARPANET 是历史上第一个分组交换网络）。 互联网基础结构发展的三个阶段 第一阶段 ：单个网络 ARPANET 向互连网发展的过程，起初 ARPANET 只是一个单个的分组交换网，不是互连网络。1983 年 TCP/IP 协议成为 ARPANET 的标准协议，使得所有使用该协议的计算机都能通过互连网通信，因此 1983 年也被成为互连网诞生年。 第二阶段：该阶段特点是构成了三级结构的互联网，分为主干网、地区网、校园网。 第三阶段：逐渐形成了全球范围的多层次 ISP 结构的互联网。所谓 ISP 就是互联网服务提供者，中国移动、电信就是我我国有名的 ISP。ISP 可以从互联网管理结构申请很多 IP 地址（IPV4 地址是有限的）并租给用户使用。为了应对互联网数据流量急剧增长，互联网交换点 IXP 诞生，其作用就是允许两个同级的 ISP 直接相连交换分组，不需要借助父级 ISP，这样就提升了数据转发速率。 互联网组成 互联网的拓扑结构十分复杂，可划分为两大块：边缘部分和核心部分。\n边缘部分：由所有连接在互联网上的主机组成（端系统），由用户直接使用。 核心部分：由大量网络和连接网络的路由器组成，为边缘部分提供服务（提供连通和交换）。 边缘部分 端系统之间通信方式分为两类：C/S 和 P2P\nC/S（客户端/服务器）：主机 A 运行客户程序主动向主机 B 请求服务，主机 B 运行服务程序被动接受多个客户的请求。注意：服务提供方和接收方都要依赖网络核心部分提供的服务。同时客户程序必须知道服务程序的地址，而服务程序不需要知道客户程序的地址。 P2P（对等连接）：两台通信的主机不区分服务请求方和服务提供方，都运行对等连接软件，进行平等通信。 核心部分 网络核心部分最重要的是**路由器，其作用是接收分组并转发。**核心部分路由器之间一般用高速链路连接，而网路边缘部分的主机接入核心部分通常以较低速率的链路连接。关于分组交换，有如下概念：\n电路交换 建立连接（占用通信资源）-\u0026gt;通话（一直占用通信资源）-\u0026gt;释放连接（归还通信资源）。\n电话刚刚问世时，为了使两部电话能够通信，使用电路将两个电话相连。 电路交换的缺点就是数据突发式出现在传输线路上，在通话的全部时间内，通话的两个用户始终占用端到端的通信资源。 分组交换 采用存储转发，把一个报文划分为几个分组进行传输。通常把**整块数据称为一个报文，**发送之前把报文划分为一个个等长数据段，每个数据段首部加上必要的控制信息，就构成分组，又称为包，分组首部是包头。分组是互联网中传送的数据单元。\n分组交换在传送数据之前不必占用一条端到端的通信资源，解决了电路交换的弊端。同时，为了保证数据传送的可靠性，路由器中运行的路由选择协议能自动找到转发分组的最优路径。\n采用存储转发的分组交换，实质上采用了数据通信的过程中断续分配传输带宽的策略。 分组交换缺点：分组在路由器转发时需要排队造成了时延，同时也不能确保通信时端到端所需带宽。同时也要携带额外的控制信息。\n报文交换 整个报文传送到相邻节点，全部存储下来查找转发表转发到下一节点。 计算机网络定义 按照作用范围分类：\n广域网 WAN、城域网 MAN、局域网 LAN、个人区域网 PAN 按照使用者分类：\n公用网、专用网 把用户接入到互联网的网络叫做接入网，本地 ISP 可以使用接入网技术把用户的端系统接入到互联网，接入网就是本地 ISP 的网络，不是互联网核心部分、也不是边缘部分。宽带接入网就是接入网技术之一。\n衡量计算机网络的性能 速率 速率指数据传送速率，也叫数据率或比特率，单位 bit/s，常见其他进制单位 K、M、G、T，相邻进制换算单位是 1000，即 1Kbit/s=1000bit/s\n带宽 计算机网络中的带宽用来表示网络中某通道传送数据的能力，即单位时间内某信道能通过的最高数据率，也就是上面的**最大速率，**单位 bit/s\n吞吐量 单位时间内通过某个网络的实际数据量。假定主机 A 和 B 接入到互联网的链路速率分别是 100Mbit/s 和 1Gbit/s，如果互联网各链路容量充足，那么 AB 交换数据时吞吐量是 100Mbit/s 而不是 1Gbit/s，因为主机 A 接收数据的最高速率就是 100Mbit/s。如果有 100 个用户同时连接主机 B，那么主机 B 的 1Gbit/s 会被平分，每个用户只能分到 10Mbit/s 的带宽\n时延 数据从一端传到另一端所需时间，也称为延迟或迟延。分为发送时延、传播时延、处理时延、排队时延。\n发送时延：出现在机器内部。从发送数据帧的第一个比特开始，到该帧的最后一个比特发完所需时间。\n传播时延：出现在机器外部。电磁波在信道中传播一定距离所需时间，计算方法如下： 处理时延和排队时延 略\n时延带宽积 传播时延和带宽相乘，表示管道的体积。 往返时间 RTT 利用率 计算机网络体系结构 下面以五层协议的体系结构为例，简单讲解各层作用。\n应用层 负责主机中进程间的交互，进程就是主机中运行的程序。该层定义了应用进程间的通信交互协议，如域名系统 DNS、HTTP 协议、SMTP 协议等。把应用层交互的数据单元称为报文。\n运输层 负责向两台主机中进程间提供通用的数据传输服务。运输层有复用和分用功能，复用是多个应用层进程同时使用下面运输层的服务，分用是把收到的信息分别交付给应用层的相应进程。\n运输层主要使用如下两种协议： 网络层 使用 IP 协议分组，也叫 IP 数据报。该层任务有两个，一是通过算法在路由器中生成转发分组的转发表，二是接收到分组时查看转发表的路径，把分组发给其他路由器。\n数据链路层 该层把网络层的IP 数据报封装成帧，每一帧加上控制信息。接收数据时，数据链路层每收到一个帧就取出数据部分，交给网络层，如果数据帧有误，就丢弃或者纠正。因此控制信息还要有检错和纠错的功能。\n物理层 该层传输单位是比特，是该体系中的最低层 OSI 参考模型把对等层次之间传送的数据单元称为该层的协议数据单元 PDU，把层与层之间交换的数据单位称为服务数据单元 SDU，多个 SDU 可以合成一个 PDU，一个 SDU 也可以划分成多个 PDU\nTCP/IP 协议族 ","date":"2024-12-09T22:18:04Z","permalink":"/zh-cn/post/2024/12/%E8%80%83%E5%89%8D%E9%A2%84%E4%B9%A01.%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0/","title":"【考前预习】1.计算机网络概述"},{"content":"IPV4 地址分类及组成 IP 地址=网络地址+主机地址，（又称：主机号和网络号） 由上图可见网络号和主机号之和是 32，而且此多彼少。\n例：IP 地址为 192.168.2.131，转换成二进制 1111 1111.1010 1000.0000 0010.1000 0011，因为 192 为 C 类 IP 地址，那么左边 24 位是网络位，代表网段，右边的 8 位是主机号，代表该网段内的唯一一台主机。\n以 C 类网络为例，左边 24 位是网络号，每一位是 0 或 1，因为第一字节十进制范围是 192-233，也就说 24 位网络号变化范围是110 x xxxx.xxxx xxxx.xxxx xxxx 到 1110 1001.xxxx xxxx.xxxx xxxx，那么共有个网段，同理右边 8 位共有个主机，也就是说每个网段下可以最多 254 个主机。那么为什么-2 呢？是因为要除掉 1111 1111 和 0000 0000 两种特殊情况（网络地址和广播地址）\n同理，A 类网段数 128，主机数 16777214，B 类网段数 16384，主机数 65534。\n子网掩码 子网掩码的计算_子网掩码计算-CSDN 博客\n子网掩码用于判断任意两台计算机的 IP 地址是否属于同一子网，也可以判断 IP 地址的网络位和主机位， 它的特征是左边全 1，右边全 0。\n判断方法 ：两台计算机各自的IP 地址与子网掩码进行与运算得到网络地址，结果相同则说明这两台计算机是处于同一个子网，可以进行直接的通讯。 同时，我们可以根据子网掩码可以计算出广播地址、主机号范围、主机数量 通常我们会见到这样的 IP 地址写法：192.168.2.131/24，其中 / 右边的 24 即为子网掩码，/24 是 CIDR 的简写形式，解析如下：\n24 是网络位，全是 1，剩余 8 位是主机位，全是 0，那么写成二进制是：1111 1111.1111 1111.1111 1111.0000 0000，再转化成十进制：255.255.255.0，所以上述 IP 也可写成 192.168.2.131/255.255.255.0 ，那么一般情况 下可以得到如下结果：\n注意表格中的子网掩码是默认的，分别是 8、16、24，子网掩码和 IP 地址类别没有必然关系，并不是说 A 类就必须是 8，B 类必须是 16，A 类也可以是 22！\n网络地址 网络地址=IP 地址和掩码与运算\n以 16.158.165.91/22 为例 ：掩码 22 位，即网络位占 22，主机位占 10 也可以将网络地址的网络号不变，主机号全部变 0 取得。 广播地址 广播地址=掩码取反和网络地址或运算\n仍以 16.158.165.91/22 为例：\n也可以将网络地址的网络号不变，主机号全部变 1 取得。 计算主机号范围及数量 可用 IP 地址范围=\n\\[ 网络地址+1，广播地址-1 \\]\n上述案例中的可用 IP 地址范围：\n\\[ 16.158.164.1，16.158.167.254 \\]\n主机数量=2^主机位二进制数位-2**（不包括网络地址和广播地址）**\n案例中的数量为：2^10 -2=1024-2=1022\n子网划分 为什么要子网划分 IPv4 地址和子网掩码_哔哩哔哩_bilibili\n子网划分思想 最开始我们说，IP 地址=网络号+主机号，其实这是不需要子网划分 的 IP 地址组成，如果 IP 地址需要子网划分 ，那么还要从主机号中借用几位作为子网号，此时的 IP 地址=网络号+子网号+主机号。 子网计算步骤 子网掩码的计算_子网掩码计算-CSDN 博客十分钟理解子网划分 路由技术基础_哔哩哔哩_bilibili子网掩码的计算_子网掩码计算-CSDN 博客\n例： 确定借几位子网号 因为上述案例中有三个部门，所以子网数=3，那么要借2 位子网号，可以表示 4 个子网（00，01，10，11），每个子网可分配主机数(减掉全 0 和全 1)\n确定每个子网的子网掩码 上述 C 类网段掩码是 24，借了 2 位就是 26，（注意！！这里是从主机号那里借了 2 位作为子网号）写成二进制：1111 1111.1111 1111.1111 1111.xx00 0000，再写成十进制即是每个子网的子网掩码，xx 就是 00，11，01，10\n确定子网的网络地址 网络地址=子网掩码和 IP 地址与运算 ，得出每个子网的网络地址如下： 确定子网的广播地址 广播地址=掩码取反和网络地址或运算 确定子网的可用 IP 地址范围 可用 IP 地址范围=\\[ 网络地址+1，广播地址-1 \\] ","date":"2024-10-12T22:07:11Z","permalink":"/zh-cn/post/2024/10/%E5%AD%90%E7%BD%91%E6%8E%A9%E7%A0%81%E7%BD%91%E7%BB%9C%E5%9C%B0%E5%9D%80%E5%B9%BF%E6%92%AD%E5%9C%B0%E5%9D%80%E5%AD%90%E7%BD%91%E5%88%92%E5%88%86%E5%8F%8A%E8%AE%A1%E7%AE%97/","title":"子网掩码、网络地址、广播地址、子网划分及计算"},{"content":" 介绍云原生之前，我们先介绍一下 CNCF，全称为 Cloud Native Computing Foundation，中文译为\u0026quot;云原生计算基金会\u0026quot;。CNCF 致力于培育和维护一个厂商中立的开源生态系统，来推广云原生技术。所以说，CNCF 是云原生领域影响力最大最有话语权的组织。以下是 CNCF 对云原生的定义：\n云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、 服务网格、微服务、不可变基础设施和声明式 API 。\n云原生概念和特点 概念 云原生是一种**构建和运行应用程序的方法，程序生于云端，长于云端。**从有构建应用的想法开始，到需求、设计、开发、测试、构建、打包、部署所有的软件生命周期全部都在云平台上面进行，从应用设计之初（技术选型、架构设计、编译机制）就充分考虑并符合了云的特征，在云平台以最佳姿态原型、为企业降本增效。 特点 弹性扩缩容：本地部署的传统应用无法动态扩展，往往需要冗余资源以抵抗流量高峰，而云原生应用利用云的弹性自动伸缩，应用程序快速复制扩展、部署。 快速启停：应用程序可以快速启停以应对流量变化 隔离性强：进程级别的故障隔离 CICD：持续集成、持续交付、持续部署 常见云模式 公有云\n阿里云、华为云、腾讯云、百度云等等，只需购买就能使用 私有云\n自己搭建或购买的私有平台，使用对象通常是政府、金融机构和企业 混合云： 混合云的优缺点 | IBM 行业云 云对外提供服务的架构模式 IaaS(Infrastructure-as-a-Service) 基础设施即服务\n向外提供硬件资源等基础设施，包括计算、存储、网络等等，用户可以基于基础设施进行上层应用开发部署。\n拿租房比喻就是提供毛坯房，自己装水电、置办家具。\nPaaS(Platform-as-a-Service) 平台即服务\n向外提供平台组件服务，如操作系统、数据库。\n拿租房比喻就是提供装好水电的房子，自己只需置办家具即可入住。\nSaaS(SoftWare-as-a-Service) 软件即服务\n直接向外提供一款成品应用型服务，屏蔽了用户对软件底层的基础设施，用户只需要拿来使用即可。如钉钉、企业微信。\n拿租房比喻就是提供装好水电、家具的房子，直接交租金就拎包入住。\nFaaS(Function-as-a-Service) 功能即服务\nhttps://www.ibm.com/cn-zh/topics/faas\nFaaS 是一种云计算服务，专注于事件驱动，在有请求时自动启动服务，没有时自动关闭服务。 Serverless 和 FaaS 经常被混为一谈，我认为 FaaS 算是无服务器的子集。\n无服务器专注于所有服务类别，无论是计算、存储、数据库、消息传递还是 API 网关等。其中服务器的配置、管理和计费对最终用户不可见，用户只需要对服务按需付费即可。\n云原生核心技术栈 微服务 单体架构：把业务所有功能集中在一个项目中开发，以整个系统为单位进行部署，这种架构简单，如果某一业务的请求量非常大，那么是无法单独扩展该业务的，只能拷贝整个单体应用，再部署一套环境，来实现集群。 微服务架构：根据业务把整个项目划分成多个功能模块，比如订单模块、购物车模块、支付模块、商品详情模块等等，模块之间通过 http 或者 RPC 进行通信。这种架构降低了服务耦合，有利于服务扩展，同时每个服务模块实现了故障隔离，提高了可用性！ SpringCloud 就是微服务中具有代表性的一个技术栈。\n容器技术-Docker、K8S 所谓容器，对操作系统（通常为 Linux）进行虚拟化，具有比虚拟机更高的可移植性和资源效率，可以解决环境差异带来的部署等问题。\n我们把单体项目拆成了微服务，各个微服务模块所需的部署环境可能大不相同，那么不妨把每个微服务模块放到容器中，这个容器包含了服务模块运行所需的除操作系统内容以外所需的函数、配置、依赖等，类似 exe 安装包，这就不仅解决了环境差异带来的应用部署问题，而且各个容器之间实现进程隔离，容器启动速度也更快。\n以 Docker 容器为资源分割和调度的基本单位，封装整个软件运行时的环境，然后发布到 Linux 机器上。\n按照 Docker 的设计方案，应用软件的交付过程如同海上运输，操作系统如同一个货轮，操作系统上的软件都如同一个集装箱。用户可以通过标准化手段自由组装运行环境，同时集装箱的内容可以由用户自定义，然后使用 k8s 编排管理容器的生命周期。如此一来，交付一个应用软件产品，就相当于交付一系列标准化组件的集合。\nDevOps\u0026amp;CI/CD Development 和 Operations，即开发运维一体化，涉及软件在整个开发生命周期中的持续开发，持续测试，持续集成，持续部署和持续监控。简单来说是开发和运维之间地高度协同，实现全生命周期的工具全链路打通与自动化、跨团队的线上协作能力。完成高频率部署的同时，提高生产环境的可靠性、稳定性、弹性和安全性。\nCI：持续集成\n持续集成：开发团队通过将代码的不同部分通过版本控制系统集成到共享存储库中，系统可以自动频繁地进行构建和测试，以确保代码的一致性和稳定性。，一定程度上避免代码冲突和重复劳动。 CD：持续交付、持续部署\n建立在持续集成的基础上，持续交付后的代码处于待发布状态，系统随时可以自动快速地部署到生产环境中，确保应用始终是最新的，支持频繁变更和金丝雀发布。代表产品有阿里云的 Serverless 应用引擎 SAE。Serverless 应用引擎 SAE_应用托管服务_零代码改造上云_容器与中间件-阿里云 Serverless Serverless 是什么？无服务器架构简介-红帽\n一文读懂 Serverless 的起源、发展和落地实践-阿里云开发者社区\nServerless 并不是不需要服务器，而是将服务器全权托管给了云厂商，用户聚焦业务代码，无需关心管理服务器，只用把业务部署到平台的容器上，服务器能自动进行弹性伸缩，这些容器在被调用时会自动按需启动。 不可变基础设施 在传统的可变服务器基础架构中，开发人员操作服务器，手动升级或降级软件包，逐个服务器地调整配置文件，服务器会不断更新和修改。\n可变基础设施通常会导致以下问题：\n持续的修改服务器，缺乏标准，易引入不稳定因素，会导致灾难发生后很难重新构建起等效的新服务。\n而不可变基础设施，最基本的指运行服务的服务器在完成部署后，就不在进行更改，如果配置发生了改变就会生成新的容器，旧容器直接销毁。这就保证了基础架构中更高的一致性和可靠性，以及更简单，更可预测的部署过程。这样云原生就有了稳定的基石！\n声明式 API(k8s) 在命令式 API 中，我们可以直接发出服务器要执行的命令，例如： *“运行容器”、“停止容器”等。*通俗的说，命令式编程是第一人称，我要做什么，我要怎么做。操作系统最喜欢这种编程范式了，操作系统几乎不用“思考”, 只要一对一的将代码翻译成指令就可以了。 *在声明式 API 中，我们声明系统要执行的操作，系统将不断向该状态驱动。*声明式编程类似于“第二人称”， 也就是你要做什么，这有点“”产品经理”和“开发”之间的关系，“产品经理”只负责提需求，而“开发”怎么实现的，他并不关心 Service Mesh(服务网格) 为什么使用服务网格\n应用程序性一定程度上能取决于服务之间通信的速度和弹性。开发人员必须跨服务监控和优化应用程序，但由于系统的分布性质，他们很难获得可见性，在没有服务网格层时，开发人员把服务间的通信逻辑编码到每个服务中，当应用程序越来越大并且在同一个服务上同时运行多个实例时，微服务之间通信将会变得愈发复杂，业务代码和非业务代码糅合在一起。 Service mesh 可以处理应用程序中服务之间的所有通信，同时提供了监控、记录、跟踪和流量控制等新功能。 服务网格如何工作 服务网格从单个服务中提取控制服务间通信的逻辑，并抽象到自己的基础设施层（如 Istio）。它使用多个网络代理来路由和跟踪服务之间的通信。\n代理充当组织网络和微服务之间的中间网关。所有进出该服务的流量都通过代理服务器路由。代理有时被称为 sidecar（直译为边车），sidercar 和微服务块并行运行，这些代理一起构成了服务网格层。\n下面的网格中，绿色是一个个微服务，代表不同的功能模块，蓝色就是每个微服务的代理他们从绿色的微服务中提取出来下沉到 Istio 等设施，负责服务间的通信、监控等。\n服务网格优点 服务发现\n服务网格使用服务注册表来动态发现和跟踪网格中的所有服务，减少管理服务端点的运维负担。 负载均衡\n服务网格使用各种算法（例如循环算法、最少连接或加权负载均衡）在多个服务实例之间智能地分配请求。负载均衡可提高资源利用率并确保高可用性和可扩展性。 流量管理\n服务网格提供高级流量管理功能，可对请求路由和流量行为进行精细控制。 流量分割\n将传入流量划分到不同的服务版本或配置中。网格将一些流量引导到更新后的版本，从而以受控方式逐步推出变更。这样可以实现平稳过渡，并最大限度地降低变更的影响。 **安全性**\n服务网格提供安全通信功能，例如双向 TLS 加密、身份验证和授权。 **监控**\n服务网格提供全面的监控和可观测性功能，可深入了解服务的运行状况、性能和行为。监控还支持故障排除和性能优化。 服务网格架构 服务网格架构中有两个主要组成部分：控制面板和数据面板。\n数据面板 数据面板是服务网格的数据处理组件。它包括所有 sidecar 代理及其功能。当一个服务想要与其他服务通信时，sidecar 代理会采取以下操作：\nsidecar 拦截请求 它将请求封装在单独的网络连接中 它在源代理和目标代理之间建立安全的加密通道 sidecar 代理处理服务之间的低级消息传递。它们还会实施断路和请求重试等功能，以增强弹性并防止服务降级。服务网格功能（例如负载均衡、服务发现和流量路由）在数据面板中实现。\n控制面板 控制面板是服务网格的中央管理和配置层。 管理员可以通过控制面板在网格内定义和配置服务。例如，指定服务端点、路由规则、负载均衡策略和安全设置等参数。定义配置后，控制面板将必要信息分发到服务网格的数据面板。 代理使用配置信息来决定如何处理传入的请求。它们还可以接收配置更改并动态调整其行为而无需重新启动或中断服务。 控制面板通常包括以下功能： 跟踪网格内所有服务的服务注册表 自动发现新服务并删除非活动服务 收集和聚合遥测数据，例如指标、日志和分布式跟踪信息 服务网格和 k8s k8s“服务”资源是简化的 service mesh，它提供服务发现和请求的轮询调度均衡。完整的 service mesh 则提供更丰富的功能，如管理安全策略和加密、“断路”以暂停对缓慢响应的实例的请求以及如上所述的负载均衡等。 服务网格本质上是微服务治理，把服务治理，服务通讯，服务安全，服务监控等逻辑从业务逻辑代码中提取出来形成代理并下沉到 istio 等基础设施中，如下图：\n服务网格面临的挑战 复杂性\n服务网格引入了其他基础设施组件、配置要求和部署注意事项，有一定的学习难度。 运维管理费用\n服务网格会带来部署、管理和监控数据面板代理和控制面板组件的额外开销。例如： 确保服务网格基础设施的高可用性和可扩展性 监控代理的运行状况和性能 处理升级和兼容性问题 必须仔细设计和配置服务网格，以最大限度地减少对整个系统的性能影响。 集成挑战 服务网格必须与现有基础设施无缝集成，才能执行其所需的功能。这包括容器编排平台、网络解决方案和技术堆栈中的其他工具。 Istio Istio / 文档\nIstio 是一个开源**服务治理框架。**Istio 的控制面板组件本身作为 k8s 工作负载运行。它使用 k8s 容器组（一组共享一个 IP 地址的紧密耦合的容器）作为 sidecar 代理设计的基础。提供了服务发现、负载均衡、路由、限流、链路监控、通信加密。\n","date":"2024-10-11T12:00:00Z","permalink":"/zh-cn/post/2024/10/%E4%BA%91%E5%8E%9F%E7%94%9F--%E5%BE%AE%E6%9C%8D%E5%8A%A1ci/cdsaaspaasiaas/","title":"云原生--微服务、CI/CD、SaaS、PaaS、IaaS"},{"content":"用 antd 做个人博客卡到前端了，迫不得已来学 react，也是干上全栈了\u0026ndash; \u0026ndash;学自尚硅谷张天禹 react\nReact 就是 js 框架，可以理解为对 js 做了封装，那么封装后的肯定用起来更方便。\n相关 JS 库\nreact.js：React 核心库。 react-dom.js：提供操作 DOM 的 react 扩展库。 babel.min.js：解析 JSX 语法代码转为 JS 代码的库。\n浏览器不能直接解析 JSX 代码, 需要 babel 转译为纯 JS 的代码才能运行。只要用了 JSX，都要加上 type=\u0026ldquo;text/babel\u0026rdquo;, 声明需要 babel 来处理。 JS 库示例 ：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;1_使用jsx创建虚拟DOM\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!-- 准备好一个\u0026#34;容器\u0026#34; --\u0026gt; \u0026lt;div id=\u0026#34;test\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!-- 引入react核心库 --\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;../js/react.development.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- 引入react-dom，用于支持react操作DOM --\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;../js/react-dom.development.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- 引入babel，用于将jsx转为js --\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;../js/babel.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/babel\u0026#34; \u0026gt; /* 此处一定要写babel */ //1.创建虚拟DOM const VDOM = ( /* 此处一定不要写引号，因为不是字符串 */ \u0026lt;h1 id=\u0026#34;title\u0026#34;\u0026gt; \u0026lt;span\u0026gt;Hello,React\u0026lt;/span\u0026gt; \u0026lt;/h1\u0026gt; ) //2.渲染虚拟DOM到页面 ReactDOM.render(VDOM,document.getElementById(\u0026#39;test\u0026#39;)) \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 浏览器控制台可能会报如下错误： 找不到 favicon.ico 的资源，那么左上角的标签页就不显示图标。解决方法是在项目中根目录下放一个同名图标即可。\n真实 DOM 和虚拟 DOM React 提供了一些 API 来创建虚拟 DOM 对象\n虚拟 dom 定义在\u0026lt;script type=\u0026ldquo;text/babel\u0026rdquo;\u0026gt; \u0026lt;/script\u0026gt;中！！！\n1 2 3 4 5 6 7 8 9 10 //创建虚拟dom 1.用React创建，语法：React.createElement(\u0026#39;标签名\u0026#39;,{标签属性},\u0026#39;标签内容\u0026#39;) const VDOM = React.createElement(\u0026#39;Good\u0026#39;,{id:\u0026#39;title\u0026#39;},\u0026#39;Hello JSX\u0026#39;) 2.用JSX语法创建，可以看到这样创建dom更简单 const VDOM = \u0026lt;Good id=\u0026#34;title\u0026#34;\u0026gt;Hello JSX\u0026lt;/Good\u0026gt; //创建真实dom，不常用 const DOM = document.createElement() 我们编码时基本只需要操作 react 的虚拟 DOM 相关数据, react 会转换为真实 DOM 变化而更新界。 虚拟 DOM 对象最终都会被 React 转换为真实的 DOM。 虚拟 dom 本质是一个 Object（控制台输出 VDOM instanceof Object 的结果为 true）。 虚拟 dom 内部元素少，真实 dom 内部元素多，因为虚拟 dom 是 react 内用，无需真实 dom 那么多属性。 注意！创建的虚拟 dom 只能有一个根标签，并且内部的标签必须闭合（有 /\u0026gt; 结束），如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 //正确的创建虚拟DOM，根标签只有一个 const VDOM = ( \u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;青秋\u0026lt;/h2\u0026gt; \u0026lt;/div\u0026gt; ) //错误的创建虚拟DOM，根标签有俩div const VDOM = ( \u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;青秋\u0026lt;/h2\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;青秋\u0026lt;/h2\u0026gt; \u0026lt;/div\u0026gt; ) //必须有/\u0026gt;结束 const VDOM = ( \u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;青秋\u0026lt;/h2\u0026gt; //\u0026lt;input type=\u0026#34;text\u0026#34; \u0026gt; 错误，没有闭合 \u0026lt;input type=\u0026#34;text\u0026#34;/\u0026gt; 或 \u0026lt;input type=\u0026#34;text\u0026#34;/\u0026gt; \u0026lt;/input\u0026gt; \u0026lt;/div\u0026gt; ) JSX 语法规则 全称是 JavaScript XML，是 react 定义的一种类似于 XML 的 JS 扩展语法，可以把 js 和 html 写在一起，类似 JSP。\nJS + XML 本质是React.createElement ( component , props , \u0026hellip; children **)**方法的语法糖\n作用: 用来简化创建虚拟 DOM\n写法：var VDOM = \u0026lt;h1\u0026gt; Hello JSX\u0026lt;/h1\u0026gt;\n注意：这样创建的\u0026lt;h1\u0026gt;不是字符串, 也不是 HTML/XML 标签，它最终产生的就是一个 JS 对象\n标签名和标签属性任意，可以是 HTML 标签属性或其它\n语法规则：\n遇到 \u0026lt; 开头的代码, 以标签的语法解析: 与 html 同名标签则转换为 html 同名元素 , 其它标签需要特别解析，如：\n1 2 3 4 5 //自定义的h1会被替换成html中同名的标签\u0026lt;h1\u0026gt; const vdom1 = \u0026lt;h1\u0026gt;hello\u0026lt;/h1\u0026gt;; //自定义的Good在html中不存在同名标签,浏览器控制台会报错 const vdom2 = \u0026lt;good\u0026gt;hello\u0026lt;/good\u0026gt;; 也就是说，定义的虚拟 dom 中，开头小写的标签会去 html 中寻找同名的元素并替换，找不到就会报上面的错误。如果是开头大写的标签，那么就是自定义的组件 ，如果写成 Good，那么浏览器就会去渲染 Good 组件，因此 Good 组件需要提前定义，否则会 undefined。\n遇到以 { 开头的代码，以 JS 语法解析，标签中的 JS 表达式必须用{ }包含，比如要在标签中引用自定义的变量，就要用{ }把自定义变量包裹起来，如：\n1 2 3 4 5 6 7 8 9 10 11 const myId = \u0026#39;青秋\u0026#39;; const myData = \u0026#39;青秋博客\u0026#39;; //1.创建虚拟DOM const VDOM = ( \u0026lt;div\u0026gt; \u0026lt;h2 id={myId}\u0026gt; \u0026lt;span\u0026gt;{myData}\u0026lt;/span\u0026gt; \u0026lt;/h2\u0026gt; \u0026lt;/div\u0026gt; ); 虚拟 DOM 中使用内联样式 在虚拟 dom 中使用内联样式（直接在标签中写 style），需要用{{ }}包裹，其中外层的{ }代表标签里要写 js 表达式，内层的{ }代表要写的是一个对象。\n另外{{ }}的 style 属性名是小驼峰的形式，比如 font-size 写成 fontSize。真实 dom 的类名是 class，**虚拟 dom 的类名是 className。**如：\n1 2 3 4 5 6 7 8 9 10 11 //真实dom内联样式 \u0026lt;div class=\u0026#39;dom\u0026#39;\u0026gt; \u0026lt;h2 style=\u0026#39;color: black;font-size: large;\u0026#39;\u0026gt;青秋博客\u0026lt;/h2\u0026gt; \u0026lt;/div\u0026gt;; //虚拟DOM内联样式 const VDOM = ( \u0026lt;div className=\u0026#39;vdom\u0026#39;\u0026gt; \u0026lt;h2 style={{ color: \u0026#39;white\u0026#39;, fontSize: \u0026#39;29px\u0026#39; }}\u0026gt;青秋博客\u0026lt;/h2\u0026gt; \u0026lt;/div\u0026gt; ); render 渲染虚拟 DOM 语法: ReactDOM . render (\u0026lt;MyComponent/\u0026gt;,document.getElementById(\u0026rsquo;test\u0026rsquo;))\n作用: 将定义的虚拟 DOM 元素渲染到页面中的真实 DOM 中显示。\n参数说明：MyComponent**是创建的虚拟 dom 对象；**document.getElementById(\u0026rsquo;test\u0026rsquo;)是根据 id 获取的真实 dom 容器，是用来用来包含虚拟 dom 的。\n函数式组件和类式组件 简单组件\n组件名必须首字母大写！！小写则会去 html 中寻找同名元素\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026lt;script type=\u0026#34;text/babel\u0026#34;\u0026gt; //1.创建函数式组件 function MyComponent(){ console.log(this); //此处的this是undefined，因为babel编译后开启了严格模式 return \u0026lt;h2\u0026gt;我是用函数定义的组件(适用于【简单组件】的定义)\u0026lt;/h2\u0026gt; } //2.渲染组件到页面 ReactDOM.render(\u0026lt;MyComponent/\u0026gt;,document.getElementById(\u0026#39;test\u0026#39;)) /* 执行了ReactDOM.render(\u0026lt;MyComponent/\u0026gt;.......之后，发生了什么？ 1.React解析组件标签，找到了MyComponent组件。 2.发现组件是使用函数定义的，随后调用该函数，将返回的虚拟DOM转为真实DOM，随后呈现在页面中。 */ \u0026lt;/script\u0026gt; 复杂组件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \u0026lt;script type=\u0026#34;text/babel\u0026#34;\u0026gt; //1.创建类式组件 class MyComponent extends React.Component { render(){ //render是放在哪里的？------ MyComponent的原型对象上，供实例使用。 //render中的this是谁？------ MyComponent的实例对象 \u0026lt;=\u0026gt; MyComponent组件实例对象。 console.log(\u0026#39;render中的this:\u0026#39;,this); return \u0026lt;h2\u0026gt;我是用类定义的组件(适用于【复杂组件】的定义)\u0026lt;/h2\u0026gt; } } //2.渲染组件到页面 ReactDOM.render(\u0026lt;MyComponent/\u0026gt;,document.getElementById(\u0026#39;test\u0026#39;)) /* 执行了ReactDOM.render(\u0026lt;MyComponent/\u0026gt;.......之后，发生了什么？ 1.React解析组件标签，找到了MyComponent组件。 2.发现组件是使用类定义的，随后new出来该类的实例，并通过该实例调用到原型上的render方法。 3.将render返回的虚拟DOM转为真实DOM，随后呈现在页面中。 */ \u0026lt;/script\u0026gt; State 状态 类的基本知识 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;1_类的基本知识\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; \u0026gt; /* 总结： 1.类中的构造器不是必须要写的，要对实例进行一些初始化的操作，如添加指定属性时才写。 2.如果A类继承了B类，且A类中写了构造器，那么A类构造器中的super是必须要调用的。 3.类中所定义的方法，都放在了类的原型对象上，供实例去使用。 */ //创建一个Person类 class Person { //构造器方法 constructor(name,age){ //构造器中的this是谁？------ 类的实例对象 this.name = name this.age = age } //一般方法 speak(){ //speak方法放在了哪里？------类的原型对象上，供实例使用 //通过Person实例调用speak时，speak中的this就是Person实例 console.log(`我叫${this.name}，我年龄是${this.age}`); } } //创建一个Student类，继承于Person类 class Student extends Person { constructor(name,age,grade){ super(name,age) this.grade = grade this.school = \u0026#39;门头沟大学\u0026#39; } //重写从父类继承过来的方法 speak(){ console.log(`我叫${this.name}，我年龄是${this.age},我读的是${this.grade}年级`); this.study() } study(){ //study方法放在了哪里？------类的原型对象上，供实例使用 //通过Student实例调用study时，study中的this就是Student实例 console.log(\u0026#39;我很努力的学习\u0026#39;); } } class Car { constructor(name,price){ this.name = name this.price = price // this.wheel = 4 } //类中可以直接写赋值语句,如下代码的含义是：给Car的实例对象添加一个属性，名为a，值为1 a = 1 wheel = 4 static demo = 100 } const c1 = new Car(\u0026#39;奔驰c63\u0026#39;,199) console.log(c1); console.log(Car.demo); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 状态即数据，状态变化会驱动视图变化，可以简单理解为存储数据的对象。 复杂组件即类定义组件有 this，那么就有 state，可以理解为一个对象的属性，可以通过 constructor()传递参数，而简单组件的 this 是 undefined 就没有 state 一说。 render 方法中的 this 就是组件实例对象 组件自定义的方法中 this 为 undefined，如何解决？ 强制绑定 this，通过函数对象的 bind() 2.箭头函数 注意！！状态必须通过 setState 进行更新，不能直接更改。\n正确的：this.setState({isHot:!isHot}) 错误的：this.state.isHot = !isHot\nstate 复杂写法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 \u0026lt;script type=\u0026#34;text/babel\u0026#34;\u0026gt; //1.创建组件 class Weather extends React.Component{ //构造器调用几次？ ------------ 1次 constructor(props){ console.log(\u0026#39;constructor\u0026#39;); super(props) //初始化状态 this.state = {isHot:false,wind:\u0026#39;微风\u0026#39;} //changeWeather是自定义的，要解决changeWeather中this指向问题 //调用bind方法会生成一个新函数，然后把新函数绑定到this实例对象上并命名为change，那么this实例对象有了名为change的方法 this.change = this.changeWeather.bind(this) } //changeWeather调用几次？ ------------ 点几次调几次 changeWeather(){ //changeWeather放在哪里？ ------------ Weather的原型对象上，供实例使用 //由于changeWeather是作为onClick的回调，所以不是通过实例调用的，是直接调用 //类中的方法默认开启了局部的严格模式，所以changeWeather中的this为undefined console.log(\u0026#39;changeWeather\u0026#39;); //获取原来的isHot值 const isHot = this.state.isHot //严重注意：状态必须通过setState进行更新,且更新是一种合并，不是替换。 this.setState({isHot:!isHot}) console.log(this); //严重注意：状态(state)不可直接更改，下面这行就是直接更改！！！ //this.state.isHot = !isHot //这是错误的写法 } //render调用几次？ ------------ 1+n次 1是初始化的那次 n是状态更新的次数 render(){ console.log(\u0026#39;render\u0026#39;); //读取状态 const {isHot,wind} = this.state //render函数中调用该类的另一个函数changeWeather，需要用this调用，否则会找不到要调用的函数！！ return \u0026lt;h1 onClick={this.change}\u0026gt;今天天气很{isHot ? \u0026#39;炎热\u0026#39; : \u0026#39;凉爽\u0026#39;}，{wind}\u0026lt;/h1\u0026gt; } } //2.渲染组件到页面 ReactDOM.render(\u0026lt;Weather/\u0026gt;,document.getElementById(\u0026#39;test\u0026#39;)) \u0026lt;/script\u0026gt; state 简单写法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \u0026lt;script type=\u0026#34;text/babel\u0026#34;\u0026gt; //1.创建组件 class Weather extends React.Component{ //初始化状态 state = {isHot:false,wind:\u0026#39;微风\u0026#39;} //自定义方法------------要用赋值语句的形式+箭头函数 changeWeather = ()=\u0026gt;{ const isHot = this.state.isHot this.setState({isHot:!isHot}) } render(){ const {isHot,wind} = this.state return \u0026lt;h1 onClick={this.changeWeather}\u0026gt;今天天气很{isHot ? \u0026#39;炎热\u0026#39; : \u0026#39;凉爽\u0026#39;}，{wind}\u0026lt;/h1\u0026gt; } } //2.渲染组件到页面 ReactDOM.render(\u0026lt;Weather/\u0026gt;,document.getElementById(\u0026#39;test\u0026#39;)) \u0026lt;/script\u0026gt; Props props 用于父组件向子组件传递数据，props 只读无法修改（）\n基本使用 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \u0026lt;script type=\u0026#34;text/babel\u0026#34;\u0026gt; //创建组件 class Person extends React.Component{ render(){ const {name,age,sex} = this.props return ( \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;姓名：{name}\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;性别：{sex}\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;年龄：{age+1}\u0026lt;/li \u0026lt;/ul\u0026gt; ) } } //渲染组件到页面 ReactDOM.render(\u0026lt;Person name=\u0026#34;jerry\u0026#34; age={19} sex=\u0026#34;男\u0026#34;/\u0026gt;,document.getElementById(\u0026#39;test1\u0026#39;)) const p = {name:\u0026#39;老刘\u0026#39;,age:18,sex:\u0026#39;女\u0026#39;} ReactDOM.render(\u0026lt;Person {...p}/\u0026gt;,document.getElementById(\u0026#39;test3\u0026#39;)) \u0026lt;/script\u0026gt; 类型限制 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;../js/prop-types.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/babel\u0026#34;\u0026gt; //创建组件 class Person extends React.Component{ render(){ const {name,age,sex} = this.props //props是只读的 //this.props.name = \u0026#39;jack\u0026#39; //此行代码会报错，因为props是只读的 return ( \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;姓名：{name}\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;性别：{sex}\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;年龄：{age+1}\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; ) } } //对标签属性进行类型、必要性的限制 Person.propTypes = { name:PropTypes.string.isRequired, //限制name必传，且为字符串 sex:PropTypes.string,//限制sex为字符串 age:PropTypes.number,//限制age为数值 speak:PropTypes.func,//限制speak为函数 } //指定默认标签属性值 Person.defaultProps = { sex:\u0026#39;男\u0026#39;,//sex默认值为男 age:18 //age默认值为18 } //渲染组件到页面 function speak(){console.log(\u0026#39;我说话了\u0026#39;);} ReactDOM.render(\u0026lt;Person name={100} speak={speak}/\u0026gt;,document.getElementById(\u0026#39;test1\u0026#39;)) const p = {name:\u0026#39;老刘\u0026#39;,age:18,sex:\u0026#39;女\u0026#39;} ReactDOM.render(\u0026lt;Person {...p}/\u0026gt;,document.getElementById(\u0026#39;test3\u0026#39;)) \u0026lt;/script\u0026gt; 订阅-发布 PubSub 订阅发布用于兄弟组件传递数据，在之前，一个父亲有两个儿子，两个儿子要通信，要借助父亲来传达消息，而现在使用发布订阅，不需要借助父亲，两个儿子可以直接通信。\n如 List 和 Search 两个兄弟组件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 import React, { Component } from \u0026#39;react\u0026#39; import PubSub from \u0026#39;pubsub-js\u0026#39; export default class List extends Component { state = { //初始化状态 users:[], //users初始值为数组 isFirst:true, //是否为第一次打开页面 isLoading:false,//标识是否处于加载中 err:\u0026#39;\u0026#39;,//存储请求相关的错误信息 } componentDidMount(){ this.token = PubSub.subscribe(\u0026#39;atguigu\u0026#39;,(_,stateObj)=\u0026gt;{ this.setState(stateObj) }) } componentWillUnmount(){ PubSub.unsubscribe(this.token) } render() { const {users,isFirst,isLoading,err} = this.state return ( \u0026lt;div className=\u0026#34;row\u0026#34;\u0026gt; { isFirst ? \u0026lt;h2\u0026gt;欢迎使用，输入关键字，随后点击搜索\u0026lt;/h2\u0026gt; : isLoading ? \u0026lt;h2\u0026gt;Loading......\u0026lt;/h2\u0026gt; : err ? \u0026lt;h2 style={{color:\u0026#39;red\u0026#39;}}\u0026gt;{err}\u0026lt;/h2\u0026gt; : users.map((userObj)=\u0026gt;{ return ( \u0026lt;div key={userObj.id} className=\u0026#34;card\u0026#34;\u0026gt; \u0026lt;a rel=\u0026#34;noreferrer\u0026#34; href={userObj.html_url} target=\u0026#34;_blank\u0026#34;\u0026gt; \u0026lt;img alt=\u0026#34;head_portrait\u0026#34; src={userObj.avatar_url} style={{width:\u0026#39;100px\u0026#39;}}/\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;p className=\u0026#34;card-text\u0026#34;\u0026gt;{userObj.login}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; ) }) } \u0026lt;/div\u0026gt; ) } } export default class Search extends Component { search = ()=\u0026gt;{ //获取用户的输入(连续解构赋值+重命名) const {keyWordElement:{value:keyWord}} = this //发送请求前通知List更新状态 PubSub.publish(\u0026#39;atguigu\u0026#39;,{isFirst:false,isLoading:true}) //发送网络请求 axios.get(`/api1/search/users?q=${keyWord}`).then( response =\u0026gt; { //请求成功后通知List更新状态 PubSub.publish(\u0026#39;atguigu\u0026#39;,{isLoading:false,users:response.data.items}) }, error =\u0026gt; { //请求失败后通知App更新状态 PubSub.publish(\u0026#39;atguigu\u0026#39;,{isLoading:false,err:error.message}) } ) } render() { return ( \u0026lt;section className=\u0026#34;jumbotron\u0026#34;\u0026gt; \u0026lt;h3 className=\u0026#34;jumbotron-heading\u0026#34;\u0026gt;搜索github用户\u0026lt;/h3\u0026gt; \u0026lt;div\u0026gt; \u0026lt;input ref={c =\u0026gt; this.keyWordElement = c} type=\u0026#34;text\u0026#34; placeholder=\u0026#34;输入关键词点击搜索\u0026#34;/\u0026gt;\u0026amp;nbsp; \u0026lt;button onClick={this.search}\u0026gt;搜索\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/section\u0026gt; ) } } Ref 为自定义的标签打标识\n字符串形式的 ref string 形式的 ref 存在效率问题，不推荐使用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 \u0026lt;script type=\u0026#34;text/babel\u0026#34;\u0026gt; //创建组件 class Demo extends React.Component{ //展示左侧输入框的数据 showData = ()=\u0026gt;{ const {input1} = this.refs alert(input1.value) } //展示右侧输入框的数据 showData2 = ()=\u0026gt;{ const {input2} = this.refs alert(input2.value) } render(){ return( \u0026lt;div\u0026gt; //ref相当于属性id，即标识了一个名为input1的input标签 \u0026lt;input ref=\u0026#34;input1\u0026#34; type=\u0026#34;text\u0026#34; placeholder=\u0026#34;点击按钮提示数据\u0026#34;/\u0026gt;\u0026amp;nbsp; \u0026lt;button onClick={this.showData}\u0026gt;点我提示左侧的数据\u0026lt;/button\u0026gt;\u0026amp;nbsp; \u0026lt;input ref=\u0026#34;input2\u0026#34; onBlur={this.showData2} type=\u0026#34;text\u0026#34; placeholder=\u0026#34;失去焦点提示数据\u0026#34;/\u0026gt; \u0026lt;/div\u0026gt; ) } } //渲染组件到页面 ReactDOM.render(\u0026lt;Demo a=\u0026#34;1\u0026#34; b=\u0026#34;2\u0026#34;/\u0026gt;,document.getElementById(\u0026#39;test\u0026#39;)) \u0026lt;/script\u0026gt; 回调函数的 ref 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 \u0026lt;script type=\u0026#34;text/babel\u0026#34;\u0026gt; //创建组件 class Demo extends React.Component{ //展示左侧输入框的数据 showData = ()=\u0026gt;{ const {input1} = this alert(input1.value) } //展示右侧输入框的数据 showData2 = ()=\u0026gt;{ const {input2} = this alert(input2.value) } render(){ return( \u0026lt;div\u0026gt; //input的ref为c，把c赋给this实例对象，即把c标识的这个input标签赋给this对象，同时把input标签命名为myinput \u0026lt;input ref={c =\u0026gt; this.myinput = c } type=\u0026#34;text\u0026#34; placeholder=\u0026#34;点击按钮提示数据\u0026#34;/\u0026gt;\u0026amp;nbsp; \u0026lt;button onClick={this.showData}\u0026gt;点我提示左侧的数据\u0026lt;/button\u0026gt;\u0026amp;nbsp; \u0026lt;input onBlur={this.showData2} ref={c =\u0026gt; this.input2 = c } type=\u0026#34;text\u0026#34; placeholder=\u0026#34;失去焦点提示数据\u0026#34;/\u0026gt;\u0026amp;nbsp; \u0026lt;/div\u0026gt; ) } } //渲染组件到页面 ReactDOM.render(\u0026lt;Demo a=\u0026#34;1\u0026#34; b=\u0026#34;2\u0026#34;/\u0026gt;,document.getElementById(\u0026#39;test\u0026#39;)) \u0026lt;/script\u0026gt; createRef() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \u0026lt;script type=\u0026#34;text/babel\u0026#34;\u0026gt; //创建组件 class Demo extends React.Component{ /* React.createRef调用后可以返回一个容器，该容器可以存储被ref所标识的节点,该容器是\u0026#34;专人专用\u0026#34;的 */ myRef = React.createRef() myRef2 = React.createRef() //展示左侧输入框的数据 showData = ()=\u0026gt;{ alert(this.myRef.current.value); } //展示右侧输入框的数据 showData2 = ()=\u0026gt;{ alert(this.myRef2.current.value); } render(){ return( \u0026lt;div\u0026gt; \u0026lt;input ref={this.myRef} type=\u0026#34;text\u0026#34; placeholder=\u0026#34;点击按钮提示数据\u0026#34;/\u0026gt;\u0026amp;nbsp; \u0026lt;button onClick={this.showData}\u0026gt;点我提示左侧的数据\u0026lt;/button\u0026gt;\u0026amp;nbsp; \u0026lt;input onBlur={this.showData2} ref={this.myRef2} type=\u0026#34;text\u0026#34; placeholder=\u0026#34;失去焦点提示数据\u0026#34;/\u0026gt;\u0026amp;nbsp; \u0026lt;/div\u0026gt; ) } } //渲染组件到页面 ReactDOM.render(\u0026lt;Demo a=\u0026#34;1\u0026#34; b=\u0026#34;2\u0026#34;/\u0026gt;,document.getElementById(\u0026#39;test\u0026#39;)) \u0026lt;/script\u0026gt; Ajax 和 Axios 前置说明 React 本身只关注于界面, 并不包含发送 ajax 请求的代码 前端应用需要通过 ajax 请求与后台进行交互(json 数据) react 应用中需要集成第三方 ajax 库(或自己封装) Ajax 请求库 jQuery: 比较重, 如果需要另外引入不建议使用 axios: 轻量级, 建议使用 封装 XmlHttpRequest 对象的 ajax（XHR） promise 风格 可以用在浏览器端和 node 服务器端 Axios 请求 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 1)\tGET请求 axios.get(\u0026#39;/user?ID=12345\u0026#39;) .then(function (response) { console.log(response.data); }) .catch(function (error) { console.log(error); }); axios.get(\u0026#39;/user\u0026#39;, { params: { ID: 12345 } }) .then(function (response) { console.log(response); }) .catch(function (error) { console.log(error); }); 2)\tPOST请求 axios.post(\u0026#39;/user\u0026#39;, { firstName: \u0026#39;Fred\u0026#39;, lastName: \u0026#39;Flintstone\u0026#39; }) .then(function (response) { console.log(response); }) .catch(function (error) { console.log(error); }); Server.js 代理 除了熟知的 nginx 代理，还可以用 js 实现代理。 代理是为了解决跨域请求，在前后端交互中，前端发给后端的请求被后端接收到，但是后端返回给前端的数据却无法被前端接收。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 export default class App extends React.Component { getStudentData = () =\u0026gt; { axios.get(\u0026#39;http://localhost:3000/api1/students\u0026#39;).then( (response) =\u0026gt; { console.log(\u0026#39;成功了\u0026#39;, response.data); }, (error) =\u0026gt; { console.log(\u0026#39;失败了\u0026#39;, error); } ); }; getCarData = () =\u0026gt; { axios.get(\u0026#39;http://localhost:3000/api2/cars\u0026#39;).then( (response) =\u0026gt; { console.log(\u0026#39;成功了\u0026#39;, response.data); }, (error) =\u0026gt; { console.log(\u0026#39;失败了\u0026#39;, error); } ); }; render() { return ( \u0026lt;div\u0026gt; \u0026lt;button onClick={this.getStudentData}\u0026gt;点我获取学生数据\u0026lt;/button\u0026gt; \u0026lt;button onClick={this.getCarData}\u0026gt;点我获取汽车数据\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; ); } } //引入了 http-proxy-middleware 库，它提供了一种简单的方法来创建代理中间件。 const proxy = require(\u0026#39;http-proxy-middleware\u0026#39;); module.exports = function (app) { app.use( proxy(\u0026#39;/api1\u0026#39;, { //遇见/api1前缀的请求，就会触发该代理配置 target: \u0026#39;http://localhost:5000\u0026#39;, //请求转发给谁 changeOrigin: true, //控制服务器收到的请求头中Host的值。Host请求标识请求来源 pathRewrite: { \u0026#39;^/api1\u0026#39;: \u0026#39;\u0026#39; }, //重写请求路径(必须) }), proxy(\u0026#39;/api2\u0026#39;, { target: \u0026#39;http://localhost:5001\u0026#39;, changeOrigin: true, pathRewrite: { \u0026#39;^/api2\u0026#39;: \u0026#39;\u0026#39; }, }) ); }; 路由 哈希路由和浏览器路由\nBrowserRouter 与 HashRouter 的区别\n1.底层原理不一样：\nBrowserRouter 使用的是 H5 的 history API，不兼容 IE9 及以下版本。\nHashRouter 使用的是 URL 的哈希值。\n2.path 表现形式不一样\nBrowserRouter 的路径中没有#,例如：localhost:3000/demo/test\nHashRouter 的路径包含#,例如：localhost:3000/#/demo/test\n3.刷新后对路由 state 参数的影响\n(1).BrowserRouter 没有任何影响，因为 state 保存在 history 对象中。\n(2).HashRouter 刷新后会导致路由 state 参数的丢失！！！\n4.备注：HashRouter 可以用于解决一些路径错误相关的问题。\n编程式路由导航\n不需要用户触发，可以自动跳转链接\n借助 this.prosp.history 对象上的 API 对操作路由跳转、前进、后退\n-this.prosp.history.push()\n-this.prosp.history.replace()\n-this.prosp.history.goBack()\n-this.prosp.history.goForward()\n-this.prosp.history.go()\n向路由组件传参 ","date":"2024-10-08T00:35:44Z","permalink":"/zh-cn/post/2024/10/%E6%B5%85%E5%AD%A6react%E5%92%8Cjsx/","title":"浅学React和JSX"},{"content":"Flink 知识图谱 Flink 发展 Apache Flink 诞生于柏林工业大学的一个研究性项目，原名 StratoSphere 。2014 年，由 StratoSphere 项目孵化出 Flink，并于同年捐赠 Apache，之后成为 Apache 的顶级项目。2019 年 1 年，阿里巴巴收购了 Flink 的母公司 Data Artisans，并宣布开源内部的 Blink，Blink 是阿里巴巴基于 Flink 优化后的版本，增加了大量的新功能，并在性能和稳定性上进行了各种优化，经历过阿里内部多种复杂业务的挑战和检验。同时阿里巴巴也表示会逐步将这些新功能和特性 Merge 回社区版本的 Flink 中，因此 Flink 成为目前最为火热的大数据处理框架。\n四代计算引擎 在国外一些社区，有很多人将大数据的计算引擎分成了 4 代，当然，也有很多人不会认同。我们先姑且这么认为和讨论。\n首先第一代的计算引擎，无疑就是 Hadoop 承载的 MapReduce。这里大家应该都 不会对 MapReduce 陌生，它将计算分为两个阶段，分别为 Map 和 Reduce。对于上层应用来说，就不得不想方设法去拆分算法，甚至于不得不在上层应用实现 多个 Job 的串联，以完成一个完整的算法，例如迭代计算 。 由于这样的弊端，催生了支持 DAG 框架的产生。 因此，支持 DAG 的框架被划分为第二代计算引擎。如 Tez 以及更上层的 Oozie。这里我们不去细究各种 DAG 实现之间的区别，不过对于当时的 Tez 和 Oozie 来说，大多还是批处理的任务。 接下来就是以 Spark 为代表的第三代的计算引擎。第三代计算引擎的特点主要 是 Job 内部的 DAG 支持（不跨越 Job），以及强调的准实时计算。在这里，很多人也会认为第三代计算引擎也能够很好的运行批处理的 Job。 随着第三代计算引擎的出现，促进了上层应用快速发展，例如各种迭代计算的性能以及对流计算和 SQL 等的支持。 Flink 的诞生就被归在了第四代。这应该主 要表现在 Flink 对流计算的支持，以及更一步的实时性上面。当然 Flink 也可 以支持 Batch 的任务，以及 DAG 的运算。 Flink 简介 Flink 是一个分布式、高性能、**有状态**的流处理框架，它能够对有界和无界的数据流进行高效的处理。Flink 的 **核心是流处理（DataStream），当然也支持批处理（DataSet），Flink 将批处理看成是流处理的一种特殊情况，即数据流是有 明确界限的。**这和 Spark Streaming 的思想是完全相反的，Spark Streaming 的核心是批处理，它将流处理看成是批处理的一种特殊情况， 即把数据流进行极小粒度的拆分，拆分为多个微批处理。 Flink 特点 支持高吞吐、低延迟、高性能的流处理 结果准确，Flink 提供了事件时间和处理时间，对乱序数据仍能提供一直准确的结果 支持高度灵活的窗口（Window）操作，支持基于 time、count、session， 以及 data-driven 的窗口操作 支持基于轻量级分布式快照（Snapshot）实现的容错 一个运行时同时支持 Batch on Streaming 处理和 Streaming 处理 Flink 在 JVM 内部实现了自己的内存管理 支持迭代计算，Spark 也支持 支持程序自动优化：避免特定情况下 Shuffle、排序等昂贵操作，中间结果有必要进行缓存 批处理和流处理 批处理\n有界、持久、大量，一般用于离线计算 流处理\n无界、实时，流处理方式无需对整个数据集执行操作，而是对通过系统传输的每个数据项执行操作，一般用于实时统计 在 Spark 生态体系中，对于批处理和流处理采用了不同的技术框架，批处理由 SparkSQL 实现，流处理由 Spark Streaming 实现，这也是大部分框架采用的策略，使用独立的处理器实现批处理和流处理，而 Flink 可以同时实现批处理和流处理，Flink 将批处理（即处理 有限的静态数据）视作一种特殊的流处理，即把数据看作是有界的 ！\n有界流和无界流 无界数据流：\n有定义流的开始，但没有定义流的结束； 它们会无休止的产生数据 无界流的数据必须持续处理，即数据被摄取后需要立刻处理 我们不能等到所有数据都到达再处理，因为输入是无限的。 有界数据流：\n有定义流的开始，也有定义流的结束 有界流可以在摄取所有数据后再进行计算 有界流所有数据可以被排序，所以并不需要有序摄取 有界流处理通常被称为批处理。 Flink 和 Spark Streaming Spark 本质是批处理\nSpark 数据模型：Spak 采用 RDD 模型，Spark Streaming 的DStream 实际上也就是一组组小批据 RDD 的集合 Spark 运行时架构：Spark 是批计算，将 DAG 划分为不同的 stage,一个完成后才可以计算下一个 Flink 以流处理为根本\nFlink 数据模型：Flink 基本据模型是数据流，以及事件(Event)序列 Flink 运行时架构：Flink 是标准的流执行模式，一个事件在一个节点处理完后可以直接发往下一个节点进行处理 Flink 三层核心架构 下图为 Flink 技术栈的核心组成部分，由上而下分别是 API \u0026amp; Libraries 层、Runtime 核心层以及物理部署层。\n**API \u0026amp; Libraries 层，提供了面向流式处理的接口（DataStream API）、面向批处理的接口（DataSet API）、用于复杂事件处理的 CEP 库**、用于结构化数据查询的 SQL \u0026amp; Table 库、基于**批处理的机器学习库 FlinkML 和 图形处理库 Gelly**。 Runtime 核心层，这一层是 Flink 分布式计算框架的核心实现层，包括作业转换，任务调度，资源分配，任务执行等功能，基于这一层的实现，可以在流式引擎下同时进行流处理和批处理。 **物理部署层，**用于支持在不同平台上部署运行 Flink 应用。 API \u0026amp; Libraries 层详解 在 API \u0026amp; Libraries 层，有如下更细致的划分，API 的一致性由下至上依次递增，接口的表现能力由下至上依次递减。 SQL\u0026amp;Table API 层 SQL \u0026amp; Table API 同时适用于批处理和流处理，这意味着可以对有界数据流和无界数据流以相同的语义进行查询，并产生相同的结果。除了基本查询外， 它还支持自定义的标量函数，聚合函数以及表值函数，可以满足多样化的查询需求。\nDataStream \u0026amp; DataSet API 层 DataStream \u0026amp; DataSet API 是 Flink 数据处理的核心 API，支持使用 Java 语言或 Scala 语言进行调用，提供了数据读取，数据转换和数据输出等一系列常用操作的封装。\nStateful Stream Processing 层 Stateful Stream Processing 是最低级别的抽象，它通过 Process Function 函数内嵌到 DataStream API 中。 Process Function 是 Flink 提供的最底层 API，具有最大的灵活性，允许开发者对时间和状态进行细粒度的控制。\n三种 Time 概念 在 Flink 中，如果以时间段划分边界的话，那么时间就是一个极其重要的字段。 Flink 中的时间有三种类型，如下图所示： Event Time：是事件创建的时间。它通常由事件中的时间戳描述，即事件本身就要携带时间信息，例如采集的日志数据中，每一条日志都会记录自己的生成时间，Flink 通过时间戳分配器访问事件时间戳。 Ingestion Time：是数据进入 Flink 的时间。 Processing Time：是每一个执行基于时间操作的算子的本地系统时间，与机器相关，默认的时间属性就是 Processing Time 在 Flink 的流式处理中，绝大部分的业务都会使用 eventTime，一般只在 eventTime 无法使用时，才会被迫使用 ProcessingTime\nWaterMark 水印 流处理从事件产生，到流经 source，再到 operator，中间有一个过程和时间，虽然大部分情况下，流到 operator 的数据都是按照事件产生的 时间顺序来的，但是也不排除由于网络、背压等原因，导致乱序的产生，所谓乱序，就是指 Flink 接收到的事件的先后顺序不是严格按照事件的 Event Time 顺序排列的，所以 Flink 最初设计的时候，就考虑到了网络延迟，网络乱序等问题，所以提出了一个抽象概念：水印（WaterMark） 当出现乱序，如果只根据 EventTime 决定 Window 的运行，我们不能明确数据是否全部到位，但又不能无限期的等下去， 此时必须要有个机制来保证一个特定的时间后，必须触发 Window 去进行计算了， 这个特别的机制，就是 Watermark。 Watermark 是用于处理乱序事件的，通常用 Watermark 机制结合 Window 来实现。 数据流中的 Watermark 用于表示 timestamp 小于 Watermark 的数据，都已经到达了，因此，Window 的执行也是由 Watermark 触发的。 Watermark 可以理解成一个延迟触发机制，我们可以设置 Watermark 的延时时长 t ，每次系统会校验已经到达的数据中最大的 maxEventTime，然后认定 EventTime 小于 maxEventTime - t 的所有数据都已经到达，如果有窗口的停止时间等于 maxEventTime - t，那么这个窗口被触发执行。 对延迟数据的理解\n延迟数据是指： 在当前窗口【假设窗口范围为 10-15】已经计算之后，又来了一个属于该窗口的 数据【假设事件时间为 13】，这时候仍会触发 Window 操作，这种数据就称为 延迟数据。 那么问题来了，延迟时间怎么计算呢？\n假设窗口范围为 10-15，延迟时间为 2s，则只要 WaterMark=15+2， 10-15 这个窗口就不能再触发 Window 操作，即使新来的数据的 Event Time 属 于这个窗口时间内 。 Windows 窗口类型 在大多数场景下，我们需要统计的数据流都是无界的，因此我们无法等待整个数据流终止后才进行统 计。通常情况下，我们只需要对某个时间范围或者数量范围内的数据进行统计分析（把无限数据分割成块进行计算分析）：如每隔五分钟统计一次过去一小时内所有商品的点击量；或者每发生 1000 次点击后，都去统计一下每个商品点击率的占比。在 Flink 中，可以使用窗口 (Window) 来实现这类功能。按照统计维度的不同，Flink 中的窗口可以分为时间窗口 (Time Windows) 和计数窗口 (Count Windows) 。\n时间窗口 时间窗口以时间点来定义窗口的开始（start）和结束（end），所以截取出的就是某一时间段的数据。到达结束时间时，窗口不再收集数据，触发计算输出结果，并将窗口关闭销毁。所以可以说基本思路就是“定点发车”。\n滚动窗口 Tumbling Windows 滚动窗口指彼此之间没有重叠的窗口。例如：每隔 1 小时统计过去 1 小时内的商品点击量，那么 1 天就只能分为 24 个窗口，每个窗口之间是不存在重叠的。\n特点：时间对齐，长度固定，窗口不重叠 滑动窗口 Sliding Windows 滑动窗口用于滚动进行聚合分析，例如：每隔 6 分钟统计一次过去一小时内所有商品的点击量，那么 1 天可以分为 240 个窗口，统计窗口之间存在重叠。\n特点：时间对齐，长度固定，窗口重叠 会话窗口 Session Windows 当用户在进行持续浏览时，可能每时每刻都会有点击数据，例如在活动区间内，用户可能频繁的将某类 商品加入和移除购物车，而你只想知道用户本次浏览最终的购物车情况，此时就等用户持有的会话结束后再进行统计。想要实现这类统计，可以通过 Session Windows 来进行实现。\n特点：时间不对齐，长度不固定，窗口不重叠 全局窗口 Global Windows 全局窗口会将所有 key 相同的元素分配到同一个窗口中，其通常配合触发器 (trigger) 进行使用。如果没有相应触发器，则计算将不会被执行。 计数窗口 计数窗口基于元素的个数来截取数据，到达固定的个数时就触发计算并关闭窗口。每个窗口截取数据的个数， 就是窗口的大小。基本思路是“人齐发车”。\nCount Windows 用于以数量为维度来进行数据聚合，同样也分为滚动窗口和滑动窗口，实现方式也和 时间窗口基本一致。\n注意：CountWindow 的 window_size 指的是相同 Key 的元素的个数，不是输入的所有元素的总数\ntumbling-count-window (无重叠数据) sliding-count-window (有重叠数据) 状态管理 参考博客：Flink 状态管理详解（超全收藏）_flink 状态后端的应用场景-CSDN 博客\n状态的 Flink 官方定义 当前计算流程需要依赖到之前计算的结果，那么之前计算的结果就是状态。\n状态分类及状态存储类型 相对于其他流计算框架，Flink 一个比较重要的特性就是其**支持有状态计算，即你可以将中间的计算结果进行保存，并提供给后续的计算使用（Spark 的 RDD 也可以保存计算结果供下个 RDD 使用，DAG）\n具体而言，Flink 有两种基本类型的状态 (State) ： 键控状态（Keyed State） 与算子状态（Operator State）。**这两种状态可以以两种形式存在：原始状态(raw state) 、托管状态(managed state），托管状态是由 Flink 框架管理的状态，原始状态由用户自行管理状态。\n算子状态 算子状态是和算子进行绑定的，与 Key 无关，一个算子的状态不能被其他算子所访问到。官方文档上对 Operator State 的解释是：each operator state is bound to one parallel operator instance，所以更为确切的说一个算子状态是与一个并发的算子实例所绑定的，即假设算子的并行度是 2，那么其应有两个对应的算子状态： 算子状态存储类型\nListState：存储列表类型的状态。 UnionListState：存储列表类型的状态，与 ListState 的区别在于：如果并行度发生变化， ListState 会将该算子的所有并发的状态实例进行汇总，然后均分给新的 Task；而 UnionListState 只是将所有并发的状态实例汇总起来，具体的划分行为则由用户进行定义。 BroadcastState：用于广播的算子状态。 键控状态 键控状态是一种特殊的算子状态，即状态是根据 key 值进行区分的，Flink 会为每类键值维护一个状态实例。如下图所示，每个颜色代表不同 key 值，对应四个不同的状态实例。需要注意的 是键控状态只能在 KeyedStream 上进行使用，我们可以通过 stream.keyBy(\u0026hellip;) 来得到 KeyedStream 。 键控状态存储类型\nValueState：存储单值类型的状态。可以使用 update(T) 进行更新，并通过 T value() 进行 检索。 ListState：存储列表类型的状态。可以使用 add(T) 或 addAll(List) 添加元素；并通过 get() 获得整个列表。 ReducingState：用于存储经过 ReduceFunction 计算后的结果，使用 add(T) 增加元素。 AggregatingState：用于存储经过 AggregatingState 计算后的结果，使用 add(IN) 添加元素。 FoldingState：已被标识为废弃，会在未来版本中移除，官方推荐使用 AggregatingState 代 替。 MapState：维护 Map 类型的状态。 Broadcast State Broadcast State 是 Flink 1.5 引入的新特性。在开发过程中，如果遇到需要 下发/广播配置、规则等低吞吐事件流到下游所有 task 时，就可以使用 Broadcast State 特性。下游的 task 接收这些配置、规则并保存为 BroadcastState, 将这些配置应用到另一个数据流的计算中 。\n状态后端(持久化存储) 默认情况下，所有的状态都存储在 JVM 的堆内存中，在状态数据过多的情况下，这种方式很有可能导致内存溢出，因此 Flink 该提供了其它方式来存储状态数据，这些存储方式统一称为状态后端 (或状态管理器)，主要有以下三种：\nMemoryStateBackend\n默认的方式，即基于 JVM 的堆内存进行存储，主要适用于本地开发和调试。 FsStateBackend\n基于文件系统进行存储，可以是本地文件系统，也可以是 HDFS 等分布式文件系统。 需要注意而是虽然选择使用了 FsStateBackend ，但正在进行的数据仍然是存储在 TaskManager 的内存中的，只有在 checkpoint 时，才会将状态快照写入到指定文件系统上。 RocksDBStateBackend\nRocksDBStateBackend 是 Flink 内置的第三方状态管理器，采用嵌入式的 key-value 型数据库 RocksDB 来存储正在进行的数据。等到 checkpoint 时，再将其中的数据持久化到指定的文件系统中， 所以采用 RocksDBStateBackend 时也需要配置持久化存储的文件系统。之所以这样做是因为 RocksDB 作为嵌入式数据库安全性比较低，但比起全文件系统的方式，其读取速率更快；比起全内存的方式，其 存储空间更大，因此它是一种比较均衡的方案。 Flink 算子 DataSet 批处理算子 Source 算子 fromCollection：从本地集合读取数据 readTextFile：从文件中读取 readTextFile：遍历目录 readTextFile：读取压缩文件 Transform 转换算子 Transform 算子基于 Source 算子操作，所以要首先构建 Flink 执行环境及 Source 算子。\n数据源读入数据之后，就可以使用各种转换算子，将一个或多个 DataStream 转换为新的 DataStream。\n基本转换算子（map/ filter/ flatMap）\nmap：将 DataSet 中的每一个元素转换为另外一个元素 flatMap：将 DataSet 中的每一个元素转换为 0\u0026hellip;n 个元素 filter：过滤出来一些符合条件的元素，返回 boolean 值为 true 的元素 聚合算子（Aggregation）\nreduce：可以对一个 dataset 或者一个 group 来进行聚合计算，最终聚合成一个元素 reduceGroup：将一个 dataset 或者一个 group 聚合成一个或多个元素。reduceGroup 是 reduce 的一种优化方案； 它会先分组 reduce，然后在做整体的 reduce；这样做的好处就是可以减少网络 IO minBy 和 maxBy：选择具有最小值或最大值的元素 Aggregate：在数据集上进行聚合求最值（最大值、最小值），注意： 使用 aggregate，只能使用字段索引名或索引名称来进行分组 groupBy(0) ，否则会报一下错误: Exception in thread \u0026ldquo;main\u0026rdquo; java.lang.UnsupportedOperationException: Aggregate does not support grouping with KeySelector functions, yet. \u0026hellip;\u0026hellip;\u0026hellip; Sink 输出算子 Flink 作为数据处理框架，最终还是要把计算处理的结果写入外部存储，为外部应用提供 支持。\ncollect 将数据输出到本地集合 writeAsText 将数据输出到文件 DataStream 流处理算子 流处理算子和批处理算子差不多，就不详细解释了。\n参考博客：一文学完 Flink 流计算常用算子（Flink 算子大全）_flink 算子 scala-CSDN 博客\nFlink 容错 Checkpoint 机制 Flink 的 checkpoint 机制原理来自“Chandy-Lamport algorithm”算法\n为了使状态具有良好的容错性，Flink 提供了检查点机制 (CheckPoints) 。通过检查点机制， Flink 定期在数据流上生成 checkpoint barrier ，当某个算子收到 barrier 时，即会基于当前状态生成一份快照，然后再将该 barrier 传递到下游算子，下游算子接收到该 barrier 后，也基于当前状态生成一份快照，依次传递直至到最后的 Sink 算子上。当出现异常后，Flink 就可以根据最近的一次的快照数据将所有算子恢复到先前的状态。（Spark 也有 Checkpoint 机制）\n简单理解为 checkpoint 是把 state 数据定时持久化存储了 Flink CEP Complex Event Processing，复杂事件处理，Flink CEP 是一个基于 Flink 的复杂事件处理库，可以从多个数据流中发现复杂事件，识别有意义的事件（例如机会或者威胁），并尽快的做出响应，而不是需要等待几天或则几个月相当长的时间，才发现问题。\n使用场景 检测恶意用户和刷屏用户\n实时反作弊和风控\n实时营销\n实时网络攻击检测\nCEP API CEP API 的核心是 Pattern(模式) API，它允许你快速定义复杂的事件模式。每 个模式包含多个阶段（stage）或者也可称为状态（state）。从一个状态切换到另一个状态，用户可以指定条件，这些条件可以作用在邻近的事件或独立事件上。\nFlink CDC CDC 是 Change Data Capture（变更数据获取）的简称。核心思想是，监测并捕获数据库的变动（包括数据或数据表的插入、更新以及删除等，和 Flume 很像，不过 Flume 是监控的系统日志），将这些变更按发生的顺序完整记录下来，写入到消息中间件中以供其他服务进行订阅及消费。 在广义的概念上，只要能捕获数据变更的技术，我们都可以称为 CDC 。通常我们说的 CDC 技术主要面向数据库的变更，是一种用于捕获数据库中数据变更的技术。 CDC 技术应用场景非常广泛：\n数据同步，用于备份，容灾；\n数据分发，一个数据源分发给多个下游；\n数据采集(E)，面向数据仓库/数据湖的 ETL 数据集成 CDC 种类 CDC 主要分为基于查询和基于 Binlog 两种方式，我们主要了解一下这两种之间的区别： Flink SQL Flink SQL 是 Flink 实时计算为简化计算模型，降低用户使用实时计算门槛而设计的一套符合标准 SQL 语义的开发语言（为了降低 Spark 门槛，也有 Spark SQL；为了降低 HBase 门槛，有了 Phoneix；为了方便的操作 HDFS 文件，有了 Hive SQL\u0026hellip;\u0026hellip;）。 自 2015 年开始，阿里巴巴开始调 研开源流计算引擎，最终决定基于 Flink 打造新一代计算引擎，针对 Flink 存在的不足进行优化和改进，并且在 2019 年初将最终代码开源，也就是我们熟知 的 Blink。Blink 在原来的 Flink 基础上最显著的一个贡献就是 Flink SQL 的 实现。 Flink SQL 是面向用户的 API 层，在我们传统的流式计算领域，比如 Storm、 Spark Streaming 都会提供一些 Function 或者 Datastream API，用户通过 Java 或 Scala 写业务逻辑，这种方式虽然灵活，但有一些不足，比如具备一定门槛且调优较难，随着版本的不断更新，API 也出现了很多不兼容的地方。在这个背景下，毫无疑问，SQL 就成了我们最佳选择！ ","date":"2024-09-05T23:07:21Z","permalink":"/zh-cn/post/2024/09/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E-flink%E4%B8%87%E5%AD%97%E8%AF%A6%E8%A7%A3%E5%8F%B2%E4%B8%8A%E6%9C%80%E5%85%A8/","title":"一文搞懂大数据流式计算引擎 Flink【万字详解，史上最全】"},{"content":"Spark 简介 Spark 于 2009 年诞生于加州大学伯克利分校 AMPLab，2013 年被捐赠给 Apache 软件基金会，2014 年 2 月成为 Apache 的顶级项目。\n相对于 MapReduce 的批处理计算，Spark 基于内存计算，可以带来上百倍的性能提升，因此它成为继 MapReduce 之后，最为广泛使用的分布式计算框架、大数据分析引擎。\nSpark 特点 快：采用DAG 执行引擎，支持循环数据流和内存计算，使得 Spark 速度更快，在内存中的速度 是 Hadoop MR 的百倍，在磁盘上的速度是 Hadoop MR 的十倍(官网数据)。 通用：Spark 提供了统一的解决方案。Spark 可以⽤于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同⼀个应用中无缝使用。 易用：Spark 支持 Java、Python、Scala 的 API 和超过 80 种⾼级算法，⽽且⽀持交互式的 Python 和 Scala 的 shell。 兼容：Spark 可以使⽤ Hadoop 的 YARN 和 Apache Mesos 作为它的资源管理和调度器，器，并且不需要任何数据迁移就可以处理所有 Hadoop 支持的数据，包括 HDFS、HBase 和 Cassandra 等。Spark 也可以不依赖于第三⽅的资源管理和调度器，它实现了 Standalone 作为其内置的资源管理和调度框架。 Spark 和 MR 处理任务对比 Spark 组件 Spark Core Spark Core 实现了 Spark 的基本功能，包含任务调度、内存管理、错误恢复、与存储系统 交互等模块。Spark Core 中还包含 了对弹性分布式数据集(resilient distributed dataset，简称 RDD)的 API 定义。\nRDD 算子 为什么有 RDD？ 在许多迭代式算法(比如机器学习、图算法等)和交互式数据挖掘中，不同计算阶段之间会重用中间结果，即一个阶段的输出结果会作为下一个阶段的输入。但是， 之前的 MapReduce 框架采用非循环式的数据流模型，把中间结果写入到 HDFS 中，带来了大量的数据复制、磁盘 IO 和序列化开销，且这些框架只能支持一些 特定的计算模式(map/reduce)，并没有提供一种通用的数据抽象。\nRDD 提供了一个抽象的数据模型，让我们不必担心底层数据的分布式特性，只需将具体的应用逻辑表达为一系列转换操作(函数) ，不同 RDD 之间的转换操作之间还可以形成依赖关系，进而实现管道化，从而避免了中间结果的存储，大大降低数据复制、磁盘 IO 和序列化开销，并且还提供了更多的 API 操作！\nRDD 介绍 RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是 Spark 中最基本的数据抽象，是 Spark 计算的基石，它代表⼀个不可变、可分区、里面的元素可并行计算的集合。 RDD 具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD 允许用户在执⾏多个查询时显式地将⼯作集缓存在内存中，后续的查询能够重⽤⼯作集，这极⼤地提升了查询速度。 MR 中对数据是没有进行抽象的，而在 Spark 中对数据进行了抽象，提供⼀些列处理⽅法也就是 RDD，为用户屏蔽了底层对数据的复杂抽象和处理，为⽤户提供了⼀组⽅便 的数据转换与求值方法，好比 Java 中类的封装。 注意 : RDD 本身是不存储数据，而是记录了数据的位置，数据的转换关系(调用什么方法、传入什么函数)！！！\n以下是 RDD 源码翻译解读： RDD 特点 弹性体现： 存储的弹性：内存与磁盘的自动切换； 容错的弹性：RDD 的血统（Lineag）会记录 RDD 的元数据信息和转换行为 ，当 RDD 的部分分区数据丢失时，它可以根据这些信息来重新运算并恢复丢失的数据分区。 计算的弹性：计算出错重试机制； 分片的弹性：可根据需要重新分片； 分布式：数据存储在大数据集群不同节点上 数据集：RDD封装了计算逻辑，并不保存数据 数据抽象：RDD 是⼀个抽象，需要具体实现 不可变：RDD 封装的计算逻辑不可改变，想要改变只能产⽣新的 RDD 可分区、并行计算 RDD 做了什么 从计算的角度来讲，数据处理过程中需要计算资源（内存 \u0026amp; CPU）和计算模型（逻辑）。执⾏时，需要将计算资源 和计算模型进行协调和整合。\nSpark 框架在执行时，先申请资源，然后将应⽤程序的数据处理逻辑分解成⼀个⼀个的计算任务。然后将任务分发到已经分配资源的计算节点上, 按照指定的计算模型进行数据计算。最后得到计算结果。\nRDD 的转换和行动操作 RDD 算子分为两种类型的操作：转换操作和行动操作 转换操作是返回⼀个新的 RDD 的操作，比如 map 和 flatMap 行动操作则是向 Driver 返回结果或将结果写出到外部存在设备，比如 collect 和 saveAsTextFile Transformation(转换)算子概述 RDD 中的所有转换都是延迟加载的，它们只是记住这些应⽤到基础数据集上的转换动作，并不会直接计算结果。只有当发生⼀个要求返回结果给 Driver 的动作时，这些转换才会真正运 行。这样可以在 Action 时对 RDD 操作形成 DAG 有向无环图进行 Stage 的划分和并行优化，这这种设计让 Spark 更加有效率地运行！ 列举部分算子： Action(行动)算子概述 在 RDD 上运⾏计算,并返回结果给 Driver 或写入文件系统， 列举部分算子： RDD 持久化和缓存 Spark 速度非常快的原因之⼀，就是在不同操作中可以在内存中持久化或缓存多个数据集。当持久化某个 RDD 后， 每⼀个节点都将把计算的分片结果保存在内存中，并在对此 RDD 或衍⽣出的 RDD 进行的其他动作中重⽤，这使得后续的动作变得更加迅速！ 缓存是 Spark 构建迭代式算法和快速交互式查询的关键。如果⼀个有持久化数据的节点发⽣故障，Spark 会在需要⽤到缓存的数据时重算丢失的数据分区。如果希望节点故障的情况不会拖累我们的执⾏速度，也可以把数据备份到多个节点上。 RDD 通过 persist 或 cache 方法可以将前面的计算结果缓存，但是并不是这两个方法被调用时立即缓存，而是触发后面的 action 时，该 RDD 将会被缓存在 计算节点的内存中，并供后面重用。 存储级别 默认的存储级别都是仅在内存存储一份，Spark 的存储级别还有好多种，存储级别在 object StorageLevel 中定义的。 Checkpoint 检查点机制 Spark 中对于数据的保存除了持久化操作之外，还提供了⼀种检查点的机制，检查点（本质是通过将 RDD 写入 Disk 做检查点）是为了通过血统（lineage）做持久化容错的辅助，lineage 过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题⽽丢失分区，从做检查点的 RDD 开始重做 Lineage，就会减少资源开销。检查点通过将数据写⼊到 HDFS 文件系统实现了 RDD 的检查点功能。\nRDD 宽窄依赖 RDD 和 它依赖的父 RDD 的关系有两种不同的类型，\n宽依赖(wide dependency/shuffle dependency) ：父 RDD 的一个分区被子 RDD 的多个分区依赖(涉及到 shuffle)\n窄依赖(narrow dependency）：父 RDD 的一个分区只会被子 RDD 的一个分区依赖； 为什么要设计宽窄依赖 对于窄依赖： 窄依赖的多个分区可以并行计算；窄依赖的一个分区的数据如果丢失只需要重新计算对应的分区的数据就可以了。\n对于宽依赖： 划分 Stage(阶段)的依据，对于宽依赖，必须等到上一阶段计算完成才能计算下 一阶段。\nDAG 生成和划分 Stage DAG 是什么？\nDAG(有向无环图)指的是数据转换执行的过程，有方向，无闭环(其实就是 RDD 执行的流程)；原始的 RDD 通过一系列的转换操作就形成了 DAG 有向无环图，任务执行时，可以按照 DAG 的描述，执行真正的计算(数据被操作的一个过程)。 DAG 的边界\n开始：通过 SparkContext 创建的 RDD\n结束：触发 Action，一旦触发 Action 就会形成一个完整的 DAG DAG 划分 Stage 一个 Spark 程序可以有多个 DAG(有几个 Action，就有几个 DAG)。 一个 DAG 可以有多个 Stage(根据宽依赖/shuffle 进行划分)。 同一个 Stage 可以有多个 Task 并行执行(task 数=分区数，如上图，Stage1 中 有三个分区 P1、P2、P3，对应的也有三个 Task)。 可以看到这个 DAG 中 reduceByKey 操作是一个宽依赖，Spark 内核会以此为边界将其前后划分成不同的 Stage。 同时在图 Stage1 中，从 textFile 到 flatMap 到 map 都是窄依赖，这几步操作可以形成一个流水线操作，通过 flatMap 操作生成 partition 不用等待整个 RDD 计算结束，可以直接进行 map 操作，这样大大 提高了计算的效率。 Spark SQL Spark SQL 发展（精彩） Spark SQL 是构建在 SparkCore 基础之上的⼀个基于 SQL 的计算模块。 SparkSQL 的前身叫【Shark】，最开始 Shark 的底层代码优化、sql 的解析、执行引擎等完全基于 Hive（Shark On Hive），Hive 实现了 SQL on Hadoop，使用 MapReduce 执行任务，但是使用 MapReduce 做计算（Hive On MR），使得 Hive 的查询延迟比较高，而 Shark 改写 Hive 的物理执行计划，使用 Shark 代替 MapReduce 物理引擎（Hive On Shark），使用列式内存存储，使得 Shark 执行速度比 Hive 快，然而 Shark 执行计划的生成严重依赖 Hive（Shark On Hive On Shark），想要增加新的优化非常困难； 并且 Hive 是进程级别的并行，Spark 是线程级别的并行，所以 Hive 中很多线程不安全的代码不适用于 Shark，所以在 15 年中旬的时候，Shark 负责⼈，将 Shark 项⽬结束掉，重新独⽴出来的⼀个项⽬，就是 SparkSQL，不再依赖 Hive，此后逐渐的形成两条互相独立的业务：SparkSQL 和 Hive-On-Spark。\n如果说 Hive 是将 SQL 转化为 MR，那么 SparkSQL 是将 SQL 转换成 RDD+优化执行，因为我们直接操作 RDD 需要编程实现（学习成本），有了 SQL 我们即使不懂编程也可以实现 RDD 计算！\nSpark SQL 概述 Spark SQL主要用于结构化数据（数据分为结构化数据、半结构化数据、非结构化数据）RDD 主要用于处理非结构化、半结构化、结构化数据。与 RDD API 编程式操作不同，Spark SQL 可以使用 SQL 完成数据分析计算，Spark SQL 提供的接口为 Spark提供了有关数据结构和正在执⾏的计算的更多信息。在内部，Spark SQL 使⽤这些额外的信息来执⾏额外的优化。有几种与 Spark SQL 交互的⽅法，包括 SQL 和 Dataset API。计算结果时，将使⽤相同的执⾏引擎，这与⽤于表示计算的 API/语⾔⽆关。这种统⼀意味着开发⼈员可以轻松地在不同的 API 之间来回切换，基于 API 的切换提供了表示给定转换的最⾃然的⽅式。\nSpark SQL 特点 集成性 统一性 集成 Hive 支持多种数据源 Spark SQL 数据模型 DataFrame 和 Dataset 我们可以通过两种方式使用 Spark，一种是命令式，使用 Spark shell 编程操作 RDD，另一种是通过 SparkSQL 的数据模型 DataFrame 和 Dataset\nDataFrame 和 Dataset 可以理解为是⼀张 mysql 中的⼆维表，表有什么？表头，表名，字段，字段类型。RDD 其实说白了也是⼀张二维表，但是这张二维表相比较于 DataFrame 和 Dataset 却少了很多东西，比如表头，表名，字段，字段类型，只有数据和操作数据的方法。 DataFrame 是 1.3 的时候出现的，Dataset 是在 spark 1.6.2 出现的，**早期的时候 DataFrame 叫 SchemaRDD，SchemaRDD 和 RDD 相比，就多了 Schema，所谓元数据信息。**相比 DataFrame，Dataset 提供了编译时类型检查，对于分布式程序来讲，提交⼀次作业要编译、打包、上传、运行，到提交到集群运行时才发现错误，很麻烦，这也是引⼊ Dataset 的⼀个重要原因！ ⼀般的，将 RDD 称之为 Spark 体系中的第一代编程模型；DataFrame 比 RDD 多了⼀个 Schema 元数据信息，被称之为 Spark 体系中的第⼆代编程模型；Dataset 吸收了 RDD 的优点(强类型推断、函数式编程)和 DataFrame 中的优化(SQL 优化引擎、内存列存储)，成为 Spark 的最新⼀代的编程模型。 如何进行 SparkSQL 编 Spark Core 中，如果想要执行应用程序，需要首先构建上下文环境对象 SparkContext，Spark SQL 其实可以理解为对 Spark Core 的⼀种封装，不仅仅在模型上进行了封装，上下文环境对象也进行了封装。 在老的版本中，SparkSQL 提供两种 SQL 查询起始点：⼀个叫SQLContext，⽤于 Spark 自己提供的 SQL 查询； ⼀个叫HiveContext，⽤于连接 Hive 的查询。 SparkSession 是 Spark 最新的 SQL 查询起始点，实质上是 SQLContext 和 HiveContext 的组合，所以在 SQLContex 和 HiveContext 上可⽤的 API 在 Spark Session 上同样是可以使⽤的。 SparkSession 内部封装了 SparkContext，所以计算实际上是由 sparkContext 完成的。 构建 SparkSession 需要依赖 SparkConf 或者 SparkContext，可以使⽤⼯⼚构建器(Builder ⽅式)模式创建 SparkSession。 Spark Streaming 简介 Spark Streaming 是 Spark 提供的对实时数据进行**流式计算（实时计算）**的组件。提供了用来操作数据流的 API，并且与 Spark Core 中的 RDD API 高度对应。 从计算的延迟上⾯，又可以分为纯实时流式计算和准实时流式计算，SparkStreaming 属于准实时计算框架 所谓纯实时的计算，指的是来⼀条记录(event 事件)，启动⼀次计算的作业；离线计算指的是每次计算非常大的⼀批(比如几百 G，好几个 T)数据；准实时则是介于纯实时和离线计算之间的⼀种计算方式，那就是微批处理，即把大量数据微分成多小批进行计算，近似看成流计算。 流式计算特点 数据是无界的(unbounded) 数据是动态的 计算速度快 计算不止一次 计算不能终止 离线计算特点：\n数据是有界的(Bounded) 数据静态的 计算速度通常较慢 计算只执⾏⼀次 计算终会终⽌ 常见流式计算和离线计算框架 离线\nmapreduce spark-core flink 的 dataset 流式\nstorm 第⼀代的流式处理框架 sparkStreaming（其实是为微批处理）第二代的流式处理框架 flink 的 datastream 第三代的流式处理框架 SparkStreaming 的基本工作原理 SparkCore 的数据模型是 RDD，SparkSQL 数据模型是 DataFrame 和 DataSet，SparkStreaming 的数据模型是 DStream，DStream 和 RDD 一样，是一种高级抽象，它基于内存处理连续的数据流，本质上还是 RDD 的基于内存的计算。\n接收实时输入数据流，然后将数据拆分成多个 batch，比如每收集 1 秒的数据封装为⼀个 batch，然后将每个 batch 交给 Spark 的计算引擎进行处理，最后会⽣产出⼀个结果数据流，其中的数据，也是由一个一个的 batch 所组成的。 DStream，英⽂全称为 Discretized Stream，中文翻译为“离散流”，它代表了⼀个持续不断的数据流。DStream 可以通过输入数据源来创建，比如 Kafka、Flume、ZMQ 和 Kinesis；也可以通过对其他 DStream 应用高阶函数来创建，比如 map、reduce、join、window。 **DStream 的内部，其实是一系列时间上连续的 RDD。DStream 中的每个 RDD 都包含了⼀个时间段内的数据。 对 DStream 应⽤的算子，比如 map，其实在底层会被翻译为对 DStream 中每个 RDD 的操作。比如对⼀个 DStream 执行⼀个 map 操作，会产生⼀个新的 DStream。但是，在底层，其实是对输入 DStream 中每个时间段的 RDD，都来⼀遍 map 操作，然后**⽣成的新的 RDD**，即作为新的 DStream 中的那个时间段的⼀个 RDD。 SparkStreaming 的缓存 SparkStreaming 的缓存，说白了就是 DStream 的缓存，DStream 的缓存就只有⼀个⽅⾯，那就是 DStream 对应的 RDD 缓存，RDD 如何缓存？rdd.persist()，所以 DStream 的缓存说⽩了就是 RDD 的缓存，使⽤ persist()指定，并指定持久化策略，⼤多算⼦默认情况下，持久化策略为 MEMORY_AND_DISK_SER_2\nSparkStreaming 的容错 每⼀个 Spark Streaming 应⽤，正常来说都是要 7*24 ⼩时运转的，这也是实时计算程序的特点。因为要持续不断的对数据进⾏计算，因此对实时计算应⽤的要求必须进行容错保底。 Spark Streaming 程序就必须将足够的信息 checkpoint 到容错的存储系统上，从⽽让它能够从失败中进行恢复。有两种数据需要被 checkpoint： 元数据 checkpoint：将定义了流式计算逻辑的信息，保存到容错的存储系统上，⽐如 HDFS。当运行 Spark Streaming 应⽤程序的 Driver 进程所在节点失败时，该信息可以⽤于进⾏恢复。元数据信息包括了： 创建 Spark Streaming 应⽤程序的配置信息，比如 SparkConf 中的信息。 定义了 Spark Stream 应⽤程序的计算逻辑的DStream 操作信息。 定义了那些 job 正在排队，还未处理的 batch 信息。 数据 checkpoint：将实时计算过程中产生的 RDD 的数据保存到可靠的存储系统中。 对于一些将多个 batch 的数据进⾏聚合的，有状态的 transformation 操作，这是⾮常有⽤的。在这种 transformation 操作中，生成的 RDD 是依赖于之前的 batch 的 RDD，这会导致随着时间的推移，RDD 的依赖链条变得越来越长。 要避免由于依赖链条越来越长，导致的⼀起变得越来越长的失败恢复时间，有状态的 transformation 操作执⾏过程中间产⽣的 RDD，会定期地被 checkpoint 到可靠的存储系统上，比如 HDFS。从而削减 RDD 的依赖链条，进而缩短失败恢复时，RDD 的恢复时间。 DStream 操作 DStream 上的操作与 RDD 的类似，分为以下两种：\nTransformations(转换) **无状态转换：**每批次处理不依赖于之前批次的数据 **有状态转换：**当前批次的处理需要使用之前批次的数据或者中间结果，有状态转换包括基于追踪状态变化的转换(updateStateByKey)和滑动窗口的转换：\nOutput Operations(输出)/Action Output Operations 可以将 DStream 的数据输出到外部的数据库或文件系统。 当某个 Output Operations 被调用时，spark streaming 程序才会开始真正的 计算过程(与 RDD 的 Action 类似)。 MLlib 提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据导入等额 外的⽀持功能\nGraphx GraphX 在 Spark 基础上提供了一站式的数据解决⽅案，可以⾼效地完成图计算的完整流⽔作业。GraphX 是⽤于图 计算和并⾏图计算的新的（alpha）Spark API。通过引⼊弹性分布式属性图（Resilient Distributed Property Graph），⼀种顶点和边都带有属性的有向多重图，扩展了 Spark RDD\nSpark 多种部署模式 Local 多⽤于本地测试，如在 eclipse，idea 中写程序测试等。 Standalone 是 Spark ⾃带的⼀个资源调度框架，它⽀持完全分布式。 Yarn ⽣态圈⾥⾯的⼀个资源调度框架，Spark 也是可以基于 Yarn 来计算的。 Mesos 资源调度框架，与 Yarn 类似。 ","date":"2024-09-04T00:12:24Z","permalink":"/zh-cn/post/2024/09/%E4%B8%80%E6%96%87%E5%85%A5%E9%97%A8%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%87%86%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E-spark%E4%B8%87%E5%AD%97%E8%AF%A6%E8%A7%A3%E5%85%A8%E7%BD%91%E6%9C%80%E6%96%B0/","title":"一文入门大数据准流式计算引擎 Spark【万字详解，全网最新】"},{"content":"前言 1991 年，数据仓库之父 比尔·恩门 著书《Building the DataWarehouse》，要求构建数据仓库 时，遵循范式建模，即从关系型数据库中提取的范式数据，仍按范式存储到数据仓库中，这样就导致数仓中有很多小表，查询的时候必然会有很多表的关联，极大地影响查询效率和性能。 1994 年，拉尔夫·金博尔 著书《The DataWarehouse Toolkit》，提出维度建模和数据集市的概念，维度建模是反范式建模，自下而上 ，然而这种方式仍有缺点：那就是每个业务平台的数据有各自的数据集市，集市之间数据隔离，存在数据不一致、重复的情况。 1998-2001 年，比尔·恩门派和金博尔派合并，比尔·恩门提出CIF 架构：数仓分层，不同层采用不同的建模方式，同时解决了数据不一致和查询效率低的问题。 基于以上，有了范式建模、维度建模、实体建模三种主要建模方式 浅谈维度建模 维度建模主要面向分析场景，分为维度表和事实表，是数据仓库中最常用的数据建模技术之一，建模过程和关系型数据库的建表很像，下图中，商家 ID、产品 ID、时间 ID 就是不同的维度列，而订单额就是度量值，维度+度量值=事实表。每个维度列同时也有自己的维度表。\n那么基于以上，有如下两种数据分析模型。\n数据分析模型 对比\n查询效率：雪花模型有很多小表，看起来更为范式化，但这导致查询时需要关联很多表，查询效率比星型模型低。 数据冗余：星型模型的表通常是宽表，伪范式，即表有很多字段，这导致星型模型存在较多的数据冗余。 数据仓库 何为数据仓库 数据仓库（Data Warehouse）即是存储历史数据的仓库，简写为DW 或 DWH。 数据仓库的目的是构建面向分析的集成化数据环境（OLAP），为企业提供决策支持。 仓库的数据来自各个业务平台，业务平台中的数据形式多种多样，可能是 MySQL 等关系数据库里的结构化数据，可能是 Word、Excel 文档中的非结构化数据，还可能是 HTML、XML 等自描述的半结构化数据。这些业务数据经过一系列的 ETL（抽取、转换、加载），最终以一种统一的格式装载进数据仓库。 数据仓库本身并不“生产”任何数据，也不需要“消费”任何的数据，只是在内部对数据做了一些数据清洗转移操作，好比流水线，数据来源于外部，最终开放给外部应用。 数据仓库特征 面向主题：\n传统数据库中，最大的特点是面向应用进行数据的组织，各个业务系统可能是相互分离的。而数据仓库则是面向主题的。主题是一个抽象的概念，是较高层次上企业信息系统中的数据综合、归类并进行分析利用的抽象。 集成性：\n通过对分散、独立、异构的数据库数据进行 ETL 并汇总得到了数据仓库的数据，这样保证了数据仓库内的数据的一致性。 数据仓库中的综合数据不能从原有的数据库系统直接得到。因此在数据进入数据仓库之前，必然要经过清洗、一致性等操作，这一步是数据仓库建设中最关键、最复杂的一步。 反应历史变化：数仓反应的是某段时间内的历史数据，这也是数仓和数据库的区别之一。 不可修改：数据进入数据仓库以后，一般情况下被较长时间保留。数据仓库中一般有大量的查询操作，但修改删除操作很少，只需定时加载更新即可。 时效性：数仓存储的是历史数据，按照时间顺序追加，有时间属性。数仓用户通过分析企业过去一段时间业务的经营状况，挖掘潜在价值。但是分析的结果只能反映过去某段时间的情况，随着业务变化时间改变，数仓中的数据就会失去价值，需要载入新数据。 数据仓库和数据库的区别 数据库与数据仓库的区别实际讲的是 OLTP 与 OLAP 的区别：\n操作型处理，叫联机事务处理 OLTP，也可以称面向交易的处理系统，针对日常事务处理。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理，像 MYSQL，Oracle 等关系型数据库一般属于 OLTP。 分析型处理，叫联机分析处理 OLAP，一般针对某些主题的历史数据进行分析，支持管理决策。 首先要明白，数据仓库的出现，并不是要取代数据库。数据库是面向事务的设计，数据仓库是面向主题设计的。数据库一般存储业务数据，数据仓库存储的一般是历史数据。\n数据库设计是尽量避免冗余，一般针对某一业务应用进行设计，比如一张简单的 User 表，记录用户名、 密码等简单数据即可，符合业务应用，但是不符合分析。 数据仓库在设计是有意引入冗余，依照分析需求，分析维度、分析指标进行设计。 数据库是为捕获数据而设计，数据仓库是为分析数据而设计。 为什么不直接用业务平台的数据而要建设数仓？ 实际在数仓出现之前，确实是这么做的，但是有很多数据分析的先驱者当时已经发现，简单的直接访问方式很难良好工作，原因如下：\n由于安全或其他因素不能直接访问某些业务数据。 业务平台存储的是当前数据，存在于 RDBMS，并且数据版本变更很频繁，而大数据需要的是历史数据，读多改少。 各个平台数据存储是隔离的，且**数据格式不统一，**难以建立、维护、汇总数据。 业务系统的表结构（OLTP）为事务处理性能而优化，有时并不适合查询与分析（OLAP）。 有时用户要看到的某些数据字段在数据库中并不存在，是后期聚合处理生成的。 业务平台是跑业务的，本身就占用了一定数据库读写资源，大数据分析再从每个表中频繁读取数据，影响业务平台的性能，不够专业。 以银行业务为例 数据库是事务系统的数据平台，客户在银行做的每笔交易都会写入数据库，被记录下来，可以简单地理解为用数据库记账。 数据仓库是分析系统的数据平台，它从事务系统获取数据，并做汇总、加工，为决策者提供决策的依据。比如，某银行某分行一个月发生多少交易，该分行当前存款余额是多少。如果存款又多，消费交易又多，那么该地区就有必要设立 ATM 了。 显然，银行的交易量是巨大的，通常以百万甚至千万次来计算。 事务系统是实时的，这就要求时效性，客户存一笔钱需要几十秒是无法忍受的，这就要求数据库只能存储很短一段时间的数据。 而分析系统是事后的，它要提供关注时间段内所有的有效数据。这些数据是海量的，汇总计算起来也要慢一些，但是，只要能够提供有效的分析数据就达到目的了。 数据仓库，是在数据库已经大量存在的情况下，为了进一步挖掘数据资源、为了决策需要而产生的，它决不是所谓的“大型数据库”。 参考博客：数据仓库系列 1：什么是数据仓库,它与传统数据库有什么不同?-CSDN 博客\n","date":"2024-09-01T21:34:40Z","permalink":"/zh-cn/post/2024/09/%E6%B5%85%E8%B0%88%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%E4%BD%95%E4%B8%BA%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"浅谈维度建模、数据分析模型，何为数据仓库，与数据库的区别"},{"content":"数仓架构 数仓架构大致分为离线数仓架构和实时数仓架构，数仓架构可以简单理解为构成数仓的各层关系，如 ODS、DWM、DWD、DWS，具体分层这里不赘述。\n离线数仓架构 显而易见，这种架构不能处理实时数据，那么必然会有数据的流失。\n任何事物都是随着时间的演进变得越来越完善，当然也是越来越复杂，数仓也不例外。\n离线数仓架构 包括数据集市架构、Inmon 企业信息工厂架构、Kimball 数据仓库架构、混合型数据仓库架构，接下来就详细说说这几种架构。\n数据集市架构 数据集市架构重点在于集市 二字，数据集市是按主题域 组织的数据集合，用于支持部门级的决策。有两种类型的数据集市：独立数据集市 和 从属数据集市。\n独立数据集市 独立数据集市集中于部门所关心的单一主题域 ，数据以部门为基础，例如制造部门、人力资源部门和其他部门都各自有他们自己的数据集市。\n优点：因为一个部门的业务相对于整个企业要简单，数据量也小得多，所以部门的独立数据集市周期短、见效快。 缺点：独立数据集市各自为政。从业务角度看，当部门的分析需求扩展 或者跨部门跨主题域分析 时，独立数据市场会力不从心。 当数据存在歧义 ，比如同一个产品在 A 部门和 B 部门的定义不同，将无法在部门间进行信息比较。 每个部门使用不同的技术，建立不同的 ETL 的过程，处理不同的事务系统，而在多个独立的数据集市之间还会存在数据的交叉与重叠，甚至会有数据不一致的情况！ 从属数据集市 从属数据集市的数据来源于数据仓库 从属数据集市的数据来源于数据仓库，即从属于数据仓库。\n优点：\n性能：当数据仓库的查询性能出现问题，可以考虑建立几个从属数据集市，将查询从数据仓库移出到数据集市。 安全：每个部门可以完全控制他们自己的数据。 数据一致：因为每个数据集市的数据来源都是同一个数据仓库，有效消除了数据不一致的情况。 Inmon 企业信息工厂架构 Inmon 架构是范式建模 企业级数据仓库是企业级别的，正如 Inmon 数据仓库所定义的，企业级数据仓库是一个细节数据的集成资源库。其中的数据以最低粒度级别被捕获，存储在满足三范式设计的关系数据库中。 部门级数据集市是企业中部门级别的，是面向主题数据的部门级视图，数据从企业级数据仓库获取。数据在进入部门数据集市时可能进行聚合。数据集市使用多维模型设计，用于数据分析。重要的一点是，所有的报表工具、BI 工具或其他数据分析应用都应该从数据集市查询数据，而不是直接查询企业级数据仓库。 Kimball 数据仓库架构 对比上一张图可以看到，Kimball 与 Inmon 两种架构的主要区别在于数据仓库的设计和建立。 Kimball 的数据仓库包含高粒度的企业数据，使用多维模型设计，是维度建模，这也意味着数据仓库由星型模式的维度表和事实表构成。分析系统或报表工具可以直接访问多维数据仓库里的数据。 在此架构中的数据集市也与 Inmon 中的不同。这里的数据集市是一个逻辑概念，只是多维数据仓库中的主题域划分，并没有自己的物理存储，也可以说是虚拟的数据集市。 混合型数据仓库架构 所谓的混合型结构，指的是在一个数据仓库环境中，联合使用 Inmon 和 Kimball 两种架构。 从架构图可以看到，这种架构将 Inmon 方法中的数据集市替换成了一个多维数据仓库，而数据集市则是多维数据仓库上的逻辑视图。 使用这种架构的好处是：既可以利用规范化设计消除数据冗余，保证数据的粒度足够细；又可以利用多维结构更灵活地在企业级实现报表和分析。 实时数仓架构 在某些场景中，数据的价值随着时间的推移而逐渐减少。所以在传统大数据离线数仓的基础上，逐渐对 数据的实时性提出了更高的要求。\nLambda 架构 传统的 Lambda 实时开发 上述架构，在实时计算链路中，如果存在多个实时业务，每个业务都要对自己的数据进行数据清洗等操作，而数据清洗这操作是重复的。所以对其进行了如下优化，提高数据复用\n升级的 Lambda 实时开发 对实时链路进行数据分层，改成实时数仓，解决了数据复用的问题，可以对数据进行统一清洗等操作。\n为什么 Lambda 架构同时存在流处理和批处理？ 假如整个系统只有一个批处理层，会导致用户必须等待很久才能获取计算结果，一般有时间延迟。电商数据分析部门只能查看前一天的统计分析结果，无法获取当前的结果，这对于实时决策来说有 一个巨大的时间鸿沟，很可能导致管理者错过最佳决策时机。 Lambda 架构属于较早的一种架构方式，早期的流处理不如现在这样成熟，在准确性、扩展性和容错性 上，流处理层无法直接取代批处理层，只能给用户提供一个近似结果，还不能为用户提供一个一致准确的结果。因此 Lambda 架构中，出现了批处理和流处理并存的现象。 Lambda 架构缺点 不管是传统的还是升级后的 Lambda 架构，严格来说并**不是纯正的实时数仓，而是离线+实时！**这就导致 Lambda 有如下缺点：\n同样的需求要开发两套一样的代码，比如批处理要统计昨天一天的人数，流处理要统计实时在线人数，都是统计人数，却要开发两套代码。 跑两套相同的代码，集群资源使用增多 离线结果和实时结果可能不一致，当然以离线为主 离线批量计算 T+1 可能算不完，数据量大 服务器存储压力大 既然离线数仓占用计算压力大，存储压力大，那就不使用离线，使用纯实时的 kappa 架构\nKappa 架构 Kappa 架构缺点 只支持流处理，没有批处理 使用 kafka 进行消息缓存，kafka 不支撑海量数据存储，数据存储也有时间限制 kafka 不支持 OLAP，即无法用 SQL 语句进行简单的数据校验 无法复用数据血缘管理体系（数据治理），因为 kafka 没有 schema 那种字段 kafka 中的数据是 append 追加，不支持数据的更新、插入 Kappa 和 Lambda 对比 湖仓一体—数据湖 基于 Lambda 和 Kappa 架构的缺点，出现了批流一体\n从架构角度来看类似 Lambda 架构，批流一体既可以处理批数据，又可以处理流数据； 从计算框架角度来看，就是 flink、spark 框架，既支持批处理，又支持流处理； 从 SQL 角度来看，就是数仓各层统一支持 SQL，这就弥补了 kappa 中 kafka 不支持 SQL 的缺点； 从存储层面来看，能做到海量数据的存储，而不是像 kappa 一样存储在 kafka 缓存中； Kafka 换成了 Iceberg，IceBerg 就是数据湖技术的一种，介于上层计算引擎和底层存储格式之间的一个中间层，我们可以把它定义成一种“数据组织格式”，底层存储还是 HDFS。除此之外数据湖还有 Hudi（发展最完善）这里不具体阐述。\n数据湖支持 SQL 查询，解决了如下问题：\n存储统一 底层存储是 HDFS，解决了 kafka 存储量小，数据有时间限制的问题 任意分层都可以 OLAP（支持 SQL 查询） Iceberg 有 Schema 概念，可以追踪数据的血缘关系（数据治理） 支持数据实时更新，数据可以 update/insert ","date":"2024-09-01T00:53:40Z","permalink":"/zh-cn/post/2024/09/%E6%95%B0%E4%BB%93%E6%9E%B6%E6%9E%84%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93lambda%E5%92%8Ckappa%E6%B9%96%E4%BB%93%E4%B8%80%E4%BD%93%E6%95%B0%E6%8D%AE%E6%B9%96/","title":"数仓架构：离线数仓、实时数仓Lambda和Kappa、湖仓一体数据湖"},{"content":"官方文档 开始使用 - Ant Design Pro\n构建项目 本次使用的是 3.1.0 老版本\n终端运行如下命令（注意 npm 包的镜像源）\n1 npm i @ant-design/pro-cli@3.1.0 -g 导入项目 打开 package.json 文件，查看脚本和依赖\n启动 start 脚本，第一次启动可能出错，原因是项目的某些依赖还没有引入，此时只需在控制台运行 yarn 命令，它就会自动下载需要引入的依赖，如图：\n前端启动后访问 引入 Umi UI umi ui 官方文档\n1 yarn add @umijs/preset-ui -D 选择要添加的组件 选择添加组件的位置 配置组件 ","date":"2024-08-10T23:28:45Z","permalink":"/zh-cn/post/2024/08/ant-design-pro%E5%88%9D%E4%BD%BF%E7%94%A8/","title":"Ant Design Pro初使用"},{"content":"数仓建模即数据仓库建模，对数据仓库中的数据进行适当的联合，类似数据库分库建表，明晰数据关系，以便进行数据处理操作。（可以适当冗余，不遵循范式）\n数仓建模在哪层建 以维度建模为例，建模是在数据源层的下一层进行建设，在上节的分层架构中，就是在DW 层进行数仓建模，所以 DW 层是数仓建设的核心层！\n数仓建模要怎么建(三种建模法) 常见的有范式建模法、维度建模法、实体建模法等，每种方法从本质上将是从不同的角度看待业务中的问题。\n范式建模法(Third Normal Form，3NF) 范式建模法其实是我们在构建数据模型常用的一个方法，该方法主要由 Inmon 所提倡，主要解决关系型数据库的数据存储，利用的一种技术层面上的方法。目前，我们在关系型数据库中的建模 方法，大部分采用的是三范式建模法。 范式是符合某一种级别的关系模式的集合。构造数据库必须遵循一定的规则，而在关系型数据库 中这种规则就是范式，这一过程也被称为规范化。目前关系数据库有六种范式：第一范式 （1NF）、第二范式（2NF）、第三范式（3NF）、Boyce-Codd 范式（BCNF）、第四范式 （4NF）和第五范式（5NF） 在数据仓库的模型设计中，一般采用第三范式 。一个符合第三范式的关系必须具有以下三个条件 : 每个属性值唯一，不具有多义性 ; 每个非主属性必须完全依赖于整个主键，而非主键的一部分 ; 每个非主属性不能依赖于其他关系中的属性，因为这样的话，这种属性应该归到其他关系中去； 据 Inmon 的观点，数据仓库模型的建设方法和业务系统的企业数据模型类似。在业务系统中，企业数据模型决定了数据的来源，而企业数据模型也分为两个层次，即主题域模型和逻辑模型。同样，主题域模型可以看成是业务模型的概念模型，而逻辑模型则是域模型在关系型数据库上的实例化。\n维度建模法(Dimensional Modeling) 维度模型是数据仓库领域另一位大师Ralph Kimall所倡导，他的《数据仓库工具箱》是数据仓库工程领域最流行的数仓建模经典。 维度建模专门应用于分析型数据库、数据仓库、数据集市建模，以分析决策的需求出发构建模型，构建的数据模型为分析需求服务，因此它重点解决用户如何更快速完成分析需求，同时还有较好的大规模复杂查询的响应性能。 典型的代表是我们比较熟知的星形模型（Star-schema），以及在一些特殊场景下适用的雪花模型（Snow-schema）。\n维度建模中比较重要的概念就是 事实表（Fact table）和维度表（Dimension table）。\n事实表：\n发生在现实世界中的操作型事件，其所产生的可度量数值，存储在事实表中。从最低的粒度级别来看， 事实表行对应一个度量事件，反之亦然。\n事实表特征：事实表=维度+度量 维度表：维度就是所分析的数据的一个量，维度表就是以合适的角度来创建的表，维度表的主键可以作为与之关联的任何事实表的外键 总的说来，在数据仓库中不需要严格遵守规范化设计原则，有时还会故意设计数据冗余。因为数据仓库的主导功能就是面向分析，以查询为主，不涉及数据更新操作。事实表的设计是以能够正确记录历史信息为准则，维度表的设计是以能够以合适的角度来聚合主题内容为准则。\n维度建模模式(数据分析模型) 星型模式 星型模式(Star Schema)是最常用的维度建模方式。星型模式是以事实表为中心，所有的维度表直接连接在事实表上，像星星一样。星形模式的维度建模由一个事实表和一组维表组成，且具有以下特点：\n维表只和事实表关联，维表之间没有关联； 每个维表主键为单列，且该主键放置在事实表中，作为两边连接的外键； 以事实表为核心，维表围绕核心呈星形分布； 星座模式 星座模式是星型模式延伸而来，星型模式是基于一张事实表的，而星座模式是基于多张事实表的，而且共享维表。很多时候维度空间内的事实表不止一个，而一个维表也可能被多个事实表用到。在业务发展后期，绝大部分维度建模都采用的是星座模式。\n雪花模式 雪花模式(Snowflake Schema)是对星型模式的扩展。雪花模式的维度表可以拥有其他维度表的，虽然这种模型相比星型更规范一些，但是由于这种模型不太容易理解，维护成本比较高，而且性能方面需要关联多层维表，性能也比星型模型要低。所以一般不是很常用。\n维度建模过程 数仓工具箱中的维度建模四步走：\n选择业务过程 维度建模是紧贴业务的，所以必须以业务为根基进行建模，那么选择业务过程，顾名思义就是在整个业务流程中选取需要建模的业务，根据运营提供的需求及日后的易扩展性等进行选择业务。 比如商城，整个商城流程分为商家端，用户端，平台端，运营需求是总订单量，订单人数，及用户购买情况等，我们选择业务过程就选择用户端的数据，商家及平台端暂不考虑。业务选择非常重要，因为后面所有的步骤都是基于此业务数据展开的。 声明粒度 相关名词不清楚的可以查看该博客\n​​​​​​ 数仓常见名词解析和名词之间的关系-CSDN 博客\n先举个例子：对于用户来说，一个用户有一个身份证号，一个户籍地址，多个手机号，多张银行卡，那么与用户粒度相同的粒度属性有身份证粒度，户籍地址粒度，比用户粒度更细的粒度有手机号粒度，银行卡粒度，存在一对一的关系就是相同粒度。 为什么要提相同粒度呢？在同一事实表中，必须具有相同的粒度，同一事实表中不要混用多种不同的粒度，不同的粒度数据建立不同的事实表。并且从给定的业务过程获取数据时，建议从关注原子粒度开始设计，也就是从最细粒度开始，因为原子粒度能够承受无法预期的用户查询。但是上卷汇总粒度对查询性能的提升很重要的，所以对于有明确需求的数据，我们建立针对需求的上卷汇总粒度，对需求不明朗的数据我们建立原子粒度。 确认维度 维度表是作为业务分析的入口和描述性标识，所以也被称为数据仓库的“灵魂”。在一堆的数据中怎么确认哪些是维度属性呢，如果该列是对具体值的描述，是一个文本或常量，某一约束和行标识的参与者， 此时该属性往往是维度属性，数仓工具箱中告诉我们牢牢掌握事实表的粒度，就能将所有可能存在的维度区分开，并且要确保维度表中不能出现重复数据，应使维度主键唯一。\n确认事实 事实表是用来度量的，基本上都以数量值表示，事实表中的每行对应一个度量，每行中的数据是一个特定级别的细节数据，称为粒度。 维度建模的核心原则之一是同一事实表中的所有度量必须具有相同的粒度。这样能确保不会出现重复计算度量的问题。有时候往往不能确定该列数据是事实属性还是维度属 性。 记住最实用的事实就是数值类型和可加类型事实。所以可以通过分析该列是否是一种包含多个值并作为计算的参与者的度量，这种情况下该列往往是事实。 实体建模法(Entity Modeling) 实体建模法并不是数据仓库建模中常见的一个方法，它来源于哲学的一个流派。从哲学的意义上说，客观世界应该是可以细分的，客观世界应该可以分成由一个个实体，以及实体与实体之间的关系组成。 那么我们在数据仓库的建模过程中完全可以引入这个抽象的方法，将整个业务也可以划分成一个个的实体，而每个实体之间的关系，以及针对这些关系的说明就是我们数据建模需要做的工作。 将任何一个业务过程划分成 3 个部分，实体，事件，说明，如下图所示：\n上图表述的是一个抽象的含义，如果我们描述一个简单的事实：“小明开车去学校上学”。以这个业务事 实为例，我们可以把“小明”，“学校”看成是一个实体，“上学”描述的是一个业务过程，我们在这里可以抽象为一个具体“事件”，而“开车去”则可以看成是事件“上学”的一个说明。\n","date":"2024-08-02T11:00:00Z","permalink":"/zh-cn/post/2024/08/dw%E5%B1%82%E7%9A%84%E6%95%B0%E4%BB%93%E5%BB%BA%E6%A8%A1%E8%8C%83%E5%BC%8F%E5%BB%BA%E6%A8%A1%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1%E5%8F%8A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%E5%AE%9E%E4%BD%93%E5%BB%BA%E6%A8%A1/","title":"DW层的数仓建模：范式建模、维度建模及数据分析模型、实体建模"},{"content":"数仓分层 那么为什么要数据仓库进行分层呢？\n用空间换时间，通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据；不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗过程，工作量巨大。 通过数据分层管理可以简化数据清洗的过程，把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要溯源并局部调整某个步骤即可。 分层是以解决当前业务快速的数据支撑为目的 抽象出共性的框架并能够赋能给其他业务线，同时为业务发展提供稳定、准确的数据支撑 并能够按照已有的模型为新业务发展提供方向，也就是数据驱动和赋能 \u0026gt; 一个好的分层架构，要有以下好处： 1. 清晰数据结构\n2. 数据血缘追踪：数据 ETL 转化过程中的流动变化\n3. 减少重复开发，提高数据复用性\n4. 数据关系条理化\n5. 屏蔽原始数据的影响\n数仓分层要结合公司业务进行，并且需要清晰明确各层职责，一般采用如下分层结构： 数仓建模在哪层建设呢？我们以维度建模 为例，建模是在数据源层的下一层进行建设，在上图中，就是在 DW 层进行数仓建模，所以 DW 层是数仓建设的核心层。 下面详细阐述下每层建设规范！\n数据源层：ODS(Operational Data Store) ODS 层是最接近数据源的一层，又叫贴源层 ，考虑后续可能需要追溯数据 问题， 因此对于这一层就不建议做过多的数据清洗工作，原封不动地接入原始数据即可， 至于数据去噪、去重、异常值处理等过程可以放在后面的 DWD 层来做！\n数据仓库层：DW(Data Warehouse) 数据仓库层是数据仓库核心层，在这里把从 ODS 层中获得的数据按照主题建立各种数据模型。该层又依次细分为 DWD、DWM、DWS\n数据明细层：DWD(Data Warehouse Detail) 该层一般保持和 ODS 层一样的数据粒度，并且提供一定的数据质量保证 。DWD 层要做的就是将数据清理、整合、规范化，把脏数据、垃圾数据、规范不一致的、状态定义不一致的、命名不规范的数据处理掉。 同时，为了提高数据明细层的易用性，该层会采用一些维度退化手法，将维度退化至事实表中，减少事实表和维表的关联。 另外，在该层也会做一部分的数据聚合，将相同主题的数据汇集到一张表中，提高数据的可用性 。 数据中间层：DWM(Data WareHouse Middle) 该层会在 DWD 层的数据基础上，数据做轻度聚合 ，生成一系列的中间表 ， 提升公共指标的复用性，减少重复加工。 直观来讲，就是对通用的核心维度进行聚合操作，算出相应的统计指标。 在实际计算中，如果直接从 DWD 或者 ODS 计算出宽表的统计指标，会存在计算量太大并且维度太少的问题，因此一般的做法是，在 DWM 层先计算出多个小的中间表，然后再拼接成一张 DWS 的宽表。由于宽和窄的界限不易界定，也可以去掉 DWM 这一层，只留 DWS 层，将所有的数据再放在 DWS 也可。 数据服务层：DWS(Data WareHouse Service) DWS 层为公共汇总层，会进行轻度汇总 ，粒度比明细数据稍粗，基于 DWD 层上的基础数据，整合汇总成分析某一个主题域的服务数据。 DWS 层应覆 盖 80% 的应用场景。又称数据集市或宽表。 按照业务划分，如主题域流量、订单、用户等，生成字段比较多的宽表，用于提供后续的业务查询，OLAP 分析，数据分发等。 一般来讲，该层的数据表会相对比较少，一张表会涵盖比较多的业务内容，由于其字段较多，因此一般也会称该层的表为宽表。 数据集市层：DM(Data Mart) 基于 DW 的基础数据，整合汇总成一个个数据集市，数据集市通常是面向部门的某个主题域的报表数据。比如用户留存表、用户活跃表、商品销量表、商品营收表等等。\n维表层：DIM(Dimension) 如果维表过多，也可针对维表设计单独一层，维表层主要包含两部分数据：\n高基数维度数据：一般是用户资料表、商品资料表类似的资料表。数据量可能是千万级或者上亿级别。 低基数维度数据：一般是配置表，比如枚举值对应的中文含义，或者日期维表。 数据量可能是个位数或者几千几万 数据应用层：ADS(Application Data Service) 在这里，主要是提供给数据产品和数据分析使用的数据 ，一般会存放在 ES、 PostgreSql、Redis 等系统中供线上系统使用，也可能会存在 Hive 或者 Druid 中供数据分析和数据挖掘使用。比如我们经常说的报表数据，一般就放在这里。\n","date":"2024-08-01T11:00:00Z","permalink":"/zh-cn/post/2024/08/%E6%95%B0%E4%BB%93%E5%88%86%E5%B1%82odsdwddwmdwsdimdmads/","title":"数仓分层ODS、DWD、DWM、DWS、DIM、DM、ADS"},{"content":"名词汇总 实体 实体是指依附的主体，就是我们分析的一个对象，比如分析华为手机近半年的销售量是多少，那华为手机就是一个实体；我们分析用户的活跃度，用户就是一个实体。 实体可以是现实中不存在的，比如虚拟的业务对象，活动，会员等都可看做一个实体。 实体的存在是为了业务分析，作为分析的一个筛选的维度，拥有描述自己的属性，本身具有可分析的价值。 维度 维度就是看待问题的角度 ，分析业务数据，从什么角度分析，就建立什么样的维度。所以维度就是要对数据进行分析时所用的一个量，比如分析产品销售情况，可以选择按商品类别来进行分析，这就构成一个维度，把所有商品类别集合在一起，就构成了维度表。\n度量 度量是业务流程节点上的一个数值 。比如分析产品销售情况，产品 ID、生产时间、生产商就是维度，而销量、价格、成本这些可度量的值就是度量。\n事实表 订单金额就是度量值，因此事实表=维度+度量\n粒度 业务流程中对度量的单位，比如商品是按件记录度量，还是按批记录度量。 例如：数仓建设中，我们说这是用户粒度的事实表，那么表中每行数据都是一个用户，无重复用户；例如还有销售粒度的表，那么表中每行都是一条销售记录。 选择合适的粒度级别是数据仓库建设好坏的重要关键内容，在设计数据粒度时，通常需重点考虑以下因素： 要接受的分析类型、可接受的数据最低粒度和能存储的数据量； 粒度的层次定义越粗，就越不能在该仓库中进行更细致的分析； 如果存储资源有一定的限制，就只能采用较粗的数据粒度划分； 数据粒度划分策略一定要保证：数据的粒度确实能够满足用户的决策分析需要，这是数据粒度划分策略中最重要的一个准则； 口径 口径就是取数逻辑（如何取数的），比如要取的数是 10 岁以下儿童中男孩的平均身高，这就是统计的口径，类似于要统计的数据范围。\n指标 指标是口径的衡量值，也就是最后的结果。比如最近七天的订单量，一个促销活动的购买转化率等。一个指标具体到计算实施，有以下几部分组成：\n指标加工逻辑：比如 count ,sum, avg 维度，比如按部门、地域进行指标统计，对应 sql 中的 group by 业务限定/修饰词：比如以不同的支付渠道来算对应的指标，微信支付的订单退款率，支付宝支付 的订单退款率 。对应 sql 中的 where。 除此之外，指标还可以分为如下几种：\n原子指标：基本业务事实，没有业务限定词、没有维度。比如订单表中的订单量、订单总金额都算原子指标。 业务方更关心的指标是有实际业务含义可以直接取数据的指标。比如店铺近 1 天订单支付金额，这就是一个派生指标，会被直接在产品上展示给商家看。 但是这个指标却不能直接从数仓的统一中间层里取数（因为没有现成的事实字段，数仓提供的一般都是大宽表）。需要有一个桥梁连接数仓中间层和业务方的指标需求，于是便有了派生指标。 派生指标：维度+修饰词+原子指标。店铺近 1 天订单支付金额，店铺是维度，近 1 天是一个时间类型的修饰词，支付金额是一个原子指标； 维度：观察各项指标的角度； 修饰词：维度的一个或某些值，比如维度性别下，男和女就是 2 种修饰词。 衍生指标：比如某一个促销活动的转化率就是衍生指标，因为需要促销投放人数指标和促销订单数指标进行计算得出。 标签 标签是人为设定的、根据业务场景需求，对目标对象运用一定的算法得到的高度精炼的特征标识。 可见标签是经过人为再加工后的结果，如网红、下头男、小仙女。对于有歧义的标签，我们内部可进行标签区分，比如：苹果，我们可以定义苹果指的是水果，苹果手机才指的是手机。 自然键 由现实中已经存在的属性组成的键，它在业务概念中是唯一的，并具有一定的业务含义，比如商品 ID， 员工 ID。\n以数仓角度看，来自于业务系统的标识符就是自然键，比如业务库中员工的编号。\n持久键 保持永久性，不发生变化。有时也被叫做超自然持久键。比如身份证号属于持久键。 自然键和持久键区别：举个例子就明白了，比如说公司员工离职之后又重新入职，他的自然键也就是员工编号发生了变化，但是他的持久键身份证号是不变的。\n代理键 不具有业务含义的键。代理键有许多其他的称呼：无意义键、整数键、非自然键、人工键、合成键等。 代理键就是简单的按照顺序序列生产的整数表示。产品行的第 1 行代理键为 1，则下一行的代理键为 2，如此进行。代理键的作用仅仅是连接维度表和事实表。\n退化维度 退化维度，就是那些看起来像是事实表的一个维度关键字，但实际上并没有对应的维度表，**就是维度属性存储到事实表中，这种存储到事实表中的维度列被称为退化维度。**与其他存储在维表中的维度一样， 退化维度也可以用来进行事实表的过滤查询、实现聚合操作等。 那么如何定义退化维度？比如说订单 ID，这种量级很大的维度，没必要用一张维度表来进行存储，而我们进行数据查询或者数据过滤的时候又非常需要，所以这种就冗余在事实表里面，这种就叫退化维度。 下钻 这是在数据分析中常见的概念，下钻可以理解成增加维的层次（升维），从而可以由粗粒度到细粒度来观察数据 ，比如对产品销售情况分析时，可以沿着时间维从年到月到日更细粒度的观察数据。从年的维度可以下钻到月的维度、日的维度等。\n上卷 知道了下钻，上卷就容易理解了，它俩是相逆的操作**（降维），所以上卷可以理解为删掉维**的某些层，由细粒度到粗粒度观察数据的操作或沿着维的层次向上聚合汇总数据。\n维度立方体 维度立方体包含了下钻和上卷的所有操作，站在多个维度角度取看待事情，那么就会有不同的结果，所以维度立方体统计可以满足任何维度需求，不过缺点是结果过多，导致程序运行速度大大降低。\n数据集市 数据集市（Data Mart），也叫数据市场，数据集市就是满足特定的部门或者用户的需求，按照多维的方式进行存储，包括定义维度、需要计算的指标、维度的层次等，生成面向决策分析需求的数据立方体。其实就是从数据仓库中抽取出来的子集。\n名词关系 实体表、事实表、维度表的联系 在Kimball 维度建模中有维度与事实，在Inmon 范式建模中有实体与关系，事实表和实体表之间有怎样区别与联系，先看下它们各自概念：\n维度表：维度表可以看成是用户用来分析一个事实的窗口，它里面的数据应该是对事实的各个方面描述，比如时间维度表，地域维度表，维度表是事实表的一个分析角度。 事实表：事实表=维度+度量 ，事实表其实就是通过各种维度和一些指标值的组合来确定一个事实的，比如通过时间维度、地域维度可以确定某时某地的一些指标值的事实。事实表的每一条数据都是几条维度表的数据和指标值交汇而得到的。 实体表：实体表就是一个实际对象的表，实体表放的数据一定是一条条客观存在的事物数据，比如说各种商品，它就是客观存在的，所以可以将其设计一个实体表。实时表只描述各个事物，并不存在具体的事实，所以也有人称实体表是无事实的事实表。 举个例子： 比如说手机商场中有苹果手机，华为手机等各品牌各型号的手机，这些数据可以组成一个手机实体表，但是表中没有可度量的数据。某天苹果手机卖了 15 台，华为手机卖了 20 台，这些手机销售数据属于事实，组成一个事实表。这样就可以使用日期维度表和地域维度表对这个事实表进行各种维度分析。\n指标与标签的区别 概念不同\n指标是客观的，用来定义、评价和描述特定事物的一种标准或方式 。比如：新增用户数、累计用户数、用户活跃率等是衡量用户发展情况的指标； 标签是主观的、人为设定的，根据业务场景需求，对目标对象运用一定的算法得到的高度精炼的特征标识。 可见标签是经过人为再加工后的结果，如网红、下头男、高富帅。 构成不同\n指标名称是对事物质与量两方面特点的命名；指标取值是指标在具体时间、地域、条件下的数量表现，如人的体重，指标名称是体重，指标的取值就是 120 斤； 标签名称通常都是形容词或形容词+名词的结构，标签一般是不可量化的，通常是孤立的，除了基础类标签，通过一定算法加工出来的标签一般都没有单位和量纲。如将超过 200 斤的称为大胖子。 分类不同\n对指标的分类： 按照指标计算逻辑，可以将指标分为原子指标、派生指标、衍生指标 三种类型； 按照对事件描述内容的不同，分为过程性指标和结果性指标； 对标签的分类： 按照标签的变化性分为静态标签和动态标签； 按照标签的指代和评估指标的不同，可分为定性标签和定量标签； 指标最擅长的应用是监测、分析、评价和建模。 标签最擅长的应用是标注、刻画、分类和特征提取。 特别需要指出的是，由于对结果的标注也是一种标签，所以在自然语言处理和机器学习相关 的算法应用场景下，标签对于监督式学习有重要价值，只是单纯的指标难以做到的。而指标在任 务分配、绩效管理等领域的作用，也是标签无法做到的。\n维度和指标的区别联系 维度就是数据的观察角度，即从哪个角度去分析问题，看待问题 。 指标就是从维度的基础上去衡量结果值。\n维度：一般是一个离散的值，比如时间维度上每一个独立的日期或地域，因此统计时，可以把维 度相同记录的聚合在一起，应用聚合函数做累加、均值、最大值、最小值等聚合计算。 指标：就是被聚合的通计算，即聚合运算的结果，一般是一个连续的值。 自然键与代理键在数仓的使用区别 维度表的唯一主键应该是代理键而不应该是自然键。有时建模人员不愿意放弃使用自然键，因为他们希望与操作型代码查询事实表，而不希望与维度表做连接操作。然而，应该避免使用包含业务含义的多维键，因为不管我们做出任何假设最终都可能变得无效，因为我们控制不了业务库的变动。 所以数据仓库中维度表与事实表的每个连接应该基于无实际含义的整数代理键。避免使用自然键作为维度表的主键。 数据集市和数据仓库的关系 数据集市是企业级数据仓库的一个子集，他主要面向部门级业务 ，并且只面向某个特定的主题。为了解决灵活性和性能之间的矛盾，数据集市就是数据仓库体系结构中增加的一种小型的部门或工作组级别的数据仓库。数据集市存储为特定用户预先计算好的数据，从而满足用户对性能的需求。数据集市可以在一定程度上缓解访问数据仓库的瓶颈。 数据集市和数据仓库的主要区别：数据仓库是企业级的，能为整个企业各个部门的运行提供决策支持手 段；而数据集市则是一种微型的数据仓库,它通常有更少的数据,更少的主题区域,以及更少的历史数据,因 此是部门级的，一般只能为某个局部范围内的管理人员服务，因此也称之为部门级数据仓库。 ","date":"2024-07-31T10:08:42Z","permalink":"/zh-cn/post/2024/07/%E6%95%B0%E4%BB%93%E5%B8%B8%E8%A7%81%E5%90%8D%E8%AF%8D%E8%A7%A3%E6%9E%90%E5%92%8C%E5%90%8D%E8%AF%8D%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/","title":"数仓常见名词解析和名词之间的关系"},{"content":"引言 要想明白为什么 HBase 的产生，就需要先了解一下 Hadoop。\nHadoop 可以通过 HDFS 来存 储结构化、半结构甚至非结构化的数据，是传统数据库的补充，是海量数据存储的最佳方法，它针对大文件的存储、批量访问和流式访问都做了优化，同时也通过多副本解决了容灾问题。\n但是 Hadoop 的缺陷在于它只能执行批处理，并且只能以顺序方式访问数据，这意味着即使是最简单的工作也必须搜索整个数据集，无法实现对数据的随机访问。实现数据的随机访问是传统的关系型数据库所擅长的，但它们却不能用于海量数据的存储。在这种情况下，必须有一种新的方案来同时解决海量数据存储和随机访问的问题，HBase 就是其中之一 (HBase，Cassandra，couchDB，Dynamo 和 MongoDB 都能存储海量数据并支持随机访问)。\n数据结构分类：\n结构化数据：即以关系型数据库表形式管理的数据； 半结构化数据：非关系模型的，有基本固定结构模式的数据，例如日志文件、XML 文档、 JSON 文档、Email 等； 非结构化数据：没有固定模式的数据，如 WORD、PDF、PPT、EXL，各种格式的图片、视 频等。 HBase 简介 HBase 全称 Hadoop Database ，是一个基于 HDFS 的分布式的、面向列的开源数据库，但是这个数据库没有 SQL，只提供了 API，需要 API 编程来使用 HBase，而后面提到的 Phoenix 才使得可以用 SQL 操作 HBase！\nHBase 有如下特点：\n容量大：一个表可以有数十亿行，上百万列，这也和它的扩展性息息相关； 面向列：数据是按照列存储，每一列都单独存放，数据即索引，在查询时可以只访问指定列的数据，有效地降低了系统的 I/O 负担； 稀疏性：空 (null) 列并不占用存储空间，表可以设计的非常稀疏 ； 易扩展：的扩展性主要体现在两个方面，一个是基于上层处理能力（RegionServer） 的扩展，一个是基于存储的扩展（HDFS）。通过横向添加 RegionSever 的机器， 进行水平扩展，提升 Hbase 上层的处理能力，提升 Hbsae 服务更多 Region 的 能力。 数据多版本：每个单元中的数据可以有多个版本，按照时间戳排序，新的数据在最上面； 采用 HDFS 作为底层存储，支持结构化、半结构化和非结构化的存储； 支持数据分片； 易于使用的 Java 客户端 API，客户端可以通过 HBase 实现对 HDFS 上数据的随机访问； HBase 的表 表 schema 仅定义列族，表具有多个列族，每个列族可以包含任意数量的列，列由多个单元格 （cell ）组成，单元格可以存储多个版本的数据，多个版本数据以时间戳进行区分。\n所有数据的底层存储格式都是字节数组； 不持复杂的事务，只支持行级事务，即单行数据的读写都是原子性的； 查询功能简单，不支持 join 等复杂操作； 仅支持通过主键(row key)和主键的 range 来检索数据； Row Key 行键 Row Key 是用来检索记录的主键。想要访问 HBase Table 中的数据，只有以下三种方式：\n通过指定的 Row Key 进行访问； 通过 Row Key 的 range 进行访问，即访问指定范围内的行； 进行全表扫描； Row Key 可以是任意字符串，存储时数据按照 Row Key 的字典序进行排序，这里需要注意以下两点：\n因为字典序对 int 排序的结果是 1,10,100,11,12,13,14,15,16,17,18,19,2,20,21,…,9,91,92,93,94,95,96,97,98,99。如果你使用整型的字符串作为行键，那么为了保持整型的自然序，行键必须用 0 作左填充。 行的一次读写操作时原子性的 (不论一次读写多少列)。 Column Family 列族 HBase 表中的每个列，都归属于某个列族。列族是表的 Schema 的一部分，所以列族需要在创建表时定义。 列族的所有列都以列族名作为前缀，例如 courses:history ， courses:math 都属于 courses 这个列族。 Column Qualifier 列限定符 列限定符，可以理解为是具体的列名，例如 courses:history ， courses:math 都属于 courses 这个列族，它们的列限定符分别是 history 和 math 。需要注意的是列限定符不是表 Schema 的一部分，可以在插入数据的过程中动态创建列。\nCell 单元格 Cell 是行，列族和列限定符的组合，并包含值和时间戳。可以等价理解为关系型数据库中由指定行和指定列确定的一个单元格，而不同的是 HBase 中的一个单元格是由多个版本的数据组成的，每个版本的数据用时间戳进行区分。\nTimestamp 时间戳 HBase 中通过 row key 和 column 确定的为一个存储单元称为 Cell 。每个 Cell 都保存着同一份数 据的多个版本。版本通过时间戳来索引，时间戳的类型是 64 位整型，时间戳可以由 HBase 在数据写入 时自动赋值，也可以由客户显式指定。每个 Cell 中，不同版本的数据按照时间戳倒序排列，即最新的数据排在最前面。\nHBase 的存储结构 Regions HBase Table 中的所有行按照 Row Key 的字典序排列。HBase Tables 通过行键的范围 (row key range) 被水平切分成多个 Region , 一个 Region 包含了在 start key 和 end key 之间的所有行。\n每个表一开始只有一个 Region ，随着数据不断增加， Region 会不断增大，当增大到一个阀值的时 候， Region 就会等分为两个新的 Region 。当 Table 中的行不断增多，就会有越来越多的 Region 。\nRegion 是 HBase 中分布式存储和负载均衡的最小单元。这意味着不同的 Region 可以分布在不同的 Region Server 上。但一个 Region 是不会拆分到多个 Server 上的。\nRegion Server Region Server 运行在 HDFS 的 DataNode 上。它具有以下组件：\nWAL(Write Ahead Log，预写日志)：对 HBase 读写数据的时候，数据不是直接写进磁盘，它会在内存中保留一段时间，但把数据保存在内存中可能有更高的概率引起数据丢失，为了解决这个问题，数据会先写在一个叫 做 Write-Ahead logfile 的文件中存储尚未进持久化存储的数据记录，以便在发生故障时进行恢复。 BlockCache：读缓存。它将频繁读取的数据存储在内存中，如果存储不足，它将按照 最近最少使 用原则 清除多余的数据。 MemStore：写缓存。它存储尚未写入磁盘的新数据，并会在数据写入磁盘之前对其进行排序。 每个 Region 上的每个列族都有一个 MemStore。 HFile ：将行数据按照 Key\\Values 的形式存储在文件系统上，是实际的存储文件。 Region Server 存取一个子表时，会创建一个 Region 对象，然后对表的每个列族创建一个 Store 实例，每个 Store 会有 0 个或多个 StoreFile 与之对应，每个 StoreFile 则对应一个 HFile ，HFile 就是实际存储在 HDFS 上的文件。\nHBase 的系统架构 HBase 系统遵循 Master/Salve 架构，由三种不同类型的组件组成：\nZookeeper 保证任何时候，集群中只有一个 Master； 存贮所有 Region 的寻址入口； 实时监控 Region Server 的状态，将 Region Server 的上线和下线信息实时通知给 Master； 存储 HBase 的 Schema，包括有哪些 Table，每个 Table 有哪些 Column Family 等信息； Master 为 Region Server 分配或移除 Region ； 负责 Region Server 的负载均衡 ； 处理 Region Server 的故障转移； 处理 GFS 上的垃圾文件回收； 处理 Schema 的更新请求； Region Server 负责维护 Master 分配给它的 Region ，并处理发送到 Region 上的 IO 请求； 负责切分在运行过程中变得过大的 Region； 维护 HLog 刷新缓存到 HDFS 存储 HBase 的实际数据 压缩数据 三个组件间的协作 HBase 使用 ZooKeeper 作为分布式协调服务来维护集群中的服务器状态。 Zookeeper 负责维护可用 服务列表，并提供服务故障通知等服务：\n每个 Region Server 都会在 ZooKeeper 上创建一个临时节点，Master 通过 Zookeeper 的 Watcher 机制对节点进行监控，从而发现新加入的 Region Server 或故障退出的 Region Server； 所有 Masters 会竞争性地在 Zookeeper 上创建同一个临时节点，由于 Zookeeper 只能有一个同名节点，所以必然只有一个 Master 能够创建成功，此时该 Master 就是主 Master，主 Master 会 定期向 Zookeeper 发送心跳。备用 Masters 则通过 Watcher 机制对主 HMaster 所在节点进行监听； 如果主 Master 未能定时发送心跳，则其持有的 Zookeeper 会话会过期，相应的临时节点也会被 删除，这会触发定义在该节点上的 Watcher 事件，使得备用的 Master Servers 得到通知。所有备 用的 Master Servers 在接到通知后，会再次去竞争性地创建临时节点，完成主 Master 的选举。 写数据流程 Client 向 Region Server 提交写请求； Region Server 找到目标 Region； Region 检查数据是否与 Schema 一致； 如果客户端没有指定版本，则获取当前系统时间作为数据版本； 将更新写入 WAL Log； 将更新写入 Memstore； 判断 Memstore 存储是否已满，如果存储已满则需要 flush 为 Store Hfile 文件。 读数据流程 以下是客户端首次读写 HBase 上数据的流程：\n客户端从 Zookeeper 获取 META 表所在的 Region Server； 客户端访问 META 表所在的 Region Server，从 META 表中查询到访问行键所在的 Region Server，之后客户端将缓存这些信息以及 META 表的位置； 客户端从行键所在的 Region Server 上获取数据。 如果再次读取，客户端将从缓存中获取行键所在的 Region Server。这样客户端就不需要再次查询 META 表，除非 Region 移动导致缓存失效，这样的话，则将会重新查询并更新缓存。 注： META 表是 HBase 中一张特殊的表，它保存了所有 Region 的位置信息，META 表自己的位置信息 则存储在 ZooKeeper 上。 Phoenix 简介 Phoenix 是 HBase 的开源 SQL 中间层，在 Phoenix 之前，如果要使用 HBase，只能调用它的 Java API，但是 Phoenix 允许使用标准 JDBC 的方式来操作 HBase ！\nPhoenix 的理念是 we put sql SQL back in NOSQL ，即可以使用标准的 SQL 就能完成对 HBase 上数据的操作，这也意味着可以通过集成 Spring Data JPA 或 Mybatis 等常用的持久层框架来操作 HBase。 其次 Phoenix 的性能表现也非常优异， Phoenix 查询引擎会将 SQL 查询转换为一个或多个 HBase Scan，通过并行执行来生成标准的 JDBC 结果集。它通过直接使用 HBase API 以及协处理器和自定义过 滤器，可以为小型数据查询提供毫秒级的性能，为千万行数据的查询提供秒级的性能。 同时 Phoenix 还 拥有二级索引等 HBase 不具备的特性，不仅如此 Phoenix 对于用户输入的 SQL 同样会有大量的优化手段（就像 hive 自带 sql 优化器一样） ","date":"2024-07-29T11:30:00Z","permalink":"/zh-cn/post/2024/07/%E5%A4%A7%E6%95%B0%E6%8D%AEhbase%E5%9B%BE%E6%96%87%E7%AE%80%E4%BB%8B%E5%8F%8Aphoenix/","title":"大数据HBase图文简介及Phoenix"},{"content":"Lettuce 和 Jedis、Redisson RedisTemplate 是 SpringDataRedis 中对 JedisApi 的高度封装，提供了 Redis 各种操作、 异常处理及序列化，支持发布订阅。\n首先我们要知道SpringData 是 Spring 中数据操作的模块，包括对各种数据库的集成，比如我们之前学过的 Spring Data JDBC、JPA 等，其中有一个模块叫做Spring Data Redis ，而RedisTemplate 就是其中提供操作 Redis 的通用模板。\nSpring Data Redis 中提供了如下的内容：\n1、对不同 Redis 客户端的整合（Lettuce 和 Jedis、Redisson）\n2、提供了 RedisTemplate 统一 API 操作 Redis\n3、⽀持 Redis 订阅发布模型\n4、⽀持 Redis 哨兵和集群\n5、⽀持基于 Lettuce 的响应式编程（底层就是 Netty）\n6、⽀持基于 JDK、JSON、字符串、Spring 对象的数据序列化、反序列化\n使用 Spring Data Redis 需要引入 RedisTemplate 依赖和 commons-pool 连接池依赖，Jedis 与 RedisTemplate 底层使用的连接池都是 commons-pool2，所以需要导入它\n1 2 3 4 5 6 7 8 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.commons\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-pool2\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 这⾥我们可以看⼀下 spring-boot-starter-data-redis 底层，发现并没有引入 Jedis 原因：\n在 SpringBoot 2.x 版本以后，从原来的 Jedis 替换成了 lettuce，所以 2.x 以后开始默认使用 Lettuce 作为 Redis 客户端，Lettuce 客户端基于 Netty 的 NIO 框架实现，只需要维持单一的连接（非阻塞式 IO）即可高效支持业务端并发请求。同时，Lettuce 支持的特性更加全面，其性能表现并不逊于，甚至优于 Jedis。 简单理解：\nJedis：\n采用的直连，多线程操作不安全 ，如果想要避免线程安全问题，就需要使用 JedisPool 连接池，但是也会有一些线程过多等其他问题，类似于BIO（阻塞式 IO） Lettuce：\n单线程实现，线程安全，底层采用 Netty，实例可以在多个线程中进行共享，不存在线程安全问题！类似 NIO Redisson：支持红锁、分布式锁等 默认的 RedisTemplate 测试 1 2 3 4 5 6 7 8 9 10 11 @SpringBootTest class RedisTestApplicationTests { @Autowired private RedisTemplate redisTemplate; @Test void contextLoads() { redisTemplate.opsForValue().set(\u0026#34;CSDN\u0026#34;,\u0026#34;青秋.\u0026#34;); System.out.println(redisTemplate.opsForValue().get(\u0026#34;CSDN\u0026#34;)); } } 通过指令来查看发现是乱码，这就涉及到了序列化的问题了。 想要解决以上问题，需要了解 RedisTemplate 序列化的问题，首先进入 RedisTemplate 源码，发现需要设置 key、hashKey 和 value、hashValue 的序列化器 再往下看有一个默认的序列化器 也就是说，RedisTemplate 默认采用的是默认的 JDK 序列化器，这种序列化方式会有一定的问题 比如可读性差、内存占用大\n所以总结来说，我们可以修改 key 和 value 的 RedisSerializer 具体实现，这⾥我们可以先看⼀下 RedisSerializer 的实现类有哪些： JacksonJsonRedisSerializer: 序列化 object 对象为 json 字符串 Jackson2JsonRedisSerializer: 跟 JacksonJsonRedisSerializer 实际上是一样的 JdkSerializationRedisSerializer: 序列化 java 对象 GenericToStringSerializer: 可以将任何对象泛化为字符串并序列化 StringRedisSerializer: 简单的字符串序列化 各个序列化器性能测试对比 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 @Test public void testSerial(){ UserPO userPO = new UserPO(1111L,\u0026#34;小明_testRedis1\u0026#34;,25); List\u0026lt;Object\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); for(int i=0;i\u0026lt;200;i++){ list.add(userPO); } JdkSerializationRedisSerializer j = new JdkSerializationRedisSerializer(); GenericJackson2JsonRedisSerializer g = new GenericJackson2JsonRedisSerializer(); Jackson2JsonRedisSerializer j2 = new Jackson2JsonRedisSerializer(List.class); Long j_s_start = System.currentTimeMillis(); byte[] bytesJ = j.serialize(list); System.out.println(\u0026#34;JdkSerializationRedisSerializer序列化时间：\u0026#34;+(System.currentTimeMillis()-j_s_start) + \u0026#34;ms,序列化后的长度：\u0026#34; + bytesJ.length); Long j_d_start = System.currentTimeMillis(); j.deserialize(bytesJ); System.out.println(\u0026#34;JdkSerializationRedisSerializer反序列化时间：\u0026#34;+(System.currentTimeMillis()-j_d_start)); Long g_s_start = System.currentTimeMillis(); byte[] bytesG = g.serialize(list); System.out.println(\u0026#34;GenericJackson2JsonRedisSerializer序列化时间：\u0026#34;+(System.currentTimeMillis()-g_s_start) + \u0026#34;ms,序列化后的长度：\u0026#34; + bytesG.length); Long g_d_start = System.currentTimeMillis(); g.deserialize(bytesG); System.out.println(\u0026#34;GenericJackson2JsonRedisSerializer反序列化时间：\u0026#34;+(System.currentTimeMillis()-g_d_start)); Long j2_s_start = System.currentTimeMillis(); byte[] bytesJ2 = j2.serialize(list); System.out.println(\u0026#34;Jackson2JsonRedisSerializer序列化时间：\u0026#34;+(System.currentTimeMillis()-j2_s_start) + \u0026#34;ms,序列化后的长度：\u0026#34; + bytesJ2.length); Long j2_d_start = System.currentTimeMillis(); j2.deserialize(bytesJ2); System.out.println(\u0026#34;Jackson2JsonRedisSerializer反序列化时间：\u0026#34;+(System.currentTimeMillis()-j2_d_start)); } 测试结果 1 2 3 4 5 6 JdkSerializationRedisSerializer序列化时间：8ms,序列化后的长度：1325 JdkSerializationRedisSerializer反序列化时间：4 GenericJackson2JsonRedisSerializer序列化时间：52ms,序列化后的长度：17425 GenericJackson2JsonRedisSerializer反序列化时间：60 Jackson2JsonRedisSerializer序列化时间：4ms,序列化后的长度：9801 Jackson2JsonRedisSerializer反序列化时间：4 JdkSerializationRedisSerializer 序列化后长度最小，Jackson2JsonRedisSerializer 效率最高。 如果综合考虑效率和可读性，牺牲部分空间，推荐 key 使用 StringRedisSerializer，保持的 key 简明易读；value 可以使用 Jackson2JsonRedisSerializer 如果空间比较敏感，效率要求不高，推荐 key 使用 StringRedisSerializer，保持的 key 简明易读；value 可以使用 JdkSerializationRedisSerializer 自定义 RedisTemplate 新建 RedisConfig 配置类，以下是固定模板，可以直接用\n这个模板我们采用的 Json 序列化 Value，String 序列化 Key\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 @Configuration public class RedisConfig { @Bean public RedisTemplate\u0026lt;String,Object\u0026gt; redisTemplate(RedisConnectionFactory factory){ // 为了研发⽅便 key直接为String类型 RedisTemplate\u0026lt;String,Object\u0026gt; template = new RedisTemplate\u0026lt;\u0026gt;(); // 设置连接⼯⼚ template.setConnectionFactory(factory); //设置key序列化 用的string序列化 template.setKeySerializer(RedisSerializer.string()); template.setHashKeySerializer(RedisSerializer.string()); //序列化配置，通过JSON解析任意对象 GenericJackson2JsonRedisSerializer jsonRedisSerializer = new GenericJackson2JsonRedisSerializer(); //设置value序列化，采用的是Json序列化方式 template.setValueSerializer(jsonRedisSerializer); template.setHashKeySerializer(jsonRedisSerializer); template.afterPropertiesSet(); return template; } } Json 序列化 Value + RedisTemplate 测试 1 2 3 4 5 6 7 8 9 10 @SpringBootTest class RedisTestApplicationTests { @Autowired private RedisTemplate\u0026lt;String,Object\u0026gt; redisTemplate; @Test void contextLoads() { redisTemplate.opsForValue().set(\u0026#34;CSDN\u0026#34;,\u0026#34;青秋.\u0026#34;); System.out.println(redisTemplate.opsForValue().get(\u0026#34;CSDN\u0026#34;)); } } 此时用 keys * 查看，没有乱码。那么再储存一个对象试试！\n1 2 3 4 5 @Test void saveUser(){ redisTemplate.opsForValue().set(\u0026#34;stringredistemplate\u0026#34;,new User(\u0026#34;Mask\u0026#34;,20)); System.out.println(redisTemplate.opsForValue().get(\u0026#34;stringredistemplate\u0026#34;)); } 用可视化工具查看，发现JSON 序列化 Value 后多了个@Class 字段 虽然实现了对象的序列化和反序列化，但这是因为添加了@class 字段，会导致额外的内存开销，在数据量特别大的时候就会有影响，但是如果没有@class 就不会实现自动序列化和反序列化。\n实际开发中，如果为了节省空间，并不会完全使用 JSON 序列化来处理 value， 而是统一采用 String 序列化器，储存 Java 对象也是如此，这就意味着我们需要重新编写 RedisTemplate，但是 SpringBoot 其实提供了一个 String 序列化器实现的 StringRedisTemplate，通过它可以完成以上的需求。 String 序列化 Value + StringRedisTemplate 测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @SpringBootTest class RedisTestApplicationTests { @Autowired private StringRedisTemplate stringRedisTemplate; //Json⼯具 private ObjectMapper mapper = new ObjectMapper(); @Test void StringTemplate() throws JsonProcessingException { User user = new User(\u0026#34;青秋\u0026#34;,18); //⼿动序列化 String json = mapper.writeValueAsString(user); //写⼊数据 stringRedisTemplate.opsForValue().set(\u0026#34;stringredistemplate\u0026#34;,json); //读取数据 String val = stringRedisTemplate.opsForValue().get(\u0026#34;stringredistemplate\u0026#34;); //反序列化 User u = mapper.readValue(val,User.class); System.out.println(u); } } 总结 RedisTemplate 的 Key 和 Value 的序列化器可以根据需要分别设置。 RedisTemplate 默认使用 JdkSerializationRedisSerializer 存入数据，会将数据先序列化成字节数组然后在存入 Redis 数据库，这种 value 不可读。 如果数据是 Object 类型，取出的时候又不想做任何的数据转换，直接从 Redis 里面取出一个对象，那么使用 RedisTemplate 是更好的选择。 当然任何情况下从 Redis 获取数据的时候，都会默认将数据当做字节数组转化，这样就会导致一个问题：当需要获取的数据不是以字节数组存在 redis 当中，而是正常的可读的字符串的时候，RedisTemplate 就无法获取数据，获取到的值是 NULL。这时就需要用 StringRedisTempate 或者专门设置 RedisTemplate 的序列化器！ ","date":"2024-07-28T11:34:59Z","permalink":"/zh-cn/post/2024/07/redistemplatestringredistemplate%E5%BA%8F%E5%88%97%E5%8C%96%E5%99%A8%E9%85%8D%E7%BD%AE/","title":"RedisTemplate、StringRedisTemplate、序列化器配置"},{"content":"Redis 是一个内存数据库 ，所以其运行效率非常高。但也存在一个问题：内存中的数据 是不持久的，若主机宕机或 Redis 关机重启，则内存中的数据全部丢失。因此 Redis 具有持久化功能 ，Redis 通过数据快照或追加操作日志的形式将数据持久化到磁盘。 根据持久化使用技术的不同，Redis 的持久化分为两种：RDB 与 AOF。\n当系统重新启动时，会自动加载持久化文件，并根据文件中数据库状态描述信息将数据恢复到内存中。\nRDB RDB（Redis DataBase）是 Redis默认开启 的持久化⽅式，是指将内存中某一时刻的数据快照全量写入到指定的 rdb 文件中 。当 Redis 启动时会自动读取 rdb 快照文件，将数据从硬盘载入到内存，以恢复 Redis 关机前的数据库状态。 由于 RDB 是生成的数据快照，因此⽣成的 RDB ⽂件⾮常适合⽤于全量复制、数据备份等场景。 RDB 创建数据快照的时间间隔可以通过 redis.conf 配置文件中的\u0026quot;save\u0026quot;配置选项进⾏设置或者通过命令即时创建快照，如\u0026quot;save 900 1\u0026quot;表示如果 900 秒内有⾄少 1 个 key 变化，则创建⼀个 snapshots 快照。 RDB 持久化命令 save\n执行 save 命令可立即进行一次持久化保存。save 命令是同步保存操作，由 Redis 主进程执行，因此执行期间会**阻塞主进程**，Redis 不能处理任何读写请求。\nbgsave\n执行 bgsave 命令可立即进行一次持久化保存，主进程会 fork 出一个子进程 ，由该子进程负责完成保存过程。在子进程进行保存过程中，不会阻塞 redis-server 进程对客户端读写请求的处理。\nfork() 函数是用于在操作系统中创建新进程的系统调用。它会将当前进程的内存内容完整地复制到内存的另一个区域，从而创建一个新的子进程。\nRDB 持久化过程 在进行持久化过程中，如果主进程接收到了用户写请求，则系统会将内存中发生数据修改的物理块 copy 出一个副本。等内存中的全量数据 copy 结束后，会再将 副本中的数据 copy 到 RDB 临时文件。这个副本的生成是由于**Linux 系统的写时复制技术 （Copy-On-Write）**实现的。 AOF AOF 持久性记录服务器接收到的每个写操作。然后在服务器启动时再次重复执行这些操作，从⽽重建原始数据集。\nAOF 持久化过程 命令追加（append）：所有的写命令会追加到 AOF 缓冲区中。 文件写入（write） ：将 AOF 缓冲区的数据写入到 AOF 文件过程中要调用write函数（系统调用），write先把数据写到系统内核缓冲区后直接返回，此时还没写到磁盘 AOF 文件中（延迟写）。 文件同步（fsync） ：AOF 缓冲区根据对应的持久化方式（ fsync 策略）向硬盘做同步操作。这一步需要调用 fsync 函数（系统调用）进行强制硬盘同步，fsync 将阻塞直到写入磁盘完成后返回，保证了数据持久化。 文件重写（rewrite）：随着 AOF 文件越来越大，需要定期对 AOF 文件进行重写，达到压缩的目的。 如果在 rewrite 过程中又有写操作命令追加，那么这些数据会暂时写入 aof_rewrite_buf 缓冲区。等将全部 rewrite 计算结果写入临时文件后，会先将AOF 重写缓冲区 中 的数据追加 到临时文件，然后再 rename 为磁盘文件的原名称，覆盖原文件。 \u0026gt; write：写入系统内核缓冲区之后直接返回（仅仅是写到缓冲区），不会立即同步到硬盘。虽然提高了效率，但也带来了数据丢失的风险。同步硬盘操作通常依赖于系统调度机制，Linux 内核通常为 30s 同步一次，具体值取决于写出的数据量和 I/O 缓冲区的状态。\nfsync：强制刷新系统内核缓冲区（同步到磁盘），确保写磁盘操作结束才会返回。 AOF 持久化 fsync 策略 appendfsync always：主线程调用 write 执行写操作后，后台线程（ aof_fsync 线程）立即调用 fsync 函数同步 AOF 文件（刷盘），fsync 完成后线程返回，这样会严重降低 Redis 的性能（write + fsync）。 appendfsync everysec：主线程调用 write 执行写操作后立即返回，由后台线程（ aof_fsync 线程）每秒钟调用 fsync 函数 （系统调用）同步一次 AOF 文件（write+fsync，fsync间隔为 1 秒） appendfsync no：主线程调用 write 执行写操作后立即返回，让操作系统决定何时进行同步，Linux 下一般为 30 秒一次（write但不fsync，fsync 的时机由操作系统决定）。 为了兼顾数据和写入性能，可以考虑 appendfsync everysec 选项 ，让 Redis 每秒同步一次 AOF 文件。\nAOF 文件拆分 从 Redis7.0 发布看 Redis 的过去与未来\n从 Redis 7.0.0 开始，Redis 使用了 Multi Part AOF 机制。顾名思义，Multi Part AOF 就是将原来的单个 AOF 文件拆分成多个 AOF 文件。在 Multi Part AOF 中，AOF 文件被分为三种类型，分别为：\nBASE：表示基础 AOF 文件，它一般由子进程通过重写产生，该文件最多只有一个。 INCR：表示增量 AOF 文件，它一般会在 AOFRW 开始执行时被创建，该文件可能存在多个。 HISTORY：表示历史 AOF 文件，它由 BASE 和 INCR AOF 变化而来，每次 AOFRW 成功完成时，本次 AOFRW 之前对应的 BASE 和 INCR AOF 都将变为 HISTORY，HISTORY 类型的 AOF 会被 Redis 自动删除。 为什么 AOF 是执行完命令才记录日志 关系型数据库（如 MySQL）通常都是执行命令之前记录日志（方便故障恢复），而 Redis AOF 持久化机制是在执行完命令之后再记录日志。\n避免额外的检查开销：AOF 记录日志不会对命令进行语法检查，如果先写入日志结果命令出错，则还得修改日志； **不会阻塞当前写操作命令的执行：**先写日志然后执行命令，结果命令出错，还得回去改日志，就阻塞了当前命令，甚至影响下一个命令。 AOF 重写 什么是重写？\n随着执行的写命令越来越多，AOF 文件也越来越大，为解决这一问题，要对 AOF 压缩，即重写。假如先后执行了 set num:1 和 set:num:2，实际结果是 2，那么重写后就不需要 set num:1 这个记录了。\n重写过程中会把新数据记录到新的 aof 文件，防止对原先的 aof 文件造成污染。\nredis 的重写 AOF 过程是通过 fork 系统调用生成后台子进程 bgrewriteaof 来完成的**，** 如果在 rewrite 过程中又有写操作命令追加，那么这些数据会暂时写入 aof_rewrite_buf 缓冲区。等将全部 rewrite 计算结果写入临时文件后，会先将AOF 重写缓冲区 中 的数据追加到临时文件，然后再 rename 为磁盘文件的原名称，覆盖原文件。\n子进程带有主进程的数据副本，这里使用子进程而不是线程，因为如果是使用线程，多线程共享内存 ，那么在修改共享内存数据的时候，需要通过加锁 来保证数据的安全，而这样就会降低性能 。而使用子进程，创建子进程时，父子进程是共享内存数据的，不过这个共享的内存只能以只读 的方式，而当父子进程任意一方修改了该共享内存，就会发生「写时复制」（这里的写时复制和 RDB 的写时复制是同一个），于是父子进程就有了独立的数据副本，就不用加锁来保证数据安全。\n主进程在通过 fork 系统调用生成 bgrewriteaof 子进程时，操作系统会把主进程的「页表」复制一份给子进程 ，这个页表记录着虚拟地址和物理地址映射关系，而不会复制物理内存，也就是说，两者的虚拟空间不同，但其对应的物理空间是同一个。这样父子进程就可以共享只读数据了。\n当父进程或者子进程在向这个内存发起写操作时，由于违反权限 CPU 会触发写保护中断，然后操作系统会在「写保护中断处理函数」里进行物理内存的复制，并重新设置其内存映射关系 ，将父子进程的内存读写权限设置为可读写，最后才会对内存进行写操作，这个过程被称为写时复制 。 混合持久化 Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。\n","date":"2024-07-28T00:12:57Z","permalink":"/zh-cn/post/2024/07/redis%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6--rdb%E5%92%8Caof/","title":"Redis持久化机制--RDB和AOF"},{"content":"JavaWeb 中，客户端通常是浏览器，通过浏览器向服务器发送请求，所以有时候称客户端为浏览器。\nCookie Cookie 是服务器生成，存储在浏览器的一段字符串，可以记录用户的身份信息等数据，但是这些信息是明文存储的，很不安全。\n浏览器把 cookie 以 k-v 形式保存到某个目录下的文本文件内，下一次请求同一网站时会把该 cookie 发送给服务器。由于 cookie 是存在于浏览器上的，所以浏览器加入了一些限制来确保 cookie 不会被恶意使用，同时不会占据太多磁盘空间，单个 cookie 保存的数据不能超过 4KB，而且每个域的 cookie 数量是有限的。\nCookie 缺陷：\ncookie 存在本地浏览器，浏览器之间的 cookie 不共享，所以用户换了浏览器就要重新登录\nSession Session 是基于 Cookie 实现的，是一种 HTTP 存储机制，目的是为无状态的 HTTP 协议提供持久机制 ，服务器会为每个用户创建的一个临时会话对象，存储在服务器，它保存了每个用户的会话信息。相较于 Cookie，session 容量更大，可以保存 Object。\n一次 session 会话可以有多个请求也就有多个 cookie 的交互，服务器为了区分当前是谁发送的请求，给每个浏览器分配了不同的\u0026quot;身份标识\u0026quot;JSESSIONID，JSESSIONID 放在 cookie 中，然后浏览器每次向服务器发请求的时候，都带上 JSESSIONID，服务器就可以根据 JSESSIONID 查找用户对应的 session 会话，所以 session 是有状态的（因为服务器要保存 session）。\n服务器使用 session 把用户的信息临时保存在了服务器上，这种用户信息存储方式相对 cookie 来说更安全。\nSession 在两种情况下会自动销毁：\n客户端关闭浏览器程序\nSession 超时（会话超时）：客户端一段时间内没有访问过该 Session 内存，服务器会清理。回话超时的时间，默认是 30 分钟，只要发起请求，会话时间从 0 重新计算。 Session 缺陷：\n如果 web 服务器做了负载均衡，那么下一个操作请求到了另一台服务器的时候 session 就会丢失，也就是每个服务器的 session 不共享。\n服务器要保存所有人的 session，如果服务器访问量大，那么对服务器来说有很大的内存开销，限制了服务器的扩展能力，是空间换时间\n误解：\n关闭浏览器 ，服务器的 session 不会立刻消失！\n对 session 来说，除非浏览器通知服务器删除 session，否则服务器会一直保留。\n然而浏览器从来不会在关闭之前主动通知服务器它将要关闭，大部分 session 机制都使用会话 cookie 来保存 JSESSIONID，而关闭浏览器后这个 JSESSIONID 就消失了，再次连接服务器时也就无法找到原来的 session。如果服务器设置的 cookie 被保存在硬盘上，或者使用某种手段改写浏览器发出的 HTTP 请求头，把原来的 JSESSIONID 发送给服务器，则再次打开浏览器仍然能够打开原来的 session。\n恰恰是由于关闭浏览器不会导致 session 被删除，迫使服务器需要为 session 设置了一个失效时间，当距离客户端上一次使用 session 的时间超过这个失效时间时，服务器就可以以为客户端已经停止了活动，才会把 session 删除以节省存储空间。\nCSRF 攻击 跨站请求攻击，攻击者冒充用户的浏览器去访问一个用户曾经认证过的网站并运行一些操作，如发邮件、甚至财产操作。这利用了 web 中用户身份验证的一个漏洞：简单的身份验证只能保证请求发自某个用户的浏览器，却不能保证请求本身是用户自愿发出的。csrf 并不能够拿到用户的任何信息，只是以用户浏览器身份进行操作。\n​\nToken 基于 session 的缺陷，我们的解决方案是怎么让服务器不保存 session，而让客户端去保存，可是如果不保存 JSESSIONID, 怎么验证客户端发给服务器 JSESSIONID 的确是服务器生成的呢？\n关键点就在于验证，比如说， A 已经登录了系统， 服务器给 A 发一个令牌(token)， 里边包含了 A 的 user id 等信息， 下一次 A 再次通过 Http 请求访问服务器的时候， 把这个 token 通过 Http header 带过来就可以了。 不过这样 token 和 session id 没有本质区别，任何人都可以可以伪造 token， 所以得想办法， 让别人伪造不了 token，那就对数据做一个签名吧！ 比如说服务器用 HMAC-SHA256 算法，加上一个只有服务器才知道的密钥，对数据做一个加密签名， 把这个签名和数据一起作为 token ，由于密钥别只有服务器知道， 那么伪造 token 就能被识别出来了。\n这个 token 服务器不保存， 当 A 把这个 token 发给服务器的时候，服务器再用同样的 HMAC-SHA256 算法和同样的密钥，对数据再计算一次签名验证。\n这样一来， 服务器就不保存 session 了， 服务器只生成 token , 然后验证 token ， 服务器用 CPU 计算时间获取了 session 存储空间 ！\n解除了 session 这个负担， 服务器机器集群可以轻松地做水平扩展， 用户访问量增大， 直接加机器就行。 这种无状态（服务器不用存储 session，只负责签名和验证）的感觉实在是太好了！ Token 缺陷：\nToken 中的数据是明文保存的（虽然会用 Base64 编码， 但那不是加密）， 还是可以被别人看到的， 所以不能在其中保存像密码这样的敏感信息。 如果一个人的 token 被别人偷走了， 那服务器也会认为小偷是合法用户， 这其实和一个人的 session id 被别人偷走是一样的。 Token 特点：\n无状态、可扩展 支持移动设备 跨域调用 ​​​​​​JWT Json Web Token JSON Web Token Introduction - jwt.io\nJWT 实现登录原理图 上面的 token 算是一种理论技术，而 JWT 算是对 token 的一种落地实现，jwt 只通过加密算法实现对 Token 合法性的验证，不依赖数据库等存储系统（无状态），因此可以做到跨服务器验证，只要密钥和算法相同，不同服务器程序生成的 Token 可以互相验证。\n用途 授权：这是 JWT 最广泛的应用场景。一次用户登录，后续请求将会包含 JWT，对于那些合法的 token，允许用户连接路由，服务和资源。目前 JWT 广泛应用在 SSO（Single Sign On 单点登录）上。因为他们开销很小并且可以在不同领域轻松使用。 信息交换：JWT 是一种在各方面之间安全信息传输的好的方式 ，因为 JWT 可以签名，例如，使用公钥/私钥对，可以确定发件人是否正确。 此外，使用标头和有效负载计算签名，可以验证内容是否未被篡改。 JWT 组成 一个 JWT 由三部分组成，各部分以点分隔\nHeader(头部） \u0026mdash;\u0026ndash;base64Url 编码的 Json 字符串\nPayload(载荷）\u0026mdash;\u0026ndash;base64url 编码的 Json 字符串\nSignature(签名)\u0026mdash;\u0026ndash;使用指定算法，通过 Header 和 Playload加盐计算的字符串\nHeader 此部分有两部分组成：\n一部分是 token 的类型，目前只能是 JWT 另一部分是签名算法，比如 HMAC 、 SHA256 、 RSA 示例：\n1 2 3 4 { \u0026#34;alg\u0026#34;: \u0026#34;HS256\u0026#34;, \u0026#34;typ\u0026#34;: \u0026#34;JWT\u0026#34; } Payload payload 包含 claims（声明）\nclaims 是关于一个实体（通常是用户）和其他数据类型的声明，有三种类型：registered、public、private claims\nRegistered（已注册的声明）：这些是一组预定义声明，不是强制性的，但建议使用，以提供一组有用的，可互操作的声明。\nJWT 规定了 7 个官方字段，供选用：\niss (issuer)：签发人\nexp (expiration time)**：过期时间**\nsub (subject)：主题\naud (audience)：受众\nnbf (Not Before)：生效时间\niat (Issued At)**：签发时间**\njti (JWT ID)：编号\n除了官方字段，可以在 payload 部分定义私有字段，下面就是一个例子。\n{\n\u0026ldquo;sub\u0026rdquo;: \u0026ldquo;1234567890\u0026rdquo;,\n\u0026ldquo;name\u0026rdquo;: \u0026ldquo;John Doe\u0026rdquo;,\n\u0026ldquo;admin\u0026rdquo;: true\n}\n注意，JWT 默认是不加密的，任何人都可以读到，所以不要把**秘密信息（密码，手机号等）**放在这个部分。\nSignature Signature 部分是对前两部分的签名，防止数据篡改。\n首先，需要指定一个密钥（secret**）**。这个密钥只有服务器才知道，不能泄露给用户。然后，使用 Header 里面指定的签名算法（默认是 HMAC SHA256），按照下面的公式产生签名。\nHMACSHA256(\nbase64UrlEncode(header) + \u0026ldquo;.\u0026rdquo; +\nbase64UrlEncode(payload),\nsecret)\nbase64Url 编码是为了 jwt 方便在网络中传输\n算出签名以后，把 Header、Payload、Signature 三个部分拼成一个字符串，每个部分之间用 \u0026ldquo;.\u0026rdquo; 分隔，就可以返回给用户。\n除了 HS256，还有RS256，RS256 是使用 RSA 私钥进行签名,使用 RSA 公钥进行验证。相比 HS256 更加安全。私钥只能被认证服务器拥有，只用来签名 JWT 不能用来校验，公钥是应用服务器使用来校验 JWT，但不能用来给 JWT 签名。\n公钥一般不需要严密保管，因为即便黑客拿到了，也无法使用它来伪造签名。\nJWT 使用方式 客户端收到服务器返回的 JWT ，可以储存在 Cookie 里面，也可以储存在 localStorage。此后，客户端每次与服务器通信，都要带上这个 JWT。把它放在 Cookie 里面自动发送，但是这样不能跨域，所以更好的做法是放在 HTTP 请求的头信息 Authorization 字段里面，如 Authorization: Bearer jwt，其中 Bearer 算是一种 JWT 的默认声明，传输的时候要手动加上，在验证 JWT 的时候要把 Bearer 手动去掉。另一种做法是在跨域的时候把 JWT 放在 POST 请求的数据体里面。\nJWT 缺陷：\n服务器无法在使用过程中废止某个 token 或者更改 token 的权限。也就是说，一旦 JWT 签发了，在到期之前就会始终有效，除非服务器部署额外的逻辑（JWT 的登出问题）\n解决：\n在用户登录成功后，把 jwt 存入 redis 并设置过期时间，用户再次访问时只需要在拦截器中验证 jwt 签名是否合法以及 redis 是否存在该 jwt，这两个条件都满足才能继续访问，如果 redis 不存在则说明 jwt 已过期删除，就需要重新登陆。\n当用户退出登录时同样验证 jwt 签名是否合法，合法就把 redis 的 jwt 删除，此时用户退出成功！\n这样就算用户退出登录时 jwt 还没过期，但是退出登录时已把 redis 的 jwt 删除，再次登录时 redis 中没有 jwt 就无法登录！\nJWT 代码 导入 JWT 依赖\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.auth0\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;java-jwt\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.11.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; JWTUtils\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 public class JwtUtils { private static final String secret = \u0026#34;secret888\u0026#34;; //密钥 public String createJwt(Integer userId, String userName, List\u0026lt;String\u0026gt; authList) { Date issDate = new Date(); //签发时间时间 // Date expireDate = new Date(issDate.getTime() + 1000 *30); //过期时间30秒 Date expireDate = new Date(issDate.getTime() + 1000 * 60 * 60 * 2); //当前时间加上两个小时 //头部 Map\u0026lt;String, Object\u0026gt; headerClaims = new HashMap\u0026lt;\u0026gt;(); headerClaims.put(\u0026#34;alg\u0026#34;, \u0026#34;HS256\u0026#34;); headerClaims.put(\u0026#34;typ\u0026#34;, \u0026#34;JWT\u0026#34;); return JWT.create().withHeader(headerClaims) .withIssuer(\u0026#34;thomas\u0026#34;) //设置签发人 .withIssuedAt(issDate) //签发时间 .withExpiresAt(expireDate) .withClaim(\u0026#34;userId\u0026#34;, userId) //自定义声明 .withClaim(\u0026#34;userName\u0026#34;, userName)//自定义声明 .withClaim(\u0026#34;userAuth\u0026#34;, authList)//自定义声明 .sign(Algorithm.HMAC256(secret)); //使用HS256进行签名，使用secret作为密钥 } public boolean verifyToken(String jwtToken){ //创建校验器 try { JWTVerifier jwtVerifier = JWT.require(Algorithm.HMAC256(secret)).build(); //校验token DecodedJWT decodedJwt = jwtVerifier.verify(jwtToken); System.out.println(\u0026#34;token验证正确\u0026#34;); // Integer userId = decodedJwt.getClaim(\u0026#34;userId\u0026#34;).asInt(); // String userName = decodedJwt.getClaim(\u0026#34;userName\u0026#34;).asString(); // List\u0026lt;String\u0026gt; userAuth = decodedJwt.getClaim(\u0026#34;userAuth\u0026#34;).asList(String.class); return true; } catch (Exception e) { System.out.println(\u0026#34;token验证不正确！！！\u0026#34;); return false; } } /** * 从jwt的payload里获取声明，获取的用户id * @param jwt * @return */ public Integer getUserIdFromToken(String jwt){ try { JWTVerifier jwtVerifier = JWT.require(Algorithm.HMAC256(secret)).build(); DecodedJWT decodedJWT = jwtVerifier.verify(jwt); return decodedJWT.getClaim(\u0026#34;userId\u0026#34;).asInt(); } catch (IllegalArgumentException e) { return -1; } catch (JWTVerificationException e) { return -1; } } /** * 从jwt的payload里获取声明，获取的用户名 * @param jwt * @return */ public String getUserNameFromToken(String jwt){ try { JWTVerifier jwtVerifier = JWT.require(Algorithm.HMAC256(secret)).build(); DecodedJWT decodedJWT = jwtVerifier.verify(jwt); return decodedJWT.getClaim(\u0026#34;userName\u0026#34;).asString(); } catch (IllegalArgumentException e) { return \u0026#34;\u0026#34;; } catch (JWTVerificationException e) { return \u0026#34;\u0026#34;; } } /** * 从jwt的payload里获取声明，获取的用户的权限 * @param jwt * @return */ public List\u0026lt;String\u0026gt; getUserAuthFromToken(String jwt){ try { JWTVerifier jwtVerifier = JWT.require(Algorithm.HMAC256(secret)).build(); DecodedJWT decodedJWT = jwtVerifier.verify(jwt); return decodedJWT.getClaim(\u0026#34;userAuth\u0026#34;).asList(String.class); } catch (IllegalArgumentException e) { return null; } catch (JWTVerificationException e) { return null; } } } JWTTest\n1 2 3 4 5 6 7 8 9 10 11 12 13 public class JwtTest { public static void main(String[] args) { JwtUtils jwtUtils=new JwtUtils(); List\u0026lt;String\u0026gt; authList= Arrays.asList(\u0026#34;student:query\u0026#34;,\u0026#34;student:add\u0026#34;,\u0026#34;student:update\u0026#34;); String myCreateJwt = jwtUtils.createJwt(1, \u0026#34;admin\u0026#34;, authList); System.out.println(myCreateJwt); boolean verifyResult = jwtUtils.verifyToken(myCreateJwt); if(verifyResult){ //从token获取权限 System.out.println(jwtUtils.getUserAuthFromToken(myCreateJwt)); } } } ","date":"2024-07-24T11:19:04Z","permalink":"/zh-cn/post/2024/07/cookiesessionjwttoken/","title":"Cookie、Session、JWT、Token"},{"content":"提示 Spring Security 源码的接口名和方法名都很长，看源码的时候要见名知意，有必要细看接口名和方法名，另外可以借助流程图，调试追踪代码，有助于理解学习！\nApache Shiro 和 Spring Security ​Spring Security 是一个**可高度可定制的身份验证（认证）和访问控制（授权）框架，**它是用于保护基于 Spring 的应用程序的实际标准，相比与另外一个安全框架 Shiro，它提供了更丰富的功能，社区资源也比 Shiro 丰富。\nShiro 轻量级，不依赖 Spring，是第三方框架，简单而灵活，可以用于非 Web 环境！ Security 重量级，依赖 Spring，控制粒度更细，老版本不能脱离 Web，新版本可以 Spring Boot 2 默认使用 Security 5，要求 JDK 至少是 8 Spring Boot 3 默认使用 Security 6，要求 JDK 至少是 17 Security 5 和 Security 6 的区别之一就是 5 到 6 废弃了WebSecurityConfigurerAdapter 类，在 Security 5 编写配置类需要继承 WebSecurityConfigurerAdapter 并重写某些方法，但是在 Security 6 已经不需要了 Spring Security：升级已弃用的 WebSecurityConfigurerAdapter - spring 中文网\n从 Spring Security 5 迁移到 Spring Security 6/Spring Boot 3 - spring 中文网\n认证和授权 认证（Authentication）：验证当前访问系统的是不是本系统的用户，并且要确认具体是哪个用户\n​ 授权（Authorization）：经过认证后判断当前用户是否有权限进行某个操作\n注：Authentication 和 Authorization 拼写很像，有必要分清，后面看源代码方便！\nRBAC Role-Based Access Control 基于角色的访问控制\n把权限打包给角色（角色拥有一组权限），给用户分配角色（用户拥有多个角色）\n最少包括五张表 （用户表、角色表、用户角色表、权限表、角色权限表）\nDemo 环境 Spring Boot：2.7.2\nSpringSecurity：5+\nJDK：1.8\nController 1 2 3 4 5 6 7 8 @RestController @RequestMapping(\u0026#34;/admin\u0026#34;) public class AdminController { @GetMapping(\u0026#34;/query\u0026#34;) public String queryInfo(){ return \u0026#34;I am a admin\u0026#34;; } } 可以直接访问 引入 Spring Security 1 2 3 4 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-security\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 再次访问时默认出现了该登陆界面，默认用户是 user，密码默认在控制台生成 ，成功登陆后才会放行。Spring Security 默认拦截了除登录、退出之外的所有请求。显示的登录和退出表单是 Security 默认生成的。\n（前端页面样式 bootstrap.min.css 是一个 CDN 地址，需要魔法，所以加载很慢）\n初探 Security 原理 **Spring Security 底层是基于 Servlet 的过滤器链，默认共有 16 个过滤器，这里面我们只需要重点关注两个过滤器即可：UsernamePasswordAuthenticationFilter负责登录认证，FilterSecurityInterceptor**负责权限授权。 认证授权图示 图中涉及的类和接口 UsernamePasswordAuthenticationFilter 类 抽象类 AbstractAuthenticationProcessingFilter 的子类，是常用的用户名和密码认证方式的主要处理类,该类中将用请求信息封装为初步的 Authentication（Authentication 实际上是一个接口，它的实现类是 UsernamePasswordAuthenticationToken），此时最多只有用户名和密码，在登录认证成功之后又会生成一个包含用户权限等信息的更全面的 Authentication 对象，然后把它保存在 SecurityContextHolder 所持有的 SecurityContext 中。\nAuthenticationManager 接口 用来处理 Authentication 请求的接口。在其中只定义了一个方法 authenticate()，该方法只接收一个代表认证请求的 Authentication 对象作为参数，如果认证成功，则会返回一个封装了当前用户权限等信息的 Authentication 对象。\n1 2 3 public interface AuthenticationManager { Authentication authenticate(Authentication authentication) throws AuthenticationException; } ProviderManager 类 1 public class ProviderManager implements AuthenticationManager, MessageSourceAware, InitializingBean AuthenticationManager 接口的实现类，AuthenticationManager 接口不直接自己处理认证请求，而是委托给ProviderManager所配置的AuthenticationProvider 列表，而 ProviderManager 的作用就是管理这个 AuthenticationProvider 列表，管理的方式是通过 for 循环去遍历该列表，因为不同的登录逻辑（表单登录、qq 登录、邮箱登录）是不一样的，那么AuthenticationProvider 列表就要支持不同的 Authentication。\n信息验证的逻辑都是在AuthenticationProvider**里面，会依次使用每一个 AuthenticationProvider 进行认证，如果有一个 AuthenticationProvider 认证后的结果不为 null，则表示该 AuthenticationProvider 已经认证成功，之后的 AuthenticationProvider 将不再继续认证。然后直接以该 AuthenticationProvider 的认证结果作为 ProviderManager 的认证结果。如果所有的 AuthenticationProvider 的认证结果都为 null，则表示认证失败，将抛出一个 **ProviderNotFoundException**。\nAuthenticationProvider 接口 该接口被实现为抽象类 AbstractUserDetailsAuthenticationProvider，然后 DaoAuthenticationProvider 类继承该抽象类，并拥有一个 UserDetailsService 的变量\n1 2 3 public abstract class AbstractUserDetailsAuthenticationProvider implements AuthenticationProvider, InitializingBean, MessageSourceAware public class DaoAuthenticationProvider extends AbstractUserDetailsAuthenticationProvider UserDetailsService 接口 1 2 3 public interface UserDetailsService { UserDetails loadUserByUsername(String username) throws UsernameNotFoundException; } 加载用户数据的核心接口，可以自定义从数据库加载数据或者从内存加载临时用户（InMemory）,登录认证的时候 Spring Security 会通过 UserDetailsService 的 loadUserByUsername() 方法获取对应的 UserDetails 类型的用户信息进行认证，认证通过后会将用户信息封装为 UserDetails 接口的实现类 User 并赋给认证通过的 Authentication 的 principal，然后再把该 Authentication 存入到 SecurityContext 中。\nInMemoryUserDetailsManager 类 该类是UserDetailsService 接口的实现类，作用是从内存中加载用户，也就是说用户是在代码中提前写好的，程序运行后被加载到内存，不是从数据库中取的没有持久化，如果是从数据库加载用户就不用这个类了。该类有一个方法是\n1 private User createUserDetails(String name, UserAttribute attr)，返回值是 User 类型 UserDetails 接口 提供用户核心信息的接口,通过 UserDetailsService 的 loadUserByUsername() 方法获取,然后将该 UserDetails 赋给认证通过的 Authentication 的 principal。\n1 public interface UserDetails extends Serializable User 类 1 public class User implements UserDetails, CredentialsContainer UserDetails 接口的实现类，User 类也是 security 的默认用户类，我们可以继承该类对其方法重写\nSecurityContextHolder 类 用来保存 SecurityContext 的，SecurityContext 中含有当前正在访问系统的用户的详细信息。默认情况下，SecurityContextHolder 使用 ThreadLocal 来保存 SecurityContext，这也就意味着在处于同一线程中的方法中我们可以从 ThreadLocal 中获取到当前的 SecurityContext。因为线程池的原因，如果我们每次在请求完成后都将 ThreadLocal 进行清除的话，那么我们把 SecurityContext 存放在 ThreadLocal 中还是比较安全的。这些工作 Spring Security 已经自动为我们做了，即在每一次 request 结束后都将清除当前线程的 ThreadLocal。\n流程总结 用户填写的用户名密码传到后端，进入 Security 的过滤器链进行验证，验证流程为：\n进入 UsernamePasswordAuthenticationFilter，里面有一个 attemptAuthentication 方法，该方法会生成一个 UsernamePasswordAuthenticationToken，也就是一个凭证 Authentication，这个 Authentication 只包含了用户名、密码这些基础信息，没有权限等其他信息，然后 Authentication 作为参数被传到 AuthenticationManager 接口的实现类 ProviderManager 类的 authenticate 方法进行认证，ProviderManager 类中有一个 List 列表（AuthenticationProvider 是接口，AbstractUserDetailsAuthenticationProvider 是接口的抽象类，DaoAuthenticationProvider 是实现类），该列表存放着不同的登录逻辑AuthenticationProvider，通过 for 循环去遍历该列表，信息验证的逻辑都是在AuthenticationProvider里面，会依次使用每一 AuthenticationProvider 进行认证，如果有一个 AuthenticationProvider 认证后的结果不为 null，则表示该 AuthenticationProvider 已经认证成功，之后的 AuthenticationProvider 将不再继续认证。然后直接以该 AuthenticationProvider 的认证结果作为 ProviderManager 的认证结果。如果所有的 AuthenticationProvider 的认证结果都为 null，则表示认证失败，将抛出一个 ProviderNotFoundException。\nDaoAuthenticationProvider 内部的认证逻辑：它有一个 retrieveUser 方法，该方法调用 UserDetailsService().loadUserByUsername 从数据库获取用户信息，并返回一个 UserDetails 类型的对象，它含有用户的详细信息，如用户权限等等，然后将 UserDetails 对象和 Authentication 校验，成功后会把 UserDetails 中的信息填入到 Authentication，一个最终的 Authentication 就产生了，它会被保存到安全上下文中！\n","date":"2024-07-23T17:48:45Z","permalink":"/zh-cn/post/2024/07/%E4%B8%80%E6%96%87%E5%85%A5%E9%97%A8springsecurity-5/","title":"一文入门SpringSecurity 5"},{"content":"共同关注功能 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 @Override public Result common(Long id) { Long userId = UserHolder.getUser().getId(); String key1=\u0026#34;follow:\u0026#34;+id; String key2=\u0026#34;follow:\u0026#34;+userId; //求两个用户关注的交集就是共同关注 Set\u0026lt;String\u0026gt; intersect = stringRedisTemplate.opsForSet().intersect(key2, key1); if (intersect.isEmpty()||intersect==null){ return Result.ok(Collections.emptyList()); } List\u0026lt;Long\u0026gt; list = intersect.stream().map(Long::valueOf).collect(Collectors.toList()); List\u0026lt;UserDTO\u0026gt; userdtos = userService.listByIds(list) .stream() .map(user -\u0026gt; BeanUtil.copyProperties(user, UserDTO.class)) .collect(Collectors.toList()); return Result.ok(userdtos); } Feed 流推送 关注推送又叫 Feed 流，直译为投喂，为用户持续的提供沉浸式体验，如抖音、快手这类短视频 APP，通过无限下滑获取最新信息。\n传统的推送模式是用户寻找内容，而 Feed 模式是内容匹配用户\n拉模式又叫读扩散\n粉丝从博主那里拉取：博主把自己的推文放到自己的发件箱，用户从多个博主那里拉取，但是每个博主发的推文时间是乱序的，那么用户拉取每个博主的推文后还要进行时间排序，比较耗时。\n推模式又叫写扩散\n博主推给粉丝：博主把写到的推文的 id 推给粉丝的收件箱，粉丝刷新后会消费到博主新的推文 id，进而根据 id 加载出新推文，但是如果博主粉丝有千百万，那么要把推文 id 推给千百万个粉丝收件箱，仍然会比较耗时。 \u0026gt; 推模式实现关注推送功能 Redis 数据结构可以选用 list 或者 zset，list 是链表有下标，可以实现排序和分页，zset 可以基于 score 进行排序，排序之后也可以实现分页查询，但是最终选取 zset 来实现，原因如下：\nlist 要依赖角标，有重复读问题 参考下面的 zset，我觉得 list 也可以利用 lastId 实现无重复读效果，只不过 zset 可以根据 score 排序并利用 score 的范围查询，就使用 zset 了\nzset 不依赖角标，依赖 lastId，无重复读问题 zset 的 score 排序后虽然有角标的效果，但我们是利用 score 值的范围查询来实现分页的，把 score 按从大到小排列，每次查询记住最小的 score 作为下次查询的最大的 score，即 lastId\nzset 集合的元素并不是存进去就有序的，而是可以根据需要用 score 排序！\n因此并不是集合最后一个元素就是最小的时间戳！\n（详见 黑马点评 实战篇 P9、P10）\n修改新增 Blog 的代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 @Override public Result saveBlog(Blog blog) { UserDTO user = UserHolder.getUser(); blog.setUserId(user.getId()); // 保存探店博文 boolean save = save(blog); if (save){ //保存成功则把博文id发给粉丝，实现消息推送 //查询该用户的粉丝 List\u0026lt;Follow\u0026gt; followUsers = followService.query().eq(\u0026#34;follow_user_id\u0026#34;, user.getId()).list(); //把blogId推给每个粉丝的收件箱，用zset集合实现，时间戳作为score，这样就可以根据时间戳进行排序 for(Follow follow:followUsers){ Long userId = follow.getUserId(); String key=\u0026#34;feed:\u0026#34;+userId; stringRedisTemplate.opsForZSet().add(key,blog.getId().toString(), System.currentTimeMillis()); } } return Result.ok(blog.getId()); } 滚动分页查询 偏移量 offset\n假设有一个有序集合，分数从高到低排序为 [70, 60, 50, 40, 30, 20, 10]，每次查询的偏移量为上次查询结果中最小分数相同的元素的个数。如果上一次查询的结果是 [50, 40, 30]，那么下一次查询的偏移量就是 3，即上一次查询结果中最小分数相同的元素的个数，这样可以确保不会重复返回已经获取过的元素。\nController 层接口\n1 2 3 4 5 6 @GetMapping(\u0026#34;/of/follow\u0026#34;) public Result queryBlogOfFollow( @RequestParam(\u0026#34;lastId\u0026#34;) Long max ,@RequestParam(value = \u0026#34;offset\u0026#34; ,defaultValue = \u0026#34;0\u0026#34;) Integer offset) { return blogService.queryBlogOfFollow(max, offset); } BlogServiceImpl 实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 @Override public Result queryBlogOfFollow(Long max, Integer offset) { //获取当前登录用户的id Long userId = UserHolder.getUser().getId(); String key=\u0026#34;feed:\u0026#34;+userId; //根据登录用户id去redis查询其收件箱得到推送的blog的id //详见黑马点评 实战篇p10 Set\u0026lt;ZSetOperations.TypedTuple\u0026lt;String\u0026gt;\u0026gt; typedTuples = stringRedisTemplate .opsForZSet() .reverseRangeByScoreWithScores(key, Double.valueOf(0), Double.valueOf(max), Long.valueOf(offset), Long.valueOf(2)); if (typedTuples==null||typedTuples.isEmpty()) { return Result.ok(Collections.emptyList()); } ArrayList\u0026lt;Long\u0026gt; IDlist = new ArrayList\u0026lt;\u0026gt;(typedTuples.size()); long minTime=0; int os=1; for (ZSetOperations.TypedTuple\u0026lt;String\u0026gt; tuple:typedTuples){ //获取id String id = tuple.getValue(); IDlist.add(Long.valueOf(id)); long time = tuple.getScore().longValue(); if (time==minTime){ os++;//偏移量，每次偏移量都会变化，第一次偏移量默认是0，后面每次后端都要把查询后的偏移量传给前端 }else { minTime=time; os=1;//重置 } } String join = StrUtil.join(\u0026#34;,\u0026#34;, IDlist); List\u0026lt;Blog\u0026gt; blogList = query().in(\u0026#34;id\u0026#34;, IDlist).last(\u0026#34;ORDER BY FIELD(id,\u0026#34; + join + \u0026#34;)\u0026#34;).list(); for (Blog blog:blogList){ //查询blog有关的用户信息 Long id = blog.getUserId(); User user = userService.getById(id); blog.setName(user.getNickName()); blog.setIcon(user.getIcon()); //判断该登录用户是否点赞 Long loginUserId = UserHolder.getUser().getId(); String k=\u0026#34;blog:liked:\u0026#34;+id; Double score = stringRedisTemplate.opsForZSet().score(k, loginUserId.toString()); blog.setIsLike(score!=null); } ScrollResult scrollResult = new ScrollResult(); scrollResult.setList(blogList); scrollResult.setOffset(os); scrollResult.setMinTime(minTime); return Result.ok(scrollResult); } ScrollResult.java\n1 2 3 4 5 6 7 8 @Data public class ScrollResult { private List\u0026lt;?\u0026gt; list; //时间戳 private Long minTime; //查询偏移量 private Integer offset; } ","date":"2024-07-16T17:43:11Z","permalink":"/zh-cn/post/2024/07/redis%E7%9A%84zset%E5%AE%9E%E7%8E%B0%E5%85%B1%E5%90%8C%E5%85%B3%E6%B3%A8%E6%87%92%E6%B1%89%E5%BC%8F%E5%85%B3%E6%B3%A8%E6%8E%A8%E9%80%81/","title":"Redis的zset实现共同关注、懒汉式关注推送"},{"content":"案例需求 方案分析 一个用户对于一个 blog 只能点赞一次，如果只使用数据库完成该功能，那么用户每点赞一次都要查询、修改数据库，这样无疑增加了数据库的压力，那么引入 redis，就可以使用 redis 的 Sorted_Set 集合解决该问题（中间件的引入当然是越少越好的，这样有利于系统的稳定性，那此处为了场景演示就引入 redis 了，毕竟现在有点规模的项目基本都离不开 redis）\nredis 的 Sorted_Set 集合 集合中的元素具有唯一性、有序性，利用唯一性完成一个用户对于一个 blog 只能点赞一次，利用有序性完成点赞 top5 的用户排序，key 是 blog 的 id，value 是用户 id 和用于排序的时间戳，如果该 key 的 value 中有当前登录用户的 userId，那么说明该用户对该 blog 已点赞，那么可以利用该用户的时间戳进行排序，如果没有当前登陆用户的 userId，则未点赞。\n点赞功能代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @Override public Result likeBlog(Long id) { //获取当前用户 Long loginUserId = UserHolder.getUser().getId(); //判断该用户是否点赞 String key=\u0026#34;blog:liked:\u0026#34;+id; Double score = stringRedisTemplate.opsForZSet().score(key, loginUserId.toString()); //score为空说明该用户未点赞 if (score==null){ //如果未点赞则点赞成功，数据库点赞数+1，用户对该blog的点赞信息添加到redis的set boolean isUpdate = update().setSql(\u0026#34;liked = liked + 1\u0026#34;).eq(\u0026#34;id\u0026#34;, id).update(); if (isUpdate){ //opsForZSet使用的是scoreSet集合，该集合较于set集合，可以对元素进行排序，基于该特性可以对点赞用户的顺序进行排名 stringRedisTemplate.opsForZSet().add(key,loginUserId.toString(),System.currentTimeMillis()); } }else { //如果已点赞则取消点赞，数据库点赞数-1，用户对该blog的点赞信息从redis的set移除 boolean isUpdate = update().setSql(\u0026#34;liked = liked - 1\u0026#34;).eq(\u0026#34;id\u0026#34;, id).update(); if (isUpdate){ stringRedisTemplate.opsForZSet().remove(key,loginUserId.toString()); } } return Result.ok(); } 最早点赞 top5 功能代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 @Override public Result queryBlogLikes(Long id) { //从redis查询top5的点赞用户 Set\u0026lt;String\u0026gt; top5 = stringRedisTemplate.opsForZSet().range(\u0026#34;blog:liked:\u0026#34; + id, 0, 4); if (top5==null||top5.isEmpty()) { return Result.ok(Collections.emptyList()); } //从查询的数据中得到top5的用户id List\u0026lt;Long\u0026gt; ids = top5.stream().map(Long::valueOf).collect(Collectors.toList()); String idsStr = StrUtil.join(\u0026#34;,\u0026#34;, ids); //根据id查询top5的用户并返回 // List\u0026lt;User\u0026gt; users = userService.listByIds(ids); //详见黑马点评 实战篇 P10 List\u0026lt;User\u0026gt; users = userService.query().in(\u0026#34;id\u0026#34;, ids).last(\u0026#34;ORDER BY FIELD (id,\u0026#34; + idsStr + \u0026#34;)\u0026#34;).list(); List\u0026lt;UserDTO\u0026gt; userDTOS = users.stream() .map(user -\u0026gt; BeanUtil.copyProperties(user, UserDTO.class)) .collect(Collectors.toList()); return Result.ok(userDTOS); } 细节注意 在最初使用 userService.listByIds(ids)查询数据库用户时，查询条件中的 id 顺序和查询结果的顺序是相反的，具体效果如下：\n先查 id=5，再查 id=1，结果是 id=1 在前面，id=5 在后面，这样导致最早点赞 top5 的用户顺序相反 为解决上述问题，使用 order by 对 id 排序 注意这里演示的 id 是确定的数值，在实际代码中要把查询到的用户 id 列表转换成动态字符串，如下：\n1 2 3 4 5 6 7 8 //从查询的数据中得到top5的用户id List\u0026lt;Long\u0026gt; ids = top5.stream().map(Long::valueOf).collect(Collectors.toList()); String idsStr = StrUtil.join(\u0026#34;,\u0026#34;, ids); List\u0026lt;User\u0026gt; users = userService.query().in(\u0026#34;id\u0026#34;, ids).last(\u0026#34;ORDER BY FIELD (id,\u0026#34; + idsStr + \u0026#34;)\u0026#34;).list(); List\u0026lt;UserDTO\u0026gt; userDTOS = users.stream() .map(user -\u0026gt; BeanUtil.copyProperties(user, UserDTO.class)) .collect(Collectors.toList()); Bug 修正 redis 报错\n1 WRONGTYPE Operation against a key holding the wrong kind of value 原因是由于之前点赞功能是使用 set 集合，使得 redis 中已经存在了一个名为\u0026quot;blog:liked:23\u0026quot;的 key 值，添加点赞 top5 功能后由于把 set 集合换成了 sorted_set 集合导致 key 冲突，解决方案是删掉之前的 key 值\n","date":"2024-07-14T18:14:45Z","permalink":"/zh-cn/post/2024/07/redis%E5%AE%9E%E7%8E%B0%E7%94%A8%E6%88%B7%E7%82%B9%E8%B5%9E%E7%82%B9%E8%B5%9Etop5%E7%94%A8%E6%88%B7%E6%8E%92%E5%BA%8F/","title":"Redis实现用户点赞、点赞top5用户排序"},{"content":"消息队列介绍 List 实现消息队列 Redis 的 list 数据结构是一个双向链表，先进先出，可以利用 LPUSH、LPOP、RPUSH、RPOP 命令来实现元素的左右进出，但是 list 并不是阻塞队列，当 list 中无元素时，线程并不会阻塞，而是从 list 中取出一个 null，这并不符合我们的业务需要！因此这里要用 BRPOP 或 BLPOP 命令实现阻塞效果！\n阻塞队列\n当线程尝试从队列中获取元素时，若阻塞队列中无元素，则线程会阻塞，直到队列中有元素线程才会被唤醒并从阻塞队列中取出元素。\nPubSub 实现消息队列 Stream 实现消息队列 Stream 的消费者组 Redis 消息队列总结 基于 Stream 的异步秒杀 Redis 创建消费者、消费者组、消息队列 1 2 3 4 5 xadd s1 * k1 v1 xgroup create s1 g1 0 xgroup create stream.orders g1 0 mkstream XREADGROUP GROUP g1 c1 count 1 Block 2000 streams s1 \u0026gt; 修改 seckill.lua 脚本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 -- 1.参数列表 -- 1.1优惠券id local voucherId=ARGV[1] --1.2 用户id local userId=ARGV[2] --1.3 订单id local orderId=ARGV[3] --2.数据key ..是拼接符号 --2.1 库存key local stockKey=\u0026#39;seckill:stock:\u0026#39;..voucherId --2.2 订单key local orderKey=\u0026#39;seckill:order:\u0026#39;..voucherId --3.脚本业务 --3.1 判断库存是否充足 if (tonumber(redis.call(\u0026#39;get\u0026#39;,stockKey))\u0026lt;=0) then return 1 end --3.2判断用户是否下单 若set集合中存在该用户id，则说明已下过单，返回1 if (tonumber(redis.call(\u0026#39;sismember\u0026#39;,orderKey,userId))==1) then return 2 end --3.4扣库存 redis.call(\u0026#39;incrby\u0026#39;,stockKey,-1) --3.5保存用户到set redis.call(\u0026#39;sadd\u0026#39;,orderKey,userId) --3.6发送消息到消息队列 redis.call(\u0026#39;xadd\u0026#39;,\u0026#39;stream.orders\u0026#39;,\u0026#39;*\u0026#39;,\u0026#39;userId\u0026#39;,userId,\u0026#39;voucherId\u0026#39;,voucherId,\u0026#39;id\u0026#39;,orderId) return 0 添加新的秒杀券 数据库和 redis 同时存入 id 为 19 的秒杀券\nApifox 模拟用户抢购秒杀券 查看 Redis 中数据变化 1、Redis 中 id=19 的秒杀券的数量都-1\n2、Redis 中成功存入用户 id 3、 Redis 中成功存入订单信息\n4、数据库中成功存入订单信息 总结 VoucherOrderServiceImpl.java 完整代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 @Slf4j @Service public class VoucherOrderServiceImpl extends ServiceImpl\u0026lt;VoucherOrderMapper, VoucherOrder\u0026gt; implements IVoucherOrderService { @Resource private StringRedisTemplate stringRedisTemplate; @Resource private ISeckillVoucherService SeckillVoucherService; @Autowired private RedissonClient redissonClient; @Resource private RedisIdWorker redisIdWorker; //阻塞队列 当线程尝试从队列中获取元素时，若队列无元素，则线程会阻塞，直到队列中有元素才会被唤醒 // private BlockingQueue\u0026lt;VoucherOrder\u0026gt; orderTasks=new ArrayBlockingQueue\u0026lt;\u0026gt;(1024*1024); //线程池，负责从阻塞队列中获取订单然后异步下单 private static final ExecutorService SECKILL_ORDER_EXECUTOR= Executors.newSingleThreadExecutor(); //spring提供的注解 作用：类初始化后就执行VoucherOrderHandler方法 //向线程池提交一个线程 @PostConstruct private void init(){ SECKILL_ORDER_EXECUTOR.submit(new VoucherOrderHandler()); } //线程 内部类 private class VoucherOrderHandler implements Runnable{ String queueName =\u0026#34;stream.orders\u0026#34;; @Override public void run() { while (true){ try { //获取队列中的订单信息 // VoucherOrder order = orderTasks.take(); //获取消息队列的订单信息 List\u0026lt;MapRecord\u0026lt;String, Object, Object\u0026gt;\u0026gt; list = stringRedisTemplate.opsForStream().read(Consumer.from(\u0026#34;g1\u0026#34;, \u0026#34;c1\u0026#34;) , StreamReadOptions.empty().count(1).block(Duration.ofSeconds(2)) , StreamOffset.create(queueName, ReadOffset.lastConsumed()) ); //判断消息是否获取成功 if (list==null||list.isEmpty()){ //获取失败说明没有消息，继续循环 continue; } //获取成功则创建订单 MapRecord\u0026lt;String, Object, Object\u0026gt; record = list.get(0); Map\u0026lt;Object, Object\u0026gt; value = record.getValue(); VoucherOrder order = BeanUtil.fillBeanWithMap(value, new VoucherOrder(), true); createVoucherOrder(order); //ACK确认信息 stringRedisTemplate.opsForStream().acknowledge(\u0026#34;s1\u0026#34;,\u0026#34;g1\u0026#34;,record.getId()); } catch (Exception e) { log.error(\u0026#34;订单处理异常\u0026#34;,e); handlePendingList(); } } } private void handlePendingList() { while (true){ try { //获取pendingList队列的订单信息 List\u0026lt;MapRecord\u0026lt;String, Object, Object\u0026gt;\u0026gt; list = stringRedisTemplate.opsForStream().read( Consumer.from(\u0026#34;g1\u0026#34;, \u0026#34;c1\u0026#34;) , StreamReadOptions.empty().count(1) , StreamOffset.create(queueName, ReadOffset.from(\u0026#34;0\u0026#34;)) ); //判断消息是否获取成功 if (list==null||list.isEmpty()){ //获取失败说明没有消息，结束循环 break; } //获取成功则下单 MapRecord\u0026lt;String, Object, Object\u0026gt; record = list.get(0); Map\u0026lt;Object, Object\u0026gt; value = record.getValue(); VoucherOrder order = BeanUtil.fillBeanWithMap(value, new VoucherOrder(), true); createVoucherOrder(order); //ACK确认信息 stringRedisTemplate.opsForStream().acknowledge(\u0026#34;s1\u0026#34;,\u0026#34;g1\u0026#34;,record.getId()); } catch (Exception e) { log.error(\u0026#34;pendingList处理异常\u0026#34;,e); } } } } //代理对象 IVoucherOrderService proxy; private void handleVoucherOrder(VoucherOrder order) { Long userId = order.getUserId(); RLock redisLock = redissonClient.getLock(\u0026#34;lock:order:\u0026#34; + userId); boolean tryLock = redisLock.tryLock(); //判断锁是否获取成功 if (!tryLock){ log.error(\u0026#34;不允许重复下单\u0026#34;); return ; } try { //锁加到这里，事务提交后才释放锁 // proxy.createVoucherOrder(order); //使用动态代理类的对象，事务可以生效 } finally { redisLock.unlock(); } } private static final DefaultRedisScript\u0026lt;Long\u0026gt; SECKILL_SCRIPT; static { SECKILL_SCRIPT=new DefaultRedisScript\u0026lt;\u0026gt;(); SECKILL_SCRIPT.setLocation(new ClassPathResource(\u0026#34;seckill.lua\u0026#34;)); SECKILL_SCRIPT.setResultType(Long.class); } @Override public Result seckillVoucher(Long voucherId) { Long userId = UserHolder.getUser().getId(); //TODO 生成了orderId，把订单信息保存到阻塞队列，由另一个线程专门根据订单信息去数据库做增删改查，这就实现了异步 long orderId = redisIdWorker.nextId(\u0026#34;order\u0026#34;); //执行lua脚本判断有无购买资格 Long result = stringRedisTemplate.execute( SECKILL_SCRIPT, Collections.emptyList(), voucherId.toString(), userId.toString(),String.valueOf(orderId) ); int i = result.intValue(); if (i!=0){ return Result.fail(i==1?\u0026#34;优惠券库存不足\u0026#34;:\u0026#34;不能重复下单\u0026#34;); } //获取事务的动态代理对象，需要在启动类加注解暴漏出对象 proxy = (IVoucherOrderService)AopContext.currentProxy();//拿到动态代理对象 //添加到阻塞队列 //orderTasks.add(order); return Result.ok(orderId); } //TODO spring对该类做了动态代理，用动态代理的对象提交的事务 @Transactional public void createVoucherOrder(VoucherOrder order) { //一人一单，根据优惠卷id和用户id去数据库查询是否已经存在该优惠卷 Long id = order.getUserId(); Long userId = order.getUserId(); RLock redisLock = redissonClient.getLock(\u0026#34;lock:order:\u0026#34; + userId); boolean isLock = redisLock.tryLock(); if (!isLock){ log.error(\u0026#34;不允许重复下单\u0026#34;); return; } //为用户id加锁而不是对整个createVoucherOrder方法加锁，减小锁范围，提升性能，这样每个用户就有不同的锁 //锁加在函数内部，锁内的代码执行完后就会释放锁，而事务的提交是在整个方法执行后提交的，也就是事务的提交在锁释放之后。 //但是锁释放后其他线程就可以进来，此时事务可能还没有提交，可能出现并发问题，重复购买 //所以要扩大锁的范围，把锁加到seckillVoucher方法后面，在事务提交后才能释放锁！ try { int count = query().eq(\u0026#34;user_id\u0026#34;, id).eq(\u0026#34;voucher_id\u0026#34;, order.getVoucherId()).count(); if (count \u0026gt;=1) { //count==1说明用户拥有了一个优惠券 log.error(\u0026#34;不能重复下单\u0026#34;); return; // return Result.fail(\u0026#34;不能重复购买优惠卷\u0026#34;); } //4.扣减库存 防止超卖，加乐观锁，扣减库存前再查询一次库存判断 // boolean b = SeckillVoucherService.update() // .setSql(\u0026#34;stock=stock-1\u0026#34;). // eq(\u0026#34;voucher_id\u0026#34;, voucherId).eq(\u0026#34;stock\u0026#34;,voucher.getStock()).update(); //使用setSql方法设置了更新语句\u0026#34;stock=stock-1\u0026#34;，接着使用eq方法添加了两个条件：\u0026#34;voucher_id\u0026#34;等于voucherId和\u0026#34;stock\u0026#34;等于voucher.getStock() //条件1：voucher_id=voucherId指当前操作的优惠卷的id=数据库中的优惠卷id，即通过优惠卷id指明了要修改哪个优惠卷的库存 //条件2：stock=voucher.getStock,说明该线程修改库存期间没有其他线程来插队修改库存，那么数据是安全的 //TODO ！！！注意！这种操作在并发情况下可能导致用户在优惠卷库存充足的情况下抢购优惠卷失败，也就是即使有库存也会抢购失败，此时可以判断库存是否充足，重新抢购 //修改如下：最后库存判断，只要\u0026gt;0就可以修改 boolean b = SeckillVoucherService.update() .setSql(\u0026#34;stock=stock-1\u0026#34;). eq(\u0026#34;voucher_id\u0026#34;, order.getVoucherId()).gt(\u0026#34;stock\u0026#34;, 0) .update(); if (!b) { // return Result.fail(\u0026#34;库存不足\u0026#34;); log.error(\u0026#34;库存不足\u0026#34;); return; } //创建订单 save(order); } finally { //释放锁 redisLock.unlock(); } } } ","date":"2024-07-11T23:06:25Z","permalink":"/zh-cn/post/2024/07/redis%E5%AE%9E%E7%8E%B0%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97listpubsubstream%E5%9F%BA%E4%BA%8Estream%E7%9A%84%E5%BC%82%E6%AD%A5%E7%A7%92%E6%9D%80/","title":"Redis实现消息队列：list、PubSub、Stream，基于Stream的异步秒杀"},{"content":"异步前 之前的秒杀业务的查询优惠券、查询订单、减库存、创建订单都要查询数据库 ，而且有分布式锁 ，使得整个业务耗时长 ，对此采用异步操作处理，异步操作类似于餐厅点餐，服务员负责点菜产生订单、厨师负责根据订单后厨做饭，整个流程由服务员和厨师两个线程完成，此为异步。\n可以看到异步优化前 ，1000 个请求的耗时均值 497ms\n异步优化方案 将判断秒杀库存和校验一人一单的操作放在 redis 进行，优惠券库存信息也放入 redis 以减少读取数据库的压力，采用 set 集合存储购买过优惠券的用户的 id，set 集合有元素不重复的特性，可以自动实现一人一单\n整体业务逻辑如下： Redis 实现库存和秒杀资格判断（需求 1 和 2） 优惠券信息保存到 redis 修改添加秒杀券的代码，在添加秒杀券的同时把信息也保存到 redis 中\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @Override @Transactional public void addSeckillVoucher(Voucher voucher) { // 保存优惠券 save(voucher); // 保存秒杀信息 SeckillVoucher seckillVoucher = new SeckillVoucher(); seckillVoucher.setVoucherId(voucher.getId()); seckillVoucher.setStock(voucher.getStock()); seckillVoucher.setBeginTime(voucher.getBeginTime()); seckillVoucher.setEndTime(voucher.getEndTime()); seckillVoucherService.save(seckillVoucher); //保存秒杀券信息到redis stringRedisTemplate.opsForValue().set(\u0026#34;seckill:stock:\u0026#34;+voucher.getId(),voucher.getStock().toString()); } 添加秒杀券，信息成功添加到 redis 中，秒杀券 id 是 13，库存是 100，如下图所示： lua 脚本查询 redis 中库存和一人一单购买资格 seckill.lua\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 --- --- Created by 懒大王Smile. --- DateTime: 2024/7/6 10:47 --- -- 1.参数列表 -- 1.1优惠券id local voucherId=ARGV[1] --1.2 用户id local userId=ARGV[2] --2.数据key ..是拼接符号 --2.1 库存key local stockKey=\u0026#39;seckill:stock:\u0026#39;..voucherId --2.2 订单key local orderKey=\u0026#39;seckill:order:\u0026#39;..voucherId --3.脚本业务 --3.1 判断库存是否充足 if (tonumber(redis.call(\u0026#39;get\u0026#39;,stockKey))\u0026lt;=0) then return 1 end --3.2判断用户是否下单 若set集合中存在该用户id，则说明已下过单，返回1 if (tonumber(redis.call(\u0026#39;sismember\u0026#39;,orderKey,userId))==1) then return 2 end --3.4扣库存 redis.call(\u0026#39;incrby\u0026#39;,stockKey,-1) --3.5保存用户到set redis.call(\u0026#39;sadd\u0026#39;,orderKey,userId) return 0 VoucherOrderServiceImpl.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @Service public class VoucherOrderServiceImpl extends ServiceImpl\u0026lt;VoucherOrderMapper, VoucherOrder\u0026gt; implements IVoucherOrderService { @Resource private StringRedisTemplate stringRedisTemplate; @Resource private ISeckillVoucherService SeckillVoucherService; @Autowired private RedissonClient redissonClient; @Resource private RedisIdWorker redisIdWorker; private static final DefaultRedisScript\u0026lt;Long\u0026gt; SECKILL_SCRIPT; static { SECKILL_SCRIPT=new DefaultRedisScript\u0026lt;\u0026gt;(); SECKILL_SCRIPT.setLocation(new ClassPathResource(\u0026#34;seckill.lua\u0026#34;)); SECKILL_SCRIPT.setResultType(Long.class); } @Override public Result seckillVoucher(Long voucherId) { Long userId = UserHolder.getUser().getId(); //执行lua脚本判断有无购买资格 Long result = stringRedisTemplate.execute(SECKILL_SCRIPT, Collections.emptyList(), voucherId.toString(), userId.toString() ); int i = result.intValue(); if (i!=0){ return Result.fail(i==1?\u0026#34;优惠券库存不足\u0026#34;:\u0026#34;不能重复下单\u0026#34;); } long orderId = redisIdWorker.nextId(\u0026#34;order:\u0026#34;); //生成了orderId，把订单信息保存到阻塞队列，由另一个线程专门根据订单信息去数据库做增删改查，这就实现了异步 //TODO return Result.ok(orderId); } } 运行效果 同一用户两次下单 id 为 13 的秒杀券，第一次成功，第二次失败，如下图： 再次查看 redis 中 voucherId=13 的秒杀券，库存减 1，且该对该秒杀券下单成功的用户已经存入 set 集合，userId=1010\n优化后 模拟大量用户抢购秒杀券 的测试 优化后，1000 个请求的耗时均值为 178ms，相比最初的 497ms 减少很多\n阻塞队列实现异步秒杀下单（需求 3 和 4） 阻塞队列\n当线程尝试从队列中获取元素时，若阻塞队列中无元素，则线程会阻塞，直到队列中有元素线程才会被唤醒并从阻塞队列中取出元素。\n前面实现了 redis 秒杀券资格判断，若该用户有资格，则其 userId 存入 redis 订单中，且 redis 中秒杀券库存自减\n订单加入阻塞队列 1 2 //定义阻塞队列 private BlockingQueue\u0026lt;VoucherOrder\u0026gt; orderTasks=new ArrayBlockingQueue\u0026lt;\u0026gt;(1024*1024); 1 2 3 4 5 6 7 8 9 10 11 //订单加入阻塞队列 //创建订单 VoucherOrder order = new VoucherOrder(); order.setVoucherId(voucherId); //TODO 生成了orderId，把订单信息保存到阻塞队列，由另一个线程专门根据订单信息去数据库做增删改查，这就实现了异步 long orderId = redisIdWorker.nextId(\u0026#34;order:\u0026#34;); order.setId(orderId); order.setUserId(userId); //添加到阻塞队列 orderTasks.add(order); 从阻塞队列中获取订单然后操作数据库 这里定义线程池，让线程去从阻塞队列中获取订单，实现异步操作数据库\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 //定义线程池，负责从阻塞队列中获取订单然后异步下单 private static final ExecutorService SECKILL_ORDER_EXECUTOR= Executors.newSingleThreadExecutor(); //定义线程 这是个内部类 private class VoucherOrderHandler implements Runnable{ @Override public void run() { while (true){ try { //获取队列中的订单 VoucherOrder order = orderTasks.take(); //创建订单 handleVoucherOrder(order); } catch (InterruptedException e) { log.error(\u0026#34;订单处理异常\u0026#34;,e); } } } } //spring提供的注解 作用：类初始化后就执行VoucherOrderHandler方法 //向线程池提交一个线程 @PostConstruct private void init(){ SECKILL_ORDER_EXECUTOR.submit(new VoucherOrderHandler()); } private void handleVoucherOrder(VoucherOrder order) { Long userId = order.getUserId(); RLock redisLock = redissonClient.getLock(\u0026#34;lock:order:\u0026#34; + userId); boolean tryLock = redisLock.tryLock(); //判断锁是否获取成功 if (!tryLock){ log.error(\u0026#34;不允许重复下单\u0026#34;); return ; } try { proxy.createVoucherOrder(order); //使用动态代理类的对象，事务可以生效 } finally { redisLock.unlock(); } } 完整代码 VoucherOrderServiceImpl.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 @Slf4j @Service public class VoucherOrderServiceImpl extends ServiceImpl\u0026lt;VoucherOrderMapper, VoucherOrder\u0026gt; implements IVoucherOrderService { @Resource private StringRedisTemplate stringRedisTemplate; @Resource private ISeckillVoucherService SeckillVoucherService; @Autowired private RedissonClient redissonClient; @Resource private RedisIdWorker redisIdWorker; //阻塞队列 当线程尝试从队列中获取元素时，若队列无元素，则线程会阻塞，直到队列中有元素才会被唤醒 private BlockingQueue\u0026lt;VoucherOrder\u0026gt; orderTasks=new ArrayBlockingQueue\u0026lt;\u0026gt;(1024*1024); //线程池，负责从阻塞队列中获取订单然后异步下单 private static final ExecutorService SECKILL_ORDER_EXECUTOR= Executors.newSingleThreadExecutor(); //spring提供的注解 作用：类初始化后就执行VoucherOrderHandler方法 //向线程池提交一个线程 @PostConstruct private void init(){ SECKILL_ORDER_EXECUTOR.submit(new VoucherOrderHandler()); } //线程 内部类 private class VoucherOrderHandler implements Runnable{ @Override public void run() { while (true){ try { //获取队列中的订单信息 VoucherOrder order = orderTasks.take(); //创建订单 handleVoucherOrder(order); } catch (InterruptedException e) { log.error(\u0026#34;订单处理异常\u0026#34;,e); } } } } //代理对象 //因为异步之后，子线程不能获取代理对象无法实现事务，所以要定义为全局变量，在主线程中就获取代理对象给子线程用 IVoucherOrderService proxy; private void handleVoucherOrder(VoucherOrder order) { Long userId = order.getUserId(); RLock redisLock = redissonClient.getLock(\u0026#34;lock:order:\u0026#34; + userId); boolean tryLock = redisLock.tryLock(); //判断锁是否获取成功 if (!tryLock){ log.error(\u0026#34;不允许重复下单\u0026#34;); return ; } try { //锁加到这里，事务提交后才释放锁 proxy.createVoucherOrder(order); //使用动态代理类的对象，事务可以生效 } finally { redisLock.unlock(); } } private static final DefaultRedisScript\u0026lt;Long\u0026gt; SECKILL_SCRIPT; static { SECKILL_SCRIPT=new DefaultRedisScript\u0026lt;\u0026gt;(); SECKILL_SCRIPT.setLocation(new ClassPathResource(\u0026#34;seckill.lua\u0026#34;)); SECKILL_SCRIPT.setResultType(Long.class); } @Override public Result seckillVoucher(Long voucherId) { Long userId = UserHolder.getUser().getId(); //执行lua脚本判断有无购买资格 Long result = stringRedisTemplate.execute(SECKILL_SCRIPT, Collections.emptyList(), voucherId.toString(), userId.toString() ); int i = result.intValue(); if (i!=0){ return Result.fail(i==1?\u0026#34;优惠券库存不足\u0026#34;:\u0026#34;不能重复下单\u0026#34;); } //创建订单 VoucherOrder order = new VoucherOrder(); order.setVoucherId(voucherId); //TODO 生成了orderId，把订单信息保存到阻塞队列，由另一个线程专门根据订单信息去数据库做增删改查，这就实现了异步 long orderId = redisIdWorker.nextId(\u0026#34;order:\u0026#34;); order.setId(orderId); order.setUserId(userId); //添加到阻塞队列 orderTasks.add(order); //获取事务的动态代理对象，需要在启动类加注解暴漏出对象 proxy = (IVoucherOrderService)AopContext.currentProxy();//拿到动态代理对象 return Result.ok(orderId); } //TODO spring对该类做了动态代理，用动态代理的对象提交的事务 @Transactional public void createVoucherOrder(VoucherOrder order) { //一人一单，根据优惠卷id和用户id去数据库查询是否已经存在该优惠卷 Long id = order.getUserId(); //为用户id加锁而不是对整个createVoucherOrder方法加锁，减小锁范围，提升性能，这样每个用户就有不同的锁 //锁加在函数内部，锁内的代码执行完后就会释放锁，而事务的提交是在整个方法执行后提交的，也就是事务的提交在锁释放之后。 //但是锁释放后其他线程就可以进来，此时事务可能还没有提交，可能出现并发问题，重复购买 //所以要扩大锁的范围，把锁加到seckillVoucher方法后面，在事务提交后才能释放锁！ int count = query().eq(\u0026#34;user_id\u0026#34;, id).eq(\u0026#34;voucher_id\u0026#34;, order).count(); if (count \u0026gt;=1) { //count==1说明用户拥有了一个优惠券 log.error(\u0026#34;不能重复下单\u0026#34;); return; // return Result.fail(\u0026#34;不能重复购买优惠卷\u0026#34;); } //4.扣减库存 防止超卖，加乐观锁，扣减库存前再查询一次库存判断 // boolean b = SeckillVoucherService.update() // .setSql(\u0026#34;stock=stock-1\u0026#34;). // eq(\u0026#34;voucher_id\u0026#34;, voucherId).eq(\u0026#34;stock\u0026#34;,voucher.getStock()).update(); //使用setSql方法设置了更新语句\u0026#34;stock=stock-1\u0026#34;，接着使用eq方法添加了两个条件：\u0026#34;voucher_id\u0026#34;等于voucherId和\u0026#34;stock\u0026#34;等于voucher.getStock() //条件1：voucher_id=voucherId指当前操作的优惠卷的id=数据库中的优惠卷id，即通过优惠卷id指明了要修改哪个优惠卷的库存 //条件2：stock=voucher.getStock,说明该线程修改库存期间没有其他线程来插队修改库存，那么数据是安全的 //TODO ！！！注意！这种操作在并发情况下可能导致用户在优惠卷库存充足的情况下抢购优惠卷失败，也就是即使有库存也会抢购失败，此时可以判断库存是否充足，重新抢购 //修改如下：最后库存判断，只要\u0026gt;0就可以修改 boolean b = SeckillVoucherService.update() .setSql(\u0026#34;stock=stock-1\u0026#34;). eq(\u0026#34;voucher_id\u0026#34;, order).gt(\u0026#34;stock\u0026#34;, 0) .update(); if (!b) { // return Result.fail(\u0026#34;库存不足\u0026#34;); log.error(\u0026#34;库存不足\u0026#34;); return; } save(order); } } 总结 所谓异步，就是把主线程的任务分给多个线程执行，提高业务执行速度 内存安全限制：我们使用的阻塞队列是 JDK 自带的，它基于 JVM 内存，因此通常需要人为规定队列长度，在高并发情况下，如果阻塞队列中的元素过多，占用的 JVM 内存也会增多（内存限制），同时如果服务宕机，阻塞队列中的数据也会丢失，因此也存在数据安全的问题。\n","date":"2024-07-06T19:11:07Z","permalink":"/zh-cn/post/2024/07/%E5%9F%BA%E4%BA%8Eredis%E5%92%8C%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97%E7%9A%84-%E5%BC%82%E6%AD%A5%E7%A7%92%E6%9D%80%E4%B8%9A%E5%8A%A1/","title":"基于Redis和阻塞队列的 异步秒杀业务"},{"content":"介绍 Redisson 什么是 Redisson？来自于官网上的描述内容如下！\nRedisson 是一个在 Redis 的基础上实现的 Java 驻内存数据网格客户端（In-Memory Data Grid）。它不仅提供了一系列的 redis 常用数据结构命令服务，还提供了许多分布式服务，例如分布式锁、分布式对象、分布式集合、分布式远程服务、分布式调度任务服务等等。\n相比于 Jedis、Lettuce 等基于 redis 命令封装的客户端，Redisson 提供的功能更加高端和抽象\n配置 Redisson 引入依赖 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.redisson\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;redisson\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.13.6\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 配置客户端类 1 2 3 4 5 6 7 8 9 10 11 @Configuration public class RedisConfig { @Bean public RedissonClient redissonClient(){ Config config = new Config(); //添加了单机redis地址，也可以使用useClusterServers()添加集群地址 config.useSingleServer().setAddress(\u0026#34;redis://192.168.2.129:6379\u0026#34;).setPassword(\u0026#34;linux02\u0026#34;); return Redisson.create(config); } } 使用 Redisson 分布式锁 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Long id = UserHolder.getUser().getId(); RLock redisLock = redissonClient.getLock(\u0026#34;lock:order:\u0026#34; + id); //尝试获取锁 boolean tryLock = redisLock.tryLock(); //判断锁是否获取成功 if (!tryLock){ return Result.fail(\u0026#34;不允许重复下单\u0026#34;); } try { //锁加到这里，事务提交后才释放锁 //获取事务的动态代理对象，需要在启动类加注解暴漏出对象 IVoucherOrderService proxy = (IVoucherOrderService)AopContext.currentProxy();//拿到动态代理对象 return proxy.createVoucherOrder(voucherId, voucher); //使用动态代理类的对象，事务可以生效 } finally { redisLock.unlock(); } 不可重入锁 在同一个线程中，method1 获取锁后，调用 method2，method2 中尝试获取锁，此时锁已经被 method1 获取，则 method2 获取锁失败，这就是不可重入锁，前面实现的锁就是不可重入锁！ Redisson 可重入锁 可重入锁，从字面来理解，就是可以重复进入的锁，也叫做递归锁，指的是同一线程外层函数获得锁之后，内层递归函数仍然有获取该锁的代码，但不受影响。\nReentrantLock 和synchronized都是可重入锁。\n在一个类中，如果 synchronized 方法 1 调用了 synchronized 方法 2，方法 2 是可以正常执行的，这说明 synchronized 是可重入锁。否则，在执行方法 2 想获取锁的时候，该锁已经在执行方法 1 时获取了，那么方法 2 将永远得不到执行。\n为了实现可重入锁，Redis 中使用 hash 类型不再使用 string 类型，**为什么要使用 hash 类型，**就不得不说到 Redisson 可重入锁在 redis 中的实现原理：\n实现原理 ：\n在同一线程中，method1 成功获取锁后调用 method2，method2 也尝试获取锁，此时要先判断 method2 所在线程和 method1 所在线程是否是同一线程，若是，则 method2 也获取锁成功，它和 method1 显然获取了同一个锁**，那么该锁被获取次数+1，而这个锁被获取的次数我们需要记录，也就是说 value 不仅要记录 线程名 还要记录 锁被获取的次数，那么我们就由此采用 hash 类型更合理！**\n为什么记录锁被获取的次数？\n一个业务的完成可能要多次获取锁，如一个业务中执行了 method1，method1 调用了 method2，method2 调用了 method3，这三个方法都加同一个锁（可重入锁），当 method3 执行完后，并不能立刻释放 method3 的锁，而是锁被获取的次数-1 ，因为锁是共享的，此时 method1 和 method2 还没执行完不能释放锁，那么什么时候释放锁？当然是锁被获取的次数减为 0 了，说明此时已经没有方法获取锁，那么可以安全的释放可重入锁了。\nSo 锁被获取的次数就是我们判断是否要释放锁的依据！\n","date":"2024-07-05T20:04:44Z","permalink":"/zh-cn/post/2024/07/redisson%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%8F%AF%E9%87%8D%E5%85%A5%E9%94%81/","title":"Redisson分布式锁、可重入锁"},{"content":"分布式场景下并发安全问题的引发 前面通过加锁解决了单机状态下一人一单的问题，但是当出现了分布式，前面的加锁形式不再适用 ，每个 jvm 有一个自己的锁监视器，只能被内部线程获取，其他 jvm 无法使用，那么多台 jvm 的锁监视器不共用一个锁监视器，就容易出现分布式场景下并发安全问题。\n问题分析 所以我们要使用可以解决分布式场景下的位于 jvm 外的锁，多个 jvm 共同使用该锁，而不是使用每个 jvm 的内部锁。\n分布式锁有如下特点：\n这里我们就选用 redis 来实现我们的分布式锁！\nRedis 锁的 demo redis 锁要实现如上两个基本操作：获取锁和删除锁，在获取锁的同时为了防止宕机出现死锁，要手动添加过期时间，那么为了防止只加锁没有加过期时间的情况出现，我们要保证加锁和加过期时间的原子性，也就是他俩必须同时进行！\n那么上述加锁的命令可以换成如下：\n分布式锁初步实现 实现锁接口 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 public class SimpleRedisLock implements ILock { //用户的userid private String name; private StringRedisTemplate stringRedisTemplate; public SimpleRedisLock(String name, StringRedisTemplate stringRedisTemplate) { this.name = name; this.stringRedisTemplate = stringRedisTemplate; } //锁的key值 private static final String KEY_PREFIX = \u0026#34;lock:\u0026#34;; //生成锁的value值 private static final String ID_PREFIX = UUID.randomUUID().toString(true) + \u0026#34;-\u0026#34;; @Override public boolean tryLock(long timeoutSec) { // 获取线程标示 String threadId = ID_PREFIX + Thread.currentThread().getId(); // 获取锁 Boolean success = stringRedisTemplate.opsForValue() .setIfAbsent(KEY_PREFIX + name, threadId, timeoutSec, TimeUnit.SECONDS); //防止Boolean和boolean拆箱出问题，如果success为null，则返回false return Boolean.TRUE.equals(success); } @Override public void unlock() { //释放锁 stringRedisTemplate.delete(KEY_PREFIX + name); } } 使用锁 在VoucherOrderServiceImpl.java 中的seckillVoucher 方法中编写如下代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 //获取用户userid Long id = UserHolder.getUser().getId(); SimpleRedisLock redisLock = new SimpleRedisLock(\u0026#34;order:\u0026#34; + id, stringRedisTemplate); //加锁，1200s是锁的过期时间 boolean tryLock = redisLock.tryLock(1200); //判断锁是否获取成功 if (!tryLock){ return Result.fail(\u0026#34;不允许重复下单\u0026#34;); } try { //锁加到这里，事务提交后才释放锁 //获取事务的动态代理对象，需要在启动类加注解暴漏出对象 IVoucherOrderService proxy = (IVoucherOrderService)AopContext.currentProxy();//拿到动态代理对象 // return createVoucherOrder(voucherId, voucher); return proxy.createVoucherOrder(voucherId, voucher); //使用动态代理类的对象，事务可以生效 } finally { //无论如何都要释放锁，防止死锁 redisLock.unlock(); } 分布式场景下调试看效果 两个 application，一个是 8081 端口，一个是 8082 端口。\napifox 模拟同一个用户发送请求，authorization 的参数值是同一个用户的，存储在 redis 中。\n如下：在 8082 的断点处获取锁失败，在 8081 的断点处获取锁成功，即只有一次成功获取锁。\n数据库中优惠券库存 stock-1 而不是-2，优惠券订单产生 1 个，数据库没问题！\nredis 中查看锁的 key 值，1010 正是 userid，问题解决，达到我们想要的效果！ Redis 分布式锁误删问题 问题分析 当线程 1 获取锁成功时，如果该业务执行时间长以至于超过了设置的锁过期时间，那么在业务还未完成时，锁便自动释放，此时线程 1 无锁，线程 2 获取到了锁执行业务，当线程 1 业务执行完后，按照业务逻辑仍会释放锁，但此时释放的是线程 2 的锁，这就出现了锁误删的问题。 解决锁误删 对于每个线程，我们获取其线程标识（每个 JVM 内部都维护了线程的 id，这个 id 是自增的，那么多个 jvm 可能出现线程 id 一致的情况，为了避免该情况出现我们用 UUID 生成一个随机字符串作为前缀，以降低线程 id 重复的概率）作为锁的 value\n在释放锁时，我们先从 redis 获取对应的 value 值，跟当前线程的 value 做对比，一致则可以删除，否则就不能删除。\n修改 trylock 和 unlock 方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 public class SimpleRedisLock implements ILock { //用户的userid private String name; private StringRedisTemplate stringRedisTemplate; public SimpleRedisLock(String name, StringRedisTemplate stringRedisTemplate) { this.name = name; this.stringRedisTemplate = stringRedisTemplate; } //锁的key值 private static final String KEY_PREFIX = \u0026#34;lock:\u0026#34;; //线程的前缀，因为分布式下，多个jvm，每个jvm中维护的线程的id都是递增的，那么可能出现多个jvm的线程id一致，所以这里用uuid生成字符串作为前缀 private static final String THREAD_PREFIX = UUID.randomUUID().toString(true)+\u0026#34;-\u0026#34;; @Override public boolean tryLock(long timeoutSec) { // 获取线程标示 线程前缀+线程id String threadId = THREAD_PREFIX + Thread.currentThread().getId(); // 获取锁 Boolean success = stringRedisTemplate.opsForValue() .setIfAbsent(KEY_PREFIX + name, threadId, timeoutSec, TimeUnit.SECONDS); //防止Boolean和boolean拆箱出问题，如果success为null，则返回false return Boolean.TRUE.equals(success); } @Override public void unlock() { // 获取线程标示 String threadId =THREAD_PREFIX + Thread.currentThread().getId(); // 获取锁中的标示 String id = stringRedisTemplate.opsForValue().get(KEY_PREFIX + name); // 判断锁标示是否一致，防止锁误删 if(threadId.equals(id)) { // 释放锁 stringRedisTemplate.delete(KEY_PREFIX + name); } } } 分布式锁的原子性 问题分析 JVM 做 Full GC 时会阻塞所有代码，时间过长会出现锁超时自动释放，那么其他线程会趁虚而入获得锁。\n那么会出现如下情况：线程 1 获取锁执行业务逻辑后要释放锁，在判断完释放锁的条件为 true 后，即 threadId.equals(id)==true， 正要释放锁时出现 Full GC，所有代码被阻塞，直到锁超时自动释放 （注意此时锁不是正常释放而是锁超时释放的），就在这时 GC 完毕代码恢复，线程 2 趁虚而入获得锁，而线程 1 也恢复了要执行释放锁的代码，**因为 GC 前已经判断过释放条件为 ture，那么此时线程 1 仍然认为锁是自己的，会错误地释放线程 2 的锁，又出现了误删问题。这里我们就要保证****锁的原子性，即 判断锁的标识 和 释放锁 两个动作必须同时发生！**\n问题解决（Lua 脚本） Lua 是一种编程语言，Redis 提供了 Lua 脚本功能，即可以在一个脚本中写多条 redis 指令，确保了多条命令执行的原子性 \u0026gt; \u0026gt; 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 public class SimpleRedisLock implements ILock { //用户的userid private String name; private StringRedisTemplate stringRedisTemplate; public SimpleRedisLock(String name, StringRedisTemplate stringRedisTemplate) { this.name = name; this.stringRedisTemplate = stringRedisTemplate; } //锁的key值 private static final String KEY_PREFIX = \u0026#34;lock:\u0026#34;; //线程的前缀，因为分布式下，多个jvm，每个jvm中维护的线程的id都是递增的，那么可能出现多个jvm的线程id一致，所以这里用uuid生成字符串作为前缀 private static final String THREAD_PREFIX = UUID.randomUUID().toString(true)+\u0026#34;-\u0026#34;; private static final DefaultRedisScript\u0026lt;Long\u0026gt; UNLOCK_SCRIPT; static { UNLOCK_SCRIPT = new DefaultRedisScript\u0026lt;\u0026gt;(); UNLOCK_SCRIPT.setLocation(new ClassPathResource(\u0026#34;unlock.lua\u0026#34;)); UNLOCK_SCRIPT.setResultType(Long.class); } @Override public boolean tryLock(long timeoutSec) { // 获取线程标示 线程前缀+线程id String threadId = THREAD_PREFIX + Thread.currentThread().getId(); // 获取锁 Boolean success = stringRedisTemplate.opsForValue() .setIfAbsent(KEY_PREFIX + name, threadId, timeoutSec, TimeUnit.SECONDS); //防止Boolean和boolean拆箱出问题，如果success为null，则返回false return Boolean.TRUE.equals(success); } @Override public void unlock() { // 调用lua脚本 stringRedisTemplate.execute( UNLOCK_SCRIPT, Collections.singletonList(KEY_PREFIX + name), THREAD_PREFIX + Thread.currentThread().getId()); } } 在 resource 资源文件夹下创建 Lua 脚本内容如下：\n1 2 3 4 if (redis.call(\u0026#39;get\u0026#39;,KEYS[1])==ARGV[1]) then return redis.call(\u0026#39;del\u0026#39;,KEYS[1]) end return 0 ","date":"2024-07-04T23:15:25Z","permalink":"/zh-cn/post/2024/07/%E5%9F%BA%E4%BA%8Eredis%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","title":"基于Redis的分布式锁"},{"content":"乐观锁、悲观锁 优惠券超卖 超卖场景复现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 @Service public class VoucherOrderServiceImpl extends ServiceImpl\u0026lt;VoucherOrderMapper, VoucherOrder\u0026gt; implements IVoucherOrderService { @Resource private ISeckillVoucherService SeckillVoucherService; @Resource private RedisIdWorker redisIdWorker; @Transactional @Override public Result seckillVoucher(Long voucherId) { //1.根据id查询优惠卷 SeckillVoucher voucher = SeckillVoucherService.getById(voucherId); //2.判断是否开始或结束 if (voucher.getBeginTime().isAfter(LocalDateTime.now())){ return Result.fail(\u0026#34;秒杀尚未开始\u0026#34;); } if (voucher.getEndTime().isBefore(LocalDateTime.now())){ return Result.fail(\u0026#34;秒杀已经结束\u0026#34;); } //3.判断库存 if (voucher.getStock()\u0026lt;1){ return Result.fail(\u0026#34;库存不足\u0026#34;); } //4.扣减库存 防止超卖，加乐观锁，扣减库存前再查询一次库存判断 boolean b = SeckillVoucherService.update() .setSql(\u0026#34;stock=stock-1\u0026#34;). eq(\u0026#34;voucher_id\u0026#34;, voucherId).gt(\u0026#34;stock\u0026#34;, voucher.getStock()).update(); if (!b) { return Result.fail(\u0026#34;库存不足\u0026#34;); } //5.创建订单，并插入数据库 VoucherOrder order = new VoucherOrder(); Long id = UserHolder.getUser().getId(); order.setVoucherId(voucherId); //生成订单的全局唯一ID long orderID = redisIdWorker.nextId(\u0026#34;voucherOrder\u0026#34;); order.setId(orderID); order.setUserId(id); save(order); //7.返回订单 return Result.ok(\u0026#34;orderID\u0026#34;); } } 上述代码适用于非高并发情况下，然而真实的情况是很多用户同时下单，是并发问题，那么此时就会出现超卖问题。 可以使用 apache JMeter 或者 ApiFox 模拟多线程并发场景：\n在数据库中设置秒杀券库存为 100，JMeter 设置 200 并发线程模拟 200 个用户同时下单。\n由图二可以看到异常值是 45.5%，但是实际应该是 50%，因为只有 100 个库存，200 个用户应该只有 100 个用户能抢到秒杀券 ，打开数据库发现：\n秒杀券数量由最初的 100 变为-9，而不是 0，此时就是超卖问题！\n原因分析 超卖问题就是线程并发安全问题 ，在一个线程修改数据的同时插入其他线程并发操作，进而出现数据错误，那么解决方法就是加锁，这里加乐观锁！\n加乐观锁 乐观锁其实并不是真的加锁，而是在最后要更新数据库数据时做版本判断，若此时的数据版本和最初的数据版本不一致，则认为在该线程执行过程中有其他线程插入做了数据修改，此时就认为数据不是安全的，就要报异常或者重试！而如果版本一致，则认为数据安全，那么就会更新数据。\n乐观锁在最后执行数据更新的时候进行判断，不用加锁，因此性能比悲观锁高。\n解决方法 这里采用第二种 CAS 法，利用库存代替版本号。\n根据以上分析，解决方法就是在扣减库存进行数据更新时多加一步操作：判断此时更新时的库存是否和上一步判断库存是否充足时的库存是否一致，一致则认为没有线程并发\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 @Service public class VoucherOrderServiceImpl extends ServiceImpl\u0026lt;VoucherOrderMapper, VoucherOrder\u0026gt; implements IVoucherOrderService { @Resource private ISeckillVoucherService SeckillVoucherService; @Resource private RedisIdWorker redisIdWorker; @Transactional @Override public Result seckillVoucher(Long voucherId) { //1.根据id查询优惠卷 SeckillVoucher voucher = SeckillVoucherService.getById(voucherId); //2.判断是否开始或结束 if (voucher.getBeginTime().isAfter(LocalDateTime.now())){ return Result.fail(\u0026#34;秒杀尚未开始\u0026#34;); } if (voucher.getEndTime().isBefore(LocalDateTime.now())){ return Result.fail(\u0026#34;秒杀已经结束\u0026#34;); } //3.判断库存 if (voucher.getStock()\u0026lt;1){ return Result.fail(\u0026#34;库存不足\u0026#34;); } //4.扣减库存 防止超卖，加乐观锁，扣减库存前再查询一次库存判断 boolean b = SeckillVoucherService.update() .setSql(\u0026#34;stock=stock-1\u0026#34;). eq(\u0026#34;voucher_id\u0026#34;, voucherId).eq(\u0026#34;stock\u0026#34;,voucher.getStock()).update(); //使用setSql方法设置了更新语句\u0026#34;stock=stock-1\u0026#34;，接着使用eq方法添加了两个条件：\u0026#34;voucher_id\u0026#34;等于voucherId和\u0026#34;stock\u0026#34;等于voucher.getStock() //条件1：voucher_id=voucherId指当前操作的优惠卷的id=数据库中的优惠卷id，即通过优惠卷id指明了要修改哪个优惠卷的库存 //条件2：stock=voucher.getStock,说明该线程修改库存期间没有其他线程来插队修改库存，那么数据是安全的 if (!b) { return Result.fail(\u0026#34;秒杀失败\u0026#34;); } //5.创建订单 VoucherOrder order = new VoucherOrder(); Long id = UserHolder.getUser().getId(); order.setVoucherId(voucherId); long orderID = redisIdWorker.nextId(\u0026#34;voucherOrder\u0026#34;); order.setId(orderID); order.setUserId(id); save(order); //7.返回订单 return Result.ok(orderID); } } 重新使用 JMeter 模拟 200 个用户并发抢购 100 库存的秒杀券：\n发现异常率提升到 89%，说明有 89%的用户抢购失败！\n数据库库存由 100 变为 79，只卖出 21 个。\n原因在于上述代码在扣减库存时加的乐观锁，我们的判断方法是：\n1 eq(\u0026#34;stock\u0026#34;,voucher.getStock()) 也就是说即使在库存充足的情况下，只要库存数量上前后判断不对等，就会抢购失败，所以要对乐观锁进行改进！\n我们实际业务是只要库存\u0026gt;0 就可以抢购，因此只需要把 eq 换成 gt 即可！\n重新模拟并发测试，异常值是 50%，说明 200 个用户抢购，只有 100 个抢购成功，库存正好清零，问题解决！\n总结 然而在实际的高并发场景下，对数据库的大量更新操作是不可取的 ，后续进行优化！\n一人一单 一人多单场景复现 一人一单的代码实现，是在上述代码的基础上，在扣减库存之前，根据用户的 id 和要下单的秒杀券 id 去数据库查询，如果没查到就可以购买，如果查到就说明已经下过单，不能重复下单。如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 @Service public class VoucherOrderServiceImpl extends ServiceImpl\u0026lt;VoucherOrderMapper, VoucherOrder\u0026gt; implements IVoucherOrderService { @Resource private ISeckillVoucherService SeckillVoucherService; @Resource private RedisIdWorker redisIdWorker; @Transactional @Override public Result seckillVoucher(Long voucherId) { //1.根据id查询优惠卷 SeckillVoucher voucher = SeckillVoucherService.getById(voucherId); //2.判断是否开始或结束 if (voucher.getBeginTime().isAfter(LocalDateTime.now())){ return Result.fail(\u0026#34;秒杀尚未开始\u0026#34;); } if (voucher.getEndTime().isBefore(LocalDateTime.now())){ return Result.fail(\u0026#34;秒杀已经结束\u0026#34;); } //3.判断库存 if (voucher.getStock()\u0026lt;1){ return Result.fail(\u0026#34;库存不足\u0026#34;); } //一人一单，根据优惠卷id和用户id去数据库查询是否已经存在该优惠卷 Long id = UserHolder.getUser().getId(); int count = query().eq(\u0026#34;user_id\u0026#34;, id).eq(\u0026#34;voucher_id\u0026#34;, voucherId).count(); if (count == 1) { return Result.fail(\u0026#34;不能重复购买优惠卷\u0026#34;); } //4.扣减库存 防止超卖，加乐观锁，扣减库存前再查询一次库存判断，只要\u0026gt;0就可以修改 boolean b = SeckillVoucherService.update() .setSql(\u0026#34;stock=stock-1\u0026#34;). eq(\u0026#34;voucher_id\u0026#34;, voucherId).gt(\u0026#34;stock\u0026#34;, voucher.getStock()).update(); if (!b) { return Result.fail(\u0026#34;库存不足\u0026#34;); } //5.创建订单，插入数据库 VoucherOrder order = new VoucherOrder(); order.setVoucherId(voucherId); long orderID = redisIdWorker.nextId(\u0026#34;voucherOrder\u0026#34;); order.setId(orderID); order.setUserId(id); save(order); //7.返回订单 return Result.ok(orderID); } } 使用 JMeter 模拟一个用户 200 次下单，结果如下，发现异常值是 95%，数据库中一个用户下了 10 个单。 原因分析 先查询再判断，多个线程并发，count == 1 都判断为假，因为此时都查询为 0 还没有下单，所以后面可以重复下单。此处解决方法是加悲观锁，在更新数据时加乐观锁，插入数据时加悲观锁。\n加悲观锁 从一人一单开始到最后的保存订单这部分提取成方法 createVoucherOrder，对该方法加锁，即\n1 public synchronized Result createVoucherOrder(Long voucherId, SeckillVoucher voucher) 但是这样是对整个方法加锁，这样该方法只能串行执行，即一个用户占用执行该方法时其他用户无法执行该方法，会极大影响性能，而我们加锁，是要判断是否是同一个用户，因此要对用户的 ID 加锁，这样同一个用户加同一把锁，缩小了锁的范围，提升了性能，并且可以并行执行方法。\n对同一个用户 ID 加锁：\n1 synchronized (id.toString()){.....} 注意！每次新的请求，Long id = UserHolder.getUser().getId() 也都是新的对象，那么每次的锁都是新的，也就是同一个用户多次请求，id 对象不是同一个，那么最后加的锁也不是同一个，这样违背了我们的想法，我们是对 ID 的值加锁而不是对 id 这个对象加锁！因此要写成这样：\n1 synchronized (id.toString().intern()){....} intern 方法返回字符串对象的规范表示，调用该方法时，如果常量池中已经包含一个等于这个 string 对象的字符串（由 equals(object)方法确定），则返回池中的字符串引用，而不是新创建！\n这样就确保了用户 id 值一样，锁也就一样。代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 @Service public class VoucherOrderServiceImpl extends ServiceImpl\u0026lt;VoucherOrderMapper, VoucherOrder\u0026gt; implements IVoucherOrderService { @Resource private ISeckillVoucherService SeckillVoucherService; @Resource private RedisIdWorker redisIdWorker; // @Transactional @Override public Result seckillVoucher(Long voucherId) { //1.根据id查询优惠卷 SeckillVoucher voucher = SeckillVoucherService.getById(voucherId); //2.判断是否开始或结束 if (voucher.getBeginTime().isAfter(LocalDateTime.now())){ return Result.fail(\u0026#34;秒杀尚未开始\u0026#34;); } if (voucher.getEndTime().isBefore(LocalDateTime.now())){ return Result.fail(\u0026#34;秒杀已经结束\u0026#34;); } //3.判断库存 if (voucher.getStock()\u0026lt;1){ return Result.fail(\u0026#34;库存不足\u0026#34;); } //库存充足就创建订单 return createVoucherOrder(voucherId, voucher); } } @Transactional public Result createVoucherOrder(Long voucherId, SeckillVoucher voucher) { //一人一单，根据优惠卷id和用户id去数据库查询是否已经存在该优惠卷 Long id = UserHolder.getUser().getId(); synchronized(id.toString.intern()){ //为用户id加锁而不是对整个createVoucherOrder方法加锁，减小锁范围，提升性能 int count = query().eq(\u0026#34;user_id\u0026#34;, id).eq(\u0026#34;voucher_id\u0026#34;, voucherId).count(); if (count == 1) { return Result.fail(\u0026#34;不能重复购买优惠卷\u0026#34;); } //4.扣减库存 防止超卖，加乐观锁，扣减库存前再查询一次库存判断，最后库存判断，只要\u0026gt;0就可以修改 boolean b = SeckillVoucherService.update() .setSql(\u0026#34;stock=stock-1\u0026#34;). eq(\u0026#34;voucher_id\u0026#34;, voucherId).gt(\u0026#34;stock\u0026#34;, voucher.getStock()).update(); if (!b) { return Result.fail(\u0026#34;库存不足\u0026#34;); } //5.创建订单 VoucherOrder order = new VoucherOrder(); order.setVoucherId(voucherId); long orderID = redisIdWorker.nextId(\u0026#34;voucherOrder\u0026#34;); order.setId(orderID); order.setUserId(id); save(order); } //7.返回订单 return Result.ok(orderID); } } 事务失效 上述代码仍有问题，那就是事务失效，createVoucherOrder 方法加了事务注解@Transactional，那么事务的提交是在整个 createVoucherOrder 方法执行后由 spring 提交的，而锁的释放是在锁代码块执行后释放，上述代码的结果是锁先释放，然后方法执行完提交事务，这样是不安全的，因为锁释放之后，可能该事务还未提交，也就是该用户的订单还未写入数据库，那么此时该用户在数据库层面上还没有下单，那么另一个线程就进来判断后就可以下新的订单，出现重复下单。\n所以这里我们锁的范围又小了，应该对调用 createVoucherOrder 方法的代码块加锁，使得先提交事务，后释放锁。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @Service public class VoucherOrderServiceImpl extends ServiceImpl\u0026lt;VoucherOrderMapper, VoucherOrder\u0026gt; implements IVoucherOrderService { @Resource private ISeckillVoucherService SeckillVoucherService; @Resource private RedisIdWorker redisIdWorker; // @Transactional @Override public Result seckillVoucher(Long voucherId) { //1.根据id查询优惠卷 SeckillVoucher voucher = SeckillVoucherService.getById(voucherId); //2.判断是否开始或结束 if (voucher.getBeginTime().isAfter(LocalDateTime.now())){ return Result.fail(\u0026#34;秒杀尚未开始\u0026#34;); } if (voucher.getEndTime().isBefore(LocalDateTime.now())){ return Result.fail(\u0026#34;秒杀已经结束\u0026#34;); } //3.判断库存 if (voucher.getStock()\u0026lt;1){ return Result.fail(\u0026#34;库存不足\u0026#34;); } //库存充足就创建订单,锁加到这里 Long id = UserHolder.getUser().getId(); synchronized(id.toString.intern()){ return createVoucherOrder(voucherId, voucher); } } } @Transactional public Result createVoucherOrder(Long voucherId, SeckillVoucher voucher) { //一人一单，根据优惠卷id和用户id去数据库查询是否已经存在该优惠卷 Long id = UserHolder.getUser().getId(); //为用户id加锁而不是对整个createVoucherOrder方法加锁，减小锁范围，提升性能 int count = query().eq(\u0026#34;user_id\u0026#34;, id).eq(\u0026#34;voucher_id\u0026#34;, voucherId).count(); if (count == 1) { return Result.fail(\u0026#34;不能重复购买优惠卷\u0026#34;); } //4.扣减库存 防止超卖，加乐观锁，扣减库存前再查询一次库存判断，最后库存判断，只要\u0026gt;0就可以修改 boolean b = SeckillVoucherService.update() .setSql(\u0026#34;stock=stock-1\u0026#34;). eq(\u0026#34;voucher_id\u0026#34;, voucherId).gt(\u0026#34;stock\u0026#34;, voucher.getStock()).update(); if (!b) { return Result.fail(\u0026#34;库存不足\u0026#34;); } //5.创建订单 VoucherOrder order = new VoucherOrder(); order.setVoucherId(voucherId); long orderID = redisIdWorker.nextId(\u0026#34;voucherOrder\u0026#34;); order.setId(orderID); order.setUserId(id); save(order); //7.返回订单 return Result.ok(orderID); } } BUT！！！seckillVoucher 方法没有加事务注解，该方法的 return createVoucherOrder(voucherId, voucher)，实际是 return this.createVoucherOrder(voucherId, voucher)，即使用 VoucherOrderServiceImpl 对象调用的 createVoucherOrder 方法，而 createVoucherOrder 方法的事务之所以能生效，是因为该方法的调用是由 VoucherOrderServiceImpl 的代理对象调用的。\n也就是说 VoucherOrderServiceImpl 调用 createVoucherOrder 事务不生效，VoucherOrderServiceImpl 的代理对象调用 createVoucherOrder 事务才生效\n解决方法 既然 VoucherOrderServiceImpl 调用 createVoucherOrder 事务不生效，那我们就使用 VoucherOrderServiceImpl 的代理对象调用 createVoucherOrder，在 seckillVoucher 方法的加锁代码中，通过如下代码拿到代理对象，并使用代理对象调用 createVoucherOrder 方法：\n1 2 IVoucherOrderService proxy = (IVoucherOrderService)AopContext.currentProxy();//拿到动态代理对象 return proxy.createVoucherOrder(voucherId, voucher); 然而要获取代理对象，还需要如下操作：\n1.添加依赖\n1 2 3 4 5 \u0026lt;!--VoucherOrderServiceImpl类获取动态代理类对象 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.aspectj\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;aspectjweaver\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 2.启动类加注解\n1 2 //暴露代理对象。这样 VoucherOrderServiceImpl 中才可以获得动态代理对象 @EnableAspectJAutoProxy(exposeProxy = true) 最终代码:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @Service public class VoucherOrderServiceImpl extends ServiceImpl\u0026lt;VoucherOrderMapper, VoucherOrder\u0026gt; implements IVoucherOrderService { @Resource private ISeckillVoucherService SeckillVoucherService; @Resource private RedisIdWorker redisIdWorker; // @Transactional @Override public Result seckillVoucher(Long voucherId) { //1.根据id查询优惠卷 SeckillVoucher voucher = SeckillVoucherService.getById(voucherId); //2.判断是否开始或结束 if (voucher.getBeginTime().isAfter(LocalDateTime.now())){ return Result.fail(\u0026#34;秒杀尚未开始\u0026#34;); } if (voucher.getEndTime().isBefore(LocalDateTime.now())){ return Result.fail(\u0026#34;秒杀已经结束\u0026#34;); } //3.判断库存 if (voucher.getStock()\u0026lt;1){ return Result.fail(\u0026#34;库存不足\u0026#34;); } Long id = UserHolder.getUser().getId(); synchronized (id.toString().intern()) { //锁加到这里，事务提交后才释放锁 //获取事务的动态代理对象，需要在启动类加注解暴漏出对象 IVoucherOrderService proxy = (IVoucherOrderService)AopContext.currentProxy();//拿到动态代理对象 return proxy.createVoucherOrder(voucherId, voucher); //使用动态代理类的对象，事务可以生效 } //TODO 这里提交的事务是由类对象提交的，不是动态代理对象，因此出现事务失效 //因此需要拿到该类的动态代理对象 } //TODO spring对该类做了动态代理，用动态代理的对象提交的事务 @Transactional public Result createVoucherOrder(Long voucherId, SeckillVoucher voucher) { //一人一单，根据优惠卷id和用户id去数据库查询是否已经存在该优惠卷 Long id = UserHolder.getUser().getId(); //为用户id加锁而不是对整个createVoucherOrder方法加锁，减小锁范围，提升性能，这样每个用户就有不同的锁 //锁加在函数内部，锁内的代码执行完后就会释放锁，而事务的提交是在整个方法执行后提交的，也就是事务的提交在锁释放之后。 //但是锁释放后其他线程就可以进来，此时事务可能还没有提交，可能出现并发问题，重复购买 //所以要扩大锁的范围，把锁加到seckillVoucher方法后面，在事务提交后才能释放锁！ int count = query().eq(\u0026#34;user_id\u0026#34;, id).eq(\u0026#34;voucher_id\u0026#34;, voucherId).count(); if (count == 1) { return Result.fail(\u0026#34;不能重复购买优惠卷\u0026#34;); } //4.扣减库存 防止超卖，加乐观锁，扣减库存前再查询一次库存判断 boolean b = SeckillVoucherService.update() .setSql(\u0026#34;stock=stock-1\u0026#34;). eq(\u0026#34;voucher_id\u0026#34;, voucherId).gt(\u0026#34;stock\u0026#34;, voucher.getStock()).update(); if (!b) { return Result.fail(\u0026#34;库存不足\u0026#34;); } //5.创建订单 VoucherOrder order = new VoucherOrder(); order.setVoucherId(voucherId); long orderID = redisIdWorker.nextId(\u0026#34;voucherOrder\u0026#34;); order.setId(orderID); order.setUserId(id); save(order); //7.返回订单 return Result.ok(orderID); } } 仍然 200 个线程模拟一个用户多次下单\n异常率 99%，数据库中库存减 1，说明用户一人一单代码成功！\n总结\n秒杀优惠券业务，更新库存时，加乐观锁而不是悲观锁，可以提升性能 一人一单数据库插入订单时，加悲观锁，原本对整个方法加锁，不能并行化，现在对用户 ID 加锁，减小锁范围，提高性能，用户得以并行化 锁代码块执行完先释放锁，方法执行完后提交事务，出现了先释放锁后提交事务的情况，不安全，因此要扩大锁范围。因此对调用事务方法的代码块加锁 事务的提交由 spring 创建的代理类对象提交，普通的类对象无法提交事务，那么为了避免出现事务失效的情况，要获取类的代理类对象，由代理类对象去调用事务方法 ","date":"2024-05-18T22:56:09Z","permalink":"/zh-cn/post/2024/05/redis%E5%AE%9E%E6%88%98%E4%B9%90%E8%A7%82%E9%94%81%E6%82%B2%E8%A7%82%E9%94%81%E4%BC%98%E6%83%A0%E5%8A%B5%E8%B6%85%E5%8D%96%E4%B8%80%E4%BA%BA%E4%B8%80%E5%8D%95%E9%97%AE%E9%A2%98/","title":"Redis实战—乐观锁、悲观锁、优惠劵超卖、一人一单问题"},{"content":" 缓存穿透\n客户端请求的数据在缓存和数据库中都不存在，这样缓存永远不会生效，那么所有请求都会直接打到数据库上，增大数据库压力 解决：\n缓存空对象、布隆过滤、增加 id 复杂度，避免被猜出规律、做好基础数据格式校验、加强用户权限校验、做热点参数限流 缓存击穿（热点 key）\n一个被高并发访问并且缓存重建业务复杂的 key 突然失效，大量请求瞬间打到数据库 解决：\n互斥锁、逻辑过期 缓存雪崩\n同一时段的大量 key 同时失效或 redis 宕机，导致大量请求直接打到数据库 解决：\n给不同的 key 的 TTL 添加随机值、提高 redis 集群的高可用、缓存业务添加限流策略、添加多级缓存 缓存穿透 解决案例\u0026mdash;缓存空对象 1 2 3 4 5 6 7 8 9 10 11 12 13 14 //缓存穿透 public Shop queryWithPassThrough(Long id){ //1.从redis查询缓存 String shopJson = stringRedisTemplate.opsForValue().get(CACHE_SHOP_KEY+id); //2.判断shopJson是否有数据，有直接返回，没有判空 if (StrUtil.isNotBlank(shopJson)){ //判断某字符串是否不为空且长度不为0且不由空白符构成 //判断参数：是否不为空，长度是否不为0，值是否不包含空白字符 Shop shop = JSONUtil.toBean(shopJson, Shop.class); return shop; } //3.是否命中缓存的空对象 //shopJson不 缓存击穿 解决方法 互斥锁案例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 public Shop queryWithMutex(Long id){ //1.从redis查询缓存 String shopJson = stringRedisTemplate.opsForValue().get(\u0026#34;cache:shop:\u0026#34;+id); if (StrUtil.isNotBlank(shopJson)){ //判断某字符串是否不为空且长度不为0且不由空白符构成 //判断参数：是否不为空，长度是否不为0，值是否不包含空白字符 Shop shop = JSONUtil.toBean(shopJson, Shop.class); return shop; } //shopJson不为空说明长度为0或者由空白符构成 //没有命中空值 if (shopJson!=null){ return null; } //shopJson为空则查数据库加缓存 //缓存重建 //1.获取互斥锁 String lockKey=\u0026#34;lock:shop:\u0026#34;+id; Shop shop = null; try { boolean triedLock = tryLock(lockKey); //2.判断是否获取锁成功 if (!triedLock){ //3.获取锁失败则休眠50ms，醒来重试 Thread.sleep(50); return queryWithMutex(id); } //4.获取锁成功则查询数据库，存在则返回并缓存重建，不存在则缓存空值并返回 shop = getById(id); //模拟重建延迟 //Thread.sleep(200); if (shop==null){ //5.防止redis穿透，查询数据库为空，就缓存一个空值，并设置过期时间 stringRedisTemplate.opsForValue().set(\u0026#34;cache:shop:\u0026#34;,\u0026#34;\u0026#34;,30L,TimeUnit.MINUTES); //数据库没有，返回 return null; } //6.数据库查询到信息，添加缓存，设置超时剔除 stringRedisTemplate.opsForValue().set(\u0026#34;cache:shop:\u0026#34;+id,JSONUtil.toJsonStr(shop),30L, TimeUnit.MINUTES); } catch (InterruptedException e) { throw new RuntimeException(e); } finally { //7.释放互斥锁 unLock(lockKey); } return shop; } //缓存击穿问题，获取互斥锁 private boolean tryLock(String key){ Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(key, \u0026#34;1\u0026#34;, 10, TimeUnit.SECONDS); return BooleanUtil.isTrue(flag); } //释放互斥锁 private void unLock(String key){ stringRedisTemplate.delete(key); } 这里模拟高并发，使用 apifox\n逻辑过期案例 逻辑过期并不是设置 TTL，而是为缓存添加一个逻辑上的时间字段，判断该字段是否过期，过期则缓存重建\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 //缓存重建，也可以用于缓存预热,提前把热点数据缓存到redis private void saveShop2Redis(Long id,Long expireSeconds) throws InterruptedException { //1.查询店铺数据 Shop shop = getById(id); //模拟缓存重建延迟 Thread.sleep(200); //2.封装逻辑过期时间 RedisData data = new RedisData(); data.setData(shop); //设置逻辑过期时间 data.setExpireTime(LocalDateTime.now().plusSeconds(expireSeconds)); //3.写入redis stringRedisTemplate.opsForValue().set(CACHE_SHOP_KEY+id,JSONUtil.toJsonStr(data)); } //线程池，10个线程 private static final ExecutorService CACHE_REBUILD= Executors.newFixedThreadPool(10); //逻辑过期解决缓存击穿 //热点key一般提前添加到缓存，也就是缓存预热。 //如果没有命中，说明业务上要求查询的该数据不展示，也就不需要再去查数据库了，这点看实际的业务需求 //命中但是过期了就需要缓存重建，此时要去查数据库 public Shop queryWithLogicExpire(Long id){ //1.从redis查询缓存 String shopJson = stringRedisTemplate.opsForValue().get(CACHE_SHOP_KEY+id); if (StrUtil.isBlank(shopJson)){ //2.没有命中直接返回null return null; } //3.命中redis，把json反序列化成对象 RedisData redisData = JSONUtil.toBean(shopJson, RedisData.class); JSONObject data = (JSONObject)redisData.getData(); Shop shop = JSONUtil.toBean(data, Shop.class); LocalDateTime expireTime = redisData.getExpireTime(); //4.判断逻辑时间是否过期 if (expireTime.isAfter(LocalDateTime.now())){ //5.未过期，直接返回 return shop; } //6.过期,缓存重建,获取互斥锁，判断是否获取锁成功 String key=\u0026#34;lock:shop:\u0026#34;+id; boolean triedLock = tryLock(key); if (triedLock){ // 7.获取锁成功，使用线程池开启独立线程缓存重建 CACHE_REBUILD.submit( ()-\u0026gt;{ try { this.saveShop2Redis(id,1800L); } catch (Exception e) { throw new RuntimeException(e); } finally { unLock(key); } } ) } //8.获取锁失败，先返回过期的商铺信息 return shop; } //获取互斥锁 private boolean tryLock(String key){ Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(key, \u0026#34;1\u0026#34;, 10, TimeUnit.SECONDS); return BooleanUtil.isTrue(flag); } //释放互斥锁 private void unLock(String key){ stringRedisTemplate.delete(key); } 缓存雪崩 ","date":"2024-05-16T18:00:00Z","permalink":"/zh-cn/post/2024/05/%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9%E5%85%A5%E9%97%A8/","title":"缓存穿透、缓存击穿、缓存雪崩【入门】"},{"content":"基于 Session Controller 层 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 /** * 发送手机验证码 */ @PostMapping(\u0026#34;code\u0026#34;) public Result sendCode(@RequestParam(\u0026#34;phone\u0026#34;) String phone, HttpSession session) { // TODO 发送短信验证码并保存验证码 return userService.sendCode(phone,session); } /** * 登录功能,不存在用户则自动创建用户 * @param loginForm 登录参数，包含手机号、验证码；或者手机号、密码 * @RequestBody注解用于把前端的json数据转换成DTO对象 */ @PostMapping(\u0026#34;/login\u0026#34;) public Result login(@RequestBody LoginFormDTO loginForm, HttpSession session){ //将请求体中的数据转换为特定的对象或数据类型绑定到方法的参数上 // TODO 实现登录功能 return userService.login(loginForm,session); } Service 层 1 2 3 Result sendCode(String phone, HttpSession session); Result login(LoginFormDTO loginForm, HttpSession session); ServiceImpl 层 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 //登录 @Override public Result login(LoginFormDTO loginForm, HttpSession session) { //发送验证码和登录是两次不同的请求，要对手机号做二次校验 //1.再次获取手机号并再次校验手机号 String phone = loginForm.getPhone(); if(RegexUtils.isPhoneInvalid(phone)){ //2.不符合返回错误信息 return Result.fail(\u0026#34;手机号格式错误\u0026#34;); } //3.手机号校验成功，校验验证码，验证码从session获取 String code = session.getAttribute(\u0026#34;code\u0026#34;);//发送的验证码 String logincode = loginForm.getCode();//用户填写的验证码 if (code==null||!code.toString().equals(logincode)){ //4.验证码错误，返回信息 return Result.fail(\u0026#34;验证码错误\u0026#34;); } //5.验证码一致，查询是否存在用户 //query()是Mybatis-Plus提供的，本类继承了extends ServiceImpl\u0026lt;UserMapper, User\u0026gt; User user = query().eq(\u0026#34;phone\u0026#34;, phone).one(); if (user==null){ //6.用户不存在则创建新用户，新用户信息要保存到session，所以这里把新建的user返回 user=createUserWithPhone(phone); } // 7.保存用户信息到session //user信息过多，不易直接保存到session，会加大内存负载，此处转化成userDTO,还可以隐藏用户敏感信息 UserDTO userDTO = BeanUtil.copyProperties(user, UserDTO.class); session.setAttribute(\u0026#34;user\u0026#34;,userDTO); //访问tomcat时。sessionID自动写到了cookie中作为以后的登录凭证 return Result.ok(); } //自动注册用户 private User createUserWithPhone(String phone) { User user=new User(); user.setPhone(phone); //生成随机用户名 user.setNickName(\u0026#34;user_\u0026#34;+RandomUtil.randomString(5)); save(user);//保存用户到数据库,mybatis-plus提供的 return user; } //发送验证码 @Override public Result sendCode(String phone, HttpSession session) { //1. 校验手机号 if(RegexUtils.isPhoneInvalid(phone)){ //2. 不符合，返回错误信息 return Result.fail(\u0026#34;手机号格式错误\u0026#34;); } //3.符合，生成6位验证码 String code = RandomUtil.randomNumbers(6); //4.验证码保存到session稍后返回给服务端，以便服务端进行登录验证 session.setAttribute(\u0026#34;code\u0026#34;,code); //5.发送验证码，这里验证码输出到控制台 log.info(\u0026#34;验证码发送成功：{}\u0026#34;,code); //6.返回 return Result.ok(); } 校验登录状态 上面完成了短信验证码的发送及登录注册，用户如果登陆成功就要进入个人主页，但是进入个人主页需要验证用户的登录状态，接下来使用拦截器和 ThreadLocal 完成登录状态校验\nThreadLocal 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public class UserHolder { private static final ThreadLocal\u0026lt;UserDTO\u0026gt; tl = new ThreadLocal\u0026lt;\u0026gt;(); public static void saveUser(UserDTO user){ tl.set(user); } public static UserDTO getUser(){ return tl.get(); } public static void removeUser(){ tl.remove(); } } 登录拦截器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 //这是一个自定义的拦截器类，用于校验登陆状态 //需要在config中添加到拦截器中才会生效 public class LoginInterceptor implements HandlerInterceptor { @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { UserHolder.removerUser(); } @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { //1.获取session HttpSession session=request.getSession(); //2.获取session中的用户 Object user=session.getAttribute(\u0026#34;user\u0026#34;); //3.判断用户是否存在 if(user=null){ //4.不存在，拦截 response.setStatus(401); return false; } //5.存在，保存到ThreadLocal，并放行 UserHolder.saveUSer(user); return true; } } 添加拦截器到 Config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @Configuration public class MvcConfig implements WebMvcConfigurer { @Override public void addInterceptors(InterceptorRegistry registry) { // 登录拦截器 registry.addInterceptor(new LoginInterceptor()) .excludePathPatterns( \u0026#34;/shop/**\u0026#34;, \u0026#34;/voucher/**\u0026#34;, \u0026#34;/shop-type/**\u0026#34;, \u0026#34;/upload/**\u0026#34;, \u0026#34;/blog/hot\u0026#34;, \u0026#34;/user/code\u0026#34;, \u0026#34;/user/login\u0026#34; ); } } Controller 层实现 1 2 3 4 5 6 7 8 9 10 //登录校验状态 @GetMapping(\u0026#34;/me\u0026#34;) //个人主页 public Result me(){ // TODO 获取当前登录的用户并返回 //在拦截器LoginInterceptor中已经把用户保存到localthread UserDTO user = UserHolder.getUser(); return Result.ok(user); } 基于 Redis 接下来在上面代码的基础上修改 serviceImpl 和拦截器及 config\nServiceImpl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 //注入stringRedisTemplate @Resource private StringRedisTemplate stringRedisTemplate; @Override public Result login(LoginFormDTO loginForm, HttpSession session) { //1.校验手机号 String phone = loginForm.getPhone(); if(RegexUtils.isPhoneInvalid(phone)){ //2.不符合返回错误信息 return Result.fail(\u0026#34;手机号格式错误\u0026#34;); } //3.手机号校验成功，校验验证码，验证码从redis中获取 String code = stringRedisTemplate.opsForValue().get(\u0026#34;login:code:\u0026#34;+phone); String logincode = loginForm.getCode(); if (code==null||!code.toString().equals(logincode)){ //4.验证码错误 return Result.fail(\u0026#34;验证码错误\u0026#34;); } //5.验证码一致，查询是否存在用户 //query()是mp提供的，本类继承了extends ServiceImpl\u0026lt;UserMapper, User\u0026gt; User user = query().eq(\u0026#34;phone\u0026#34;, phone).one(); if (user==null){ //6.不存在用户，创建新用户 user=createUserWithPhone(phone); } //7.用户存在，生成用户的token作为登录凭证 String token = UUID.randomUUID().toString(true); //8.存入redis时用的hash存储，因此这里要把userDto转换成hashmap UserDTO userDTO = BeanUtil.copyProperties(user, UserDTO.class); Map\u0026lt;String, Object\u0026gt; userMap = BeanUtil.beanToMap(userDTO,new HashMap\u0026lt;\u0026gt;(), CopyOptions.create() .setIgnoreNullValue(true) .setFieldValueEditor((fieldName,fieldValue)-\u0026gt;fieldValue.toString())); //9.token存入redis并设置token有效期 stringRedisTemplate.opsForHash().putAll(\u0026#34;login:token:\u0026#34;+token,userMap); stringRedisTemplate.expire(\u0026#34;login:token:\u0026#34;,30,TimeUnit.MINUTES); return Result.ok(token); } @Override public Result sendCode(String phone, HttpSession session) { //1.校验手机号 if(RegexUtils.isPhoneInvalid(phone)){ //2.不符合返回错误信息 return Result.fail(\u0026#34;手机号格式错误\u0026#34;); } //2.符合生成验证码 String code = RandomUtil.randomNumbers(6); //3.保存到redis中,设置有效期1分钟 stringRedisTemplate.opsForValue().set(\u0026#34;login:code:\u0026#34;+phone,code,1L, TimeUnit.MINUTES); //4.发送验证码给客户 log.info(\u0026#34;验证码发送成功：{}\u0026#34;,code); //返回 return Result.ok(); } 新增刷新拦截器 用于刷新 Token 有效期，这里做了拦截器的优化。在第一个拦截器会获取 token，根据 token 去 redis 查询用户，如果查到就说明用户已经登陆过了，此时只需要刷新 token 有效期并保存到 threadlocal 然后放行。如果没有查到说明是未登录用户或者登录已经过期，那么 token 也是 null 的，也直接放行，所以第一个拦截器虽然拦截一切路径，但不论如何最后都会放行，其主要作用是对已登录用户刷新 token，在第二个拦截器才会对未登录用户进行限制，决定放不放行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 public class RefreshTokenInterceptor implements HandlerInterceptor { private StringRedisTemplate stringRedisTemplate; public RefreshTokenInterceptor(StringRedisTemplate stringRedisTemplate) { this.stringRedisTemplate = stringRedisTemplate; } @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { // 1.获取请求头中的token String token = request.getHeader(\u0026#34;authorization\u0026#34;); if (StrUtil.isBlank(token)) { return true; } // 2.基于TOKEN获取redis中的用户 String key = \u0026#34;login:token\u0026#34; + token; Map\u0026lt;Object, Object\u0026gt; userMap = stringRedisTemplate.opsForHash().entries(key); // 3.判断用户是否存在 if (userMap.isEmpty()) { return true; } // 5.将查询到的hash数据转为UserDTO UserDTO user = BeanUtil.fillBeanWithMap(userMap, new UserDTO(), false); // 6.存在，保存用户信息到 ThreadLocal UserHolder.saveUser(user); // 7.刷新token有效期 stringRedisTemplate.expire(key, 30L, TimeUnit.MINUTES); // 8.放行 return true; } @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { // 移除用户 UserHolder.removeUser(); } } 添加拦截器到 Config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @Configuration public class MvcConfig implements WebMvcConfigurer { @Resource private StringRedisTemplate stringRedisTemplate; @Override public void addInterceptors(InterceptorRegistry registry) { // 登录拦截器 registry.addInterceptor(new LoginInterceptor()) .excludePathPatterns( \u0026#34;/shop/**\u0026#34;, \u0026#34;/voucher/**\u0026#34;, \u0026#34;/shop-type/**\u0026#34;, \u0026#34;/upload/**\u0026#34;, \u0026#34;/blog/hot\u0026#34;, \u0026#34;/user/code\u0026#34;, \u0026#34;/user/login\u0026#34; ).order(1);//设置拦截器的优先级 // token刷新的拦截器，拦截所有请求 registry.addInterceptor(new RefreshTokenInterceptor(stringRedisTemplate)).addPathPatterns(\u0026#34;/**\u0026#34;).order(0); //后注册的拦截器先执行 } } ","date":"2024-05-15T20:01:48Z","permalink":"/zh-cn/post/2024/05/redis%E5%AE%9E%E6%88%98%E9%AA%8C%E8%AF%81%E7%A0%81%E7%99%BB%E5%BD%95%E6%B3%A8%E5%86%8C/","title":"Redis实战—验证码登录注册"},{"content":" VUE 参考官网：https://cli.vuejs.org/zh/guide/\nNVM 安装 我们开发过程中常常遇到 nodejs 版本不适配的问题，需要切换到不同版本的 nodejs，nodejs 卸载安装麻烦，这就需要用到 nvm 了。\nnvm 全名 node.js version management，顾名思义是一个 node.js 的版本管理工具。通过它可以安装和切换不同版本的 nodejs。\n卸载 node.js 为了确保彻底删除 node 在看看你的 node 安装目录中还有没有 node 文件夹，有的话一起删除。再看看 C:\\Users\\用户名 文件夹下有没有.npmrc以及.yarnrc等等统统删除。再去看看你的环境变量有没有 node 相关的，有的话也一起删除了。一定要卸载干净！\n安装 nvm Releases · coreybutler​​​​​​/nvm-windows · GitHub\n分别选择 nvm 的安装路径和 nodejs 的安装路径\n终端输入 nvm -v 查到版本号则安装成功！\n配置 环境变量在安装过程中会自动配置好\n在 nvm 的安装目录找到 settings.txt，配置下载源\nnode_mirror: https://npm.taobao.org/mirrors/node/ npm_mirror: https://npm.taobao.org/mirrors/npm/ 里面的 root 和 path 应该是安装的时候配好的，不要动。\n使用 nvm 安装 node.js nvm list available 查看可安装版本\n选择一个安装：npm install 版本号\n安装后 nvm list 查看已安装的版本和正在使用的版本\nnvm 常用命令 命令 说明 nvm version 查看当前的版本 nvm root \\[path\\] 设置和查看 root 路径 nvm install 版本号 安装指定版本的 node nvm list 查看已经安装的版本 nvm list available 查看网络可以安装的版本 nvm use 切换指定的 node 版本 nvm uninstall 卸载指定的版本 创建 VUE 项目 使用 vue init 创建 vue2（不推荐） 前提：安装了 vue-cli\n1 2 3 4 5 6 7 8 # 安装（最新版） npm install -g vue-cli # 安装（指定版本） npm install -g @vue/cli@4.5.14 #测试版本,显示版本号即安装成功 vue -V 创建项目\nvue init webpack 命令是 vue -cli2.x 版本的初始化方式，启动方式默认为 npm run dev ，webpack 为官方推荐模板。\n1 2 3 4 5 6 7 8 # 生成一个基于 webpack 模板的新项目 vue init webpack project_name #进入项目目录 cd project_name #启动项目 npm run serve 使用vue init构建项目的时候，会有如下几步提示信息：\n1 2 3 \u0026gt;? Project name vuedemo1`，设置项目名称，这一步直接回车，使用默认即可。 \u0026gt;? Project description A Vue.js project`，项目描述，我这也直接回车略过。 \u0026gt;? Author (zhangkai \u0026lt;xxxxx@163.com\u0026gt;)`，如果你的电脑上安装了 git，这里会默认提取你的 git 账户名作为作者，我同样回车使用默认。 使用 vue create 创建 vue2 和 3（较推荐） vue create 是 vue -cli3.x 版本的初始方式 ，启动方式默认为 npm run serve\n如果在执行 vue create 时，提示如下内容，那么就按照提示重新安装下高版本的 vue/cli 就行了。\n提示是否从https://registry.npmmirror.com这个仓库进行快速安装 ，选 yes 或 no\n使用 npm create vue 创建 vue2 和 3 1 2 npm create vue@2 project_name npm create vue@3 project_name 使用 npm create vite 创建（推荐） 基于 Vite 创建 vue 项目是目前 vue 官方比较推荐的创建方式。\nVite 需要 Node.js 版本 14.18+，16+，有些模板需要依赖更高的 Node 版本才能正常运行\n这种创建方式依赖 npm 版本，不同版本命令不太一样：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #查看npm版本 npm -v # npm 6.x npm create vite@latest vite-vue --template vue # npm 7+, 需要额外的加两个短横线: npm create vite@latest vite-vue -- --template vue # yarn yarn create vite vite-vue --template vue # pnpm pnpm create vite vite-vue --template vue 使用 vue ui 创建（推荐） 1 2 #vue的web端可视化创建 vue ui ","date":"2024-05-14T17:22:29Z","permalink":"/zh-cn/post/2024/05/nvm%E5%AE%89%E8%A3%85%E5%8F%8Avue%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE%E7%9A%84n%E7%A7%8D%E6%96%B9%E5%BC%8F/","title":"NVM安装及VUE创建项目的N种方式"},{"content":"集群配置 三台服务器：linux01、linux02、linux03\n每台服务器均安装了 zookeeper、kafka，服务器之间做了 ssh 免密登录（集群启停脚本用）\nkafka 虽然内置了 zk，但是这里用的是自己安装的 zk。\n服务器之间加了 ip 映射，如 hosts 文件所示，这样就不需要 p 地址，只需要服务器名字就可以了\n集群启动 注意事项\n启动时先启动 zk，再启动 kafka 关闭时先关闭 kafka，再关闭 zk，因为 kafka 需要 zk 来维护数据信息，再关闭前 kafka 要和 zk 通讯。 kafka-server-start.sh -daemon config/server.properties kafka-server-stop.sh 脚本启动 zk 集群 脚本启动 kafka 集群 启动成功 启动成功，三台服务器均显示如下：\n查看 zk 客户端，根节点下已经有了 kafka 节点\n默认直接在根节点下生成 admin、brokers、cluster 等节点，但是不方便维护，因此在 server.properties 文件中改了配置，让所有节点统一生成在 kafka 节点。\nzk 集群启停脚本\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #!/bin/bash #zookeeper集群启停及状态查看脚本 ZOOKEEPER=\u0026#34;/export/server/zookeeper\u0026#34; case $1 in \u0026#34;start\u0026#34;) for i in linux01 linux02 linux03 do echo ---------- zookeeper $i 启动 ------------ ssh $i \u0026#34;$ZOOKEEPER/bin/zkServer.sh start\u0026#34; done ;; \u0026#34;stop\u0026#34;) for i in linux01 linux02 linux03 do echo ---------- zookeeper $i 停止 ------------ ssh $i \u0026#34;$ZOOKEEPER/bin/zkServer.sh stop\u0026#34; done ;; \u0026#34;status\u0026#34;) for i in linux01 linux02 linux03 do echo ---------- zookeeper $i 状态 ------------ ssh $i \u0026#34;$ZOOKEEPER/bin/zkServer.sh status\u0026#34; done ;; esac kafka 集群启停脚本\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/bin/bash case $1 in \u0026#34;start\u0026#34;){ for i in linux01 linux02 linux03 do echo --------$i 启动kafka--------- ssh $i \u0026#34;source /etc/profile;/export/server/kafka/bin/kafka-server-start.sh -daemon /export/server/kafka/config/server.properties\u0026#34; done };; \u0026#34;stop\u0026#34;){ for i in linux01 linux02 linux03 do echo --------$i 停止kafka--------- ssh $i \u0026#34;source /etc/profile;/export/server/kafka/bin/kafka-server-stop.sh stop\u0026#34; done };; esac Kafka 操作 \u0026ndash;bootstrap-server 是连接 kafka，对于集群而言，连接任何一台服务器的 kafka 都是一样的\n命令行创建 Topic 消费者生产者联动 先启动生产者，生产 hello、hahaha，再启动消费者，生产者再生产 aaaaa、bbbb。此时 hello、hahaha 属于历史消息，不会显示，只显示 aaaaa、bbbb，若想显示历史消息，需要如下，此时消息是乱序的： Linux 配置 EFAK3.0.1 1. 配置 EFAK 的环境变量\nke.sh 文件中引用的 efak 变量名是 KE_HOME，所以环境变量名一定是 KE_HOME，否则 efak 无法启动\nsource /etc/profile\n2. 修改 kafka 的 bin/kafka-server-start.sh 的内存配置，如果不修改，可能无法启动 efak\n内容如下：\n1 2 3 4 5 6 if [ \u0026#34;x$KAFKA_HEAP_OPTS\u0026#34; = \u0026#34;x\u0026#34; ]; then #export KAFKA_HEAP_OPTS=\u0026#34;-Xmx1G -Xms1G\u0026#34; export KAFKA_HEAP_OPTS=\u0026#34;-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70\u0026#34; #监控kafka运行的端口号9999 export JMX_PORT=\u0026#34;9999\u0026#34; fi 注意！修改 kafka 配置文件后记得重新分发给集群其他的 kafka！\nscp kafka-server-start.sh root@linux02:/export/server/kafka/bin\nscp kafka-server-start.sh root@linux03:/export/server/kafka/bin\n3. 修改 EFAK 的 conf/system-config.properties 文件，关键内容如下\nEFAK 需要配置 mysql 的ke 数据库来存储元数据，username 是连接 mysql 的登录用户，名字随便起，和 linux03 服务器无关，需要提前在 mysql 创建好并授权访问。\n我的mysql5.7 在 linux01 服务器，而 EFAK 在 linux03 服务器，这就需要跨服务器连接，解决方法如下。\n4. 在 linux01 的 mysql 创建名为 linux03 的用户，并授予对 ke 数据库的所有权，并规定只有服务器 linux03 的 ip 地址才能访问。\nCREATE USER \u0026rsquo;linux03\u0026rsquo;@\u0026lsquo;xxx.xxx.x.xxx\u0026rsquo; IDENTIFIED BY \u0026lsquo;#252012\u0026rsquo;;\nGRANT ALL PRIVILEGES ON ke.* TO \u0026rsquo;linux03\u0026rsquo;@\u0026lsquo;xxx.xxx.x.xxx\u0026rsquo;;\n注：这里的 xxx.xxx.x.xxx 是部署了 EFAK 的服务器 ip，也就是服务器 linux03 的 ip\n如果创建用户失败，提示了创建的用户密码安全级别过低，那么可以降低密码安全级别\nSET GLOBAL validate_password.policy = LOW; flush privileges;\n大概意思就是允许 ip 为 xxx.xxx.x.xxx 的 linux03 用户访问数据库\n5. 启动并登录 EFAK\n依次启动 zk、kafka\nzk 和 kafka 集群启动脚本是我自己编写的\nzk-All.sh start\nAll-kafka start\n然后 ke.sh start\n启动成功！\n账户是 admin，密码 123456\n\u0026gt; 访问成功，可以看到 3 台 broker 成功运行\nKraft 模式集群 在 Kafka 2.8.0 版本，移除了对 Zookeeper 的依赖，通过 Kraft 模式的 controller管理集群，使用 Kafka 内部的 Quorum 控制器来取代 ZooKeeper 管理元数据，元数据保存在 controller中，这样我们无需维护 zk 集群，只要维护 Kafka 集群就可以了，节省运算资源。\n优点\nkafka 不再依赖外部框架，能独立运行 controller 管理集群时不需要和 zk 通讯，集群性能提升 脱离了 zk 依赖，集群扩展不受 zk 读写能力的限制 controller 不再动态选举，而由配置文件决定，这样可以针对性的加强 controller 的节点配置，而不是像以前一样对随机 controller 节点的高负载束手无策。 配置 不在原来的 kafka 集群操作，这里换新的 kafka 集群\n编辑 kafka 的 config/kraft 目录下的 server.properties 文件\nlinux01 服务器配置如下：\n配好后分发该配置文件，并在各个服务器修改对应的参数，如 node.id、advertised.listeners、log.dirs\n启动前初始化集群 在 linux01 生成存储目录唯一 ID\n用该 ID 格式化所有服务器的 kafka 存储目录\n启动 kraft 集群，这里使用自定义脚本，把脚本配置到环境变量，效果更佳\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #!/bin/bash case $1 in \u0026#34;start\u0026#34;){ for i in linux01 linux02 linux03 do echo --------$i 启动kraft--------- ssh $i \u0026#34;source /etc/profile;/export/server/kraft/bin/kafka-server-start.sh -daemon /export/server/kraft/config/kraft/server.properties\u0026#34; done };; \u0026#34;stop\u0026#34;){ for i in linux01 linux02 linux03 do echo --------$i 停止kraft--------- ssh $i \u0026#34;source /etc/profile;/export/server/kraft/bin/kafka-server-stop.sh stop\u0026#34; done };; \u0026#34;status\u0026#34;){ for i in linux01 linux02 linux03 do echo --------$i 查看kraft状态--------- ssh $i \u0026#34;source /etc/profile;jps -ml\u0026#34; done };; esac 查看是否启动成功，这里也使用脚本一键查看集群所有 jps，把脚本配置到环境变量，效果更佳\n1 2 3 4 5 6 #!/bin/bash for i in linux01 linux02 linux03 do echo --------$i 查看jps--------- ssh $i \u0026#34;source /etc/profile; jps\u0026#34; done 无需 zk，启动成功！\n浅浅把玩 Kraft 创建主题 first\n在 linux01 创建生产者，在 linux03 创建消费者\nFlume 联动 kafka Flume 作为生产者 案例玩法：依次启动 zk、kafka 集群，在 linux01 编辑 flume 的 file_to_kafka.conf 任务配置，监控 app.log 文件内容，把监控的内容发送到 kafka 的 first 主题，然后启动 flume 任务作为生产者。在 linux02 启动 kafka 消费者，消费 first 主题，检查 linux01 的监控文件 app.log 变化时，消费者是否消费到了消息。\nflume 的 job/group3/file_to_kafka.conf 配置:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #定义source，sink，channel a3.sources = r3 a3.sinks = k3 a3.channels = c3 # 配置source # a3.sources.r3.type = TAILDIR a3.sources.r3.filegroups=f3 #监控的目录 a3.sources.r3.filegroups.f3=/export/server/flume/job/group3/applog/app.* #断点续传的json a3.sources.r3.positionFile=/export/server/flume/job/group3/tail_dir2.json # 配置 sink a3.sinks.k3.type = org.apache.flume.sink.kafka.KafkaSink a3.sinks.k3.kafka.bootstrap.servers=linux01:9092,linux02:9092,linux03:9092 a3.sinks.k3.kafka.topic=first a3.sinks.k3.kafka.flumeBatchSize=20 a3.sinks.k3.kafka.producer.acks=1 a3.sinks.k3.kafka.producer.linger.ms=1 # 配置channel a3.channels.c3.type = memory a3.channels.c3.capacity = 1000 a3.channels.c3.transactionCapacity = 100 # Bind the source and sink to the channel a3.sources.r3.channels = c3 a3.sinks.k3.channel = c3 确保 zk，kafka 集群已启动\n在 linux02 创建 kafka 消费者，消费 first 主题\nkafka-console-consumer.sh \u0026ndash;bootstrap-server linux02:9092 \u0026ndash;topic first 在 linux01 启动 flume 任务\nbin/flume-ng agent -c conf/ -n a3 -f job/group3/file_to_kafka.conf 对监控的文件 app.log 追加内容，kafka 消费者成功接收到消息\n\u0026gt; Flume 作为消费者 在 linux01 的 flume 的 job 目录下编辑 kafka_to_file.conf 文件，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # Name the components on this agent a2.sources = r2 a2.sinks = k2 a2.channels = c2 # Describe/configure the source a2.sources.r2.type = org.apache.flume.source.kafka.KafkaSource a2.sources.r2.batchSize=50 a2.sources.r2.batchDurationMillis=200 a2.sources.r2.kafka.bootstrap.servers=linux02:9092 a2.sources.r2.kafka.topics=first a2.sources.r2.kafka.consumer.group.id=custom.g.id # Describe the sink a2.sinks.k2.type = logger # Use a channel which buffers events in memory a2.channels.c2.type = memory a2.channels.c2.capacity = 1000 a2.channels.c2.transactionCapacity = 100 # Bind the source and sink to the channel a2.sources.r2.channels = c2 a2.sinks.k2.channel = c2 依次启动 zk、kafka 集群，然后启动 flume 任务和 kakfa 生产者\nflume-ng agent -n a2 -c conf/ -f job/kafka_to_file.conf -Dflume.root.logger=INFO,console kafka-console-producer.sh \u0026ndash;bootstrap-server linux03:9092 \u0026ndash;topic first 案例成功！\nSpringBoot 联动 kakfa SpringBoot 作为生产者 创建 springboot 工程\napplication.properties 文件内容如下：\n1 2 3 4 5 6 7 server.port=8080 #连接kafka集群 spring.kafka.bootstrap-servers=linux01:9092,linux02:9092,linux03:9092 #key-value序列化 spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer pom 文件内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;org.mykafka\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;springboot_kafka\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;name\u0026gt;springboot_kafka\u0026lt;/name\u0026gt; \u0026lt;description\u0026gt;springboot_kafka\u0026lt;/description\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; \u0026lt;project.reporting.outputEncoding\u0026gt;UTF-8\u0026lt;/project.reporting.outputEncoding\u0026gt; \u0026lt;spring-boot.version\u0026gt;2.6.13\u0026lt;/spring-boot.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.kafka\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-kafka\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.kafka\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-kafka-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring-boot.version}\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.8.1\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;source\u0026gt;1.8\u0026lt;/source\u0026gt; \u0026lt;target\u0026gt;1.8\u0026lt;/target\u0026gt; \u0026lt;encoding\u0026gt;UTF-8\u0026lt;/encoding\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring-boot.version}\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;mainClass\u0026gt;org.mykafka.springboot_kafka.SpringbootKafkaApplication\u0026lt;/mainClass\u0026gt; \u0026lt;skip\u0026gt;true\u0026lt;/skip\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;repackage\u0026lt;/id\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;repackage\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; ProducerController 代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 package org.mykafka.springboot_kafka; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.kafka.core.KafkaTemplate; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; /** * @Author:懒大王Smile * @Date: 2024/5/12 * @Time: 22:42 * @Description: */ @RestController public class ProducerController { @Autowired KafkaTemplate\u0026lt;String,String\u0026gt; kafka; @RequestMapping(\u0026#34;/ProducerSend\u0026#34;) public String date(String msg){ kafka.send(\u0026#34;first\u0026#34;,msg); System.out.println(msg); return \u0026#34;ok\u0026#34;; } } 项目展示\n项目启动之后在浏览器访问，kafka 消费者成功接收到数据！\nSpringBoot 作为消费者 application.properties 文件内容如下：\n1 2 3 4 5 6 7 8 server.port=8080 #key-value反序列化 spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer #消费者组id spring.kafka.consumer.group-id=mykafka ConsumerController 代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package org.mykafka.springboot_kafka; import org.springframework.context.annotation.Configuration; import org.springframework.kafka.annotation.KafkaListener; /** * @Author:懒大王Smile * @Date: 2024/5/13 * @Time: 23:12 * @Description: */ @Configuration public class ConsumerController { @KafkaListener(topics = \u0026#34;first\u0026#34;) public void consumerTopic(String msg){ System.out.println(\u0026#34;收到消息：\u0026#34;+msg); } } Spark 联动 kafka\n尚硅谷 p73\n","date":"2024-05-05T20:57:35Z","permalink":"/zh-cn/post/2024/05/kafka%E5%AE%9E%E6%88%98-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA-kraft%E6%A8%A1%E5%BC%8F/","title":"kafka实战 集群搭建-Kraft模式"},{"content":"Kafka 是一个由 Scala 和 Java 语言开发的，经典高吞吐量的分布式消息发布和订阅系统，也是大数据技术领域中用作数据交换的核心组件之一。它具有以下特点：\n支持消息的发布和订阅，类似于 RabbtMQ、ActiveMQ 等消息队列； 支持数据实时处理； 能保证消息的可靠性投递； 支持消息的持久化存储，并通过多副本分布式的存储方案来保证消息的容错； 高吞吐率，单 Broker 可以轻松处理数千个分区以及每秒百万级的消息量； 消息队列（MQ） Kafka 软件最初的设计就是专门用于数据传输的消息系统，类似功能的软件有 RabbitMQ、ActiveMQ、RocketMQ 等，这些软件的核心功能是传输数据，而 Java 中如果想要实现数据传输功能，那么这个软件一般需要遵循 Java 消息服务技术规范 JMS。前面提到的 ActiveMQ 软件就完全遵循了 JMS 技术规范，而 RabbitMQ 是遵循了类似 JMS 规范并兼容 JMS 规范的跨平台的 AMQP 规范。除了上面描述的 JMS，AMQP 外，还有一种用于物联网小型设备之间传输消息的 MQTT 通讯协议。\nKafka 拥有作为一个消息系统应该具备的功能，但是却有着独特的设计。Kafka 借鉴了 JMS 规范的思想，但是却并没有完全遵循 JMS 规范。这也恰恰是软件名称为 Kafka，而不是 KafkaMQ 的原因。\n消息队列一般应用场景 **应用耦合：**多应用间通过消息队列对同一消息进行处理，避免调用接口失败导致整个过程失败。 **异步处理：**多应用对消息队列中同一消息进行处理，应用间并发处理消息，相比串行处理，减少处理时间 限流削峰： 广泛应用于秒杀或抢购活动中，避免流量过大导致应用系统挂掉的情况。该方法有如下优点： 1.请求先入消息队列，而不是由业务处理系统直接处理，做了一次缓冲,极 大地减少了业务处理系统的压力； 2.队列长度可以做限制，事实上，秒杀时，后入队列的用户无法秒杀到商品，这些请求可以直接被抛弃，返回活动已结束或商品已售完信息； 消息驱动的系统： 系统分为消息队列、消息生产者、消息消费者，生产者 负责产生消息，消费者(可能有多个)负责对消息进行处理。具体场景：用户新上传了一批照片，人脸识别系统需要对这个用户的所有照片进行聚类，聚类完成后由对账系统重新生成用户的人脸索引(加快查询)。这三个子 系统间由消息队列连接起来，前一个阶段的处理结果放入队列中，后一个阶段从 队列中获取消息继续处理。 该方法有如下优点：1.避免了直接调用下一个系统导致当前系统失败； 2.每个子系统对于消息的处理方式可以更为灵活，可以选择收到消息时就处理，可以选择定时处理，也可以划分时间段按不同处理速度处理； JMS JMS 类似于 JDBC，是 java 平台的消息中间件通用规范，定义了系统和系统之间传输消息的接口。\n为了实现系统和系统之间的数据传输，JMS 规范中定义很多用于通信的组件： JMS Producer **：**JMS 消息生产者。所谓的生产者，就是生产数据的客户端应用程序，这些应用通过 JMS 接口发送 JMS 消息。 JMS Provider：JMS 消息提供者。其实就是实现 JMS 接口和规范的消息中间件，也就是我们提供消息服务的软件系统，比如 RabbitMQ、ActiveMQ、Kafka。 JMS Message：JMS 消息。这里的消息指的就是数据。一般采用 Java 数据模型进行封装，其中包含消息头，消息属性和消息主体内容。 JMS Consumer：JMS 消息消费者。所谓的消费者，就是从消息提供者中获取数据的客户端应用程序，这些应用通过 JMS 接口接收 JMS 消息。 JMS 模型 点对点模型（peer to peer） 特点：\n每个消息只有一个接收者（Consumer）(即一旦被消费，就会被删除)； 发送者和接发收者间没有依赖性，发送者发送消息之后，不管有没有接收者在运行，都不会影响到发送者下次发送消息； 接收者在成功接收消息之后需向队列应答成功，以便消息队列删除当前接 收的消息 发布订阅模型 特点：\n每个消息可以有多个订阅者，但是订阅者必须来自不同的消费者组； 针对某个主题（Topic）的订阅者，它必须创建一个订阅者之后，才能消费发布者的消息。 为了消费消息，订阅者需要提前订阅该角色主题，并保持在线运行； Kafka 采用就是这种模型。\nKafka 架构 在 Kafka 2.8.0 版本，移除了对 Zookeeper 的依赖，通过Kraft 模式 进行自己的集群管理，使用 Kafka内部的 Quorum 控制器来取代 ZooKeeper 管理元数据，这样我们无需维护 zk 集群，只要维护 Kafka 集群就可以了，节省运算资源。\nkafka 基本数据单元被称为 message(消息)，为减少网络开销，提高效率，多个消息会被放入同一批次(Batch) 中后再写入。\nBroker kafka 集群中包含多个服务实例（节点），这种服务实例被称为 broker（一个 broker 就是一个节点/一个服务器），每个 broker 都有一个唯一标识 broker.id，用于标识自己在集群中的身份，可以在配置文件 server.properties 中进行配置，或由程序自动生成。 Broker 接收来自生产者的消息，为消息设置偏移量，并提交消息到磁盘保存。Broker 为消费者提供服务，对读取分区的请求做出响应，返回已经提交到磁盘的消息。 Controller 选举 每一个集群都会选举出一个 Broker 作为集群控制器 **(Controller)，它负责分区 Leader 选举，还负责管理主题分区及其副本的状态、元数据管理。**如果在运行过程中，Controller 节点出现了故障，那么 Kafka 会依托于 ZooKeeper 软件选举其他的节点作为新的 Controller，让 Kafka 集群实现高可用。\n特殊情况\nController 节点并没有宕掉，而是因为网络的抖动，不稳定，导致和 ZooKeeper 之间的会话超时，那么此时，整个 Kafka 集群就会认为之前的 Controller 已经下线（退出）从而选举出新的 Controller，而之前的 Controller 的网络又恢复了，以为自己还是 Controller 了，继续管理整个集群，那么此时，整个 Kafka 集群就有两个 controller 进行管理，那么其他的 broker 就懵了，不知道听谁的了，这种情况，我们称之为脑裂现象，为了解决这个问题，Kafka 通过一个任期（epoch:纪元）的概念来解决，也就是说，每一个 Broker 当选 Controller 时，会告诉当前 Broker 是第几任 Controller，一旦重新选举时，这个任期会自动增 1，那么不同任期的 Controller 的 epoch 值是不同的，那么旧的 controller 一旦发现集群中有新任 controller 的时候，那么它就会完成退出操作（清空缓存，中断和 broker 的连接，并重新加载最新的缓存），让自己重新变成一个普通的 Broker。\nBroker 上下线 Controller 在初始化时，会利用 ZK 的 watch 机制注册很多不同类型的监听器，当监听的事件被触发时，Controller 就会触发相应的操作。Controller 在初始化时，会注册多种类型的监听器，主要有以下几种：\n/kafka/admin/reassign_partitions 节点，用于分区副本迁移的监听 /kafka/isr_change_notification 节点，用于 Partition ISR 变动的监听 /kafka/admin/preferred_replica_election 节点，用于需要进行 Partition 最优 leader 选举的监听 /kafka/brokers/topics 节点，用于 Topic 新建的监听 /kafka/brokers/topics/TOPIC_NAME 节点，用于 Topic Partition 扩容的监听 /kafka/admin/delete_topics 节点，用于 Topic 删除的监听 /kafka/brokers/ids 节点，用于 Broker 上下线的监听，记录有哪些 kafka 服务器在线。 /kafka/controller 节点，辅助选举 leader 每台 Broker 在上线时，都会与 ZK 建立一个建立一个 session，并在 /brokers/ids 下注册一个节点，节点名字就是 broker id，这个节点是临时节点，该节点内部会有这个 Broker 的详细节点信息。Controller 会监听/brokers/ids 这个路径下的所有子节点，如果有新的节点出现，那么就代表有新的 Broker 上线，如果有节点消失，就代表有 broker 下线，Controller 会进行相应的处理，Kafka 就是利用 ZK 的这种 watch 机制及临时节点的特性来完成集群 Broker 的上下线。无论 Controller 监听到的哪一种节点的变化，都会进行相应的处理，同步整个集群元数据。\nBroker 工作流程 Producer 一般情况下，生产者在把消息均衡地分布到在主题的所有分区上，而并不关心消息会被写到哪个分区。如果我们想要把消息写到指定的分区，可以通过自定义分区器来实现。\nConsumer 消费者一定是归属于某个消费组中的，消费者可以订阅一或多个主题，并按照分区中消息的顺序来读取。消费者通过检查消息的偏移量 (offset) 来区分读取过的消息。偏移量是一个不 断递增的数值，在创建消息时，Kafka 会把它添加到其中，在给定的分区里，每个消息的偏移量都是唯一的。消费者把每个分区最后读取的偏移量保存在 Zookeeper 或 Kafka 上，如果消费者关闭或者重启，它还可以重新获取该偏移量，以保证读取状态不会丢失。\nConsumer Group 消费者组由一个或者多个消费者组成，同一个组中的消费者对于同一条消息只消费一次。\n每个消费者组都有一个 ID，即 group ID。组内的所有消费者协调在一起来消费 一个订阅主题的所有分区。当然，每个分区只能由同一个消费组内的一个消费者来消费，但可以由不同的消费组来消费。partition 数量决定了每个 consumer group 中并发消费者的最大数。\n因此要合理设置消费者组中的消费者数量，避免出现消费者闲置。\nTopic Kafka 的消息通过 Topics(主题) 进行分类，Kafka 中有两个固定的，用于记录消费者偏移量和事务处理的主题，一个主题可以被分为若干个 Partitions(分区)，一个分区就是 一个提交日志 (commit log)。消息以追加的方式写入分区，然后以先入先出的顺序读取。Kafka 通过分区来实现数据的冗余和伸缩性，分区可以分布在不同的服务器上，这意味着一个 Topic 可以横跨多个服务器，以提供比单个服务器更强大的性能。\n由于一个 Topic 包含多个分区，因此无法在整个 Topic 范围内保证消息的顺序性，但可以保证消息在单个分区内的顺序性。 \u0026gt;\nPartition 分区 Kafka 消息传输采用发布、订阅模式，所以消息生产者必须将数据发送到一个主题，假如发送给这个主题的数据非常多，那么主题所在 broker 节点的负载和吞吐量就会受到极大的考验，甚至有可能因为热点问题引起 broker 节点故障，导致服务不可用解决方案就是分区。\ntopic 是逻辑上的概念，而 partition 是物理上的概念，每个 topic 包含一个或者多个 partition，每个分区保存部分 topic 的数据，所有的 partition 当中的数据全部合并起来， 就是一个 topic 当中的所有的数据。一个 broker 服务下，有多个 Topic，每个 Topic 可以创建多个分区，broker 数与分区数没有关系； 在 kafka 中，每一个分区会有一个编号，编号从 0 开始。\n**单个分区的消息是有序的，而全局的 topic 的多个分区的消息****是无序的。这就是为什么一条消息只能被同一个消费者组里面的一个消费者消费，这样就某种程度上保证了消息的不重复消费和乱序消费。**\n分区好处 合理使用存储资源：海量资源按照分区切割成一块块存储在多台 broker，合理控制分区任务，实现负载均衡。 提高并行度：生产者以分区为单位发送数据，消费者以分区为单位消费数据。 生产者发送消息的分区策略 默认分区器 DefaultPartitioner\n自定义分区\n自己创建类实现 Partitioner 接口，重写 partition 方法 1 package com.atguigu.test; import org.apache.kafka.clients.producer.Partitioner; import org.apache.kafka.common.Cluster;\nimport java.util.Map;\n/**\nTODO 自定义分区器实现步骤：\n1. 实现Partitioner接口 2. 重写方法 partition : 返回分区编号，从0开始 close configure _/ public class KafkaPartitionerMock implements Partitioner { /** _ 分区算法 - 根据业务自行定义即可 _ @param topic The topic name _ @param key The key to partition on (or null if no key) _ @param keyBytes The serialized key to partition on( or null if no key) _ @param value The value to partition on or null _ @param valueBytes The serialized value to partition on or null _ @param cluster The current cluster metadata _ @return 分区编号，从 0 开始 _/ @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) { return 0; }\n@Override public void close() { } @Override public void configure(Map\u0026lt;String, ?\u0026gt; configs) { } }\n``` 配置分区器 1 package com.atguigu.test; import org.apache.kafka.clients.producer.*; import org.apache.kafka.common.serialization.StringSerializer;\nimport java.util.HashMap; import java.util.Map; import java.util.concurrent.Future;\npublic class ProducerPartitionTest { public static void main(String[] args) { Map\u0026lt;String, Object\u0026gt; configMap = new HashMap\u0026lt;\u0026gt;(); configMap.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \u0026ldquo;localhost:9092\u0026rdquo;); configMap.put( ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); configMap.put( ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); configMap.put( ProducerConfig.PARTITIONER_CLASS_CONFIG, KafkaPartitionerMock.class.getName());\nKafkaProducer\u0026lt;String, String\u0026gt; producer = null; try { producer = new KafkaProducer\u0026lt;\u0026gt;(configMap); for ( int i = 0; i \u0026lt; 1; i++ ) { ProducerRecord\u0026lt;String, String\u0026gt; record = new ProducerRecord\u0026lt;String, String\u0026gt;(\u0026quot;test\u0026quot;, \u0026quot;key\u0026quot; + i, \u0026quot;value\u0026quot; + i); final Future\u0026lt;RecordMetadata\u0026gt; send = producer.send(record, new Callback() { public void onCompletion(RecordMetadata recordMetadata, Exception e) { if ( e != null ) { e.printStackTrace(); } else { System.out.println(\u0026quot;数据发送成功：\u0026quot; + record.key() + \u0026quot;,\u0026quot; + record.value()); } } }); } } catch ( Exception e ) { e.printStackTrace(); } finally { if ( producer != null ) { producer.close(); } } } }\n``` 文件存储 Segment 每个 partition 对应一个 log 文件，该log 文件存储的就是生产的数据，生产的数据不断追加到 log 文件，为了防止 log 文件过大导致数据定位效率低下，kafka 采取了分片和索引，把每个 log 文件分成多个 segment 文件段，每个 segment 包括 index 文件、log 文件、timeindex 文件等等，统一放在一个文件夹下，文件夹命名规则是：topic 名+分区序号\nkafka 消费完数据之后不会立刻删除，而是有专门的清理机制，默认保存数据 7 天，7 天后删除，而 timeindex 文件就记录了数据的时间\nindex 是稀疏索引\n偏移量是一个 64 位的长整形数，固定是 20 位数字，长度未达到，用 0 进行填补，索引文件和日志文件都由该作为文件名命名规则：\n00000000000000000000.index：索引文件，记录偏移量映射到 .log 文件的字节偏移量，此映射用于从任何特定偏移量读取记录 0000000000000000000.timeindex：时间戳索引文件，此文件包含时间戳到记录偏移量的映射，该映射使用.index 文件在内部映射到记录的字节偏移量。这有助于从特定时间戳访问记录 00000000000000000000.log：此文件包含实际记录，并将记录保持到特定偏移量,文件名描述了添加到此文件的起始偏移量，如果日志文件名为 00000000000000000004.log ，则当前日志文件的第一条数据偏移量就是 4（偏移量从 0 开始） 分区的副本 在创建主题时，Kafka 会首先决定如何在 broker 间分配分区副本，它遵循以下原则：\n在所有 broker 上尽可能均匀地分配分区副本，负载均衡； 确保分区的每个副本分布在不同的 broker 上； 如果使用了 broker.rack 参数为 broker 指定了机架信息，那么会尽可能的把每个分区的副本分配到不同机架的 broker 上，以避免一个机架不可用而导致整个分区不可用。 分区数可以\u0026gt;bro**kers**数，但是副本因子必须\u0026lt;=可用 broker 数，这样才能保证每个副本分布在不同的 broker 上，进而保证数据的完整性。 Why 分区副本 为了保证高可用（提高负载均衡和系统伸缩性），kafka 的分区是多副本的，如果一个副本丢失了，还可以从其他 borker 的副本中获取分区数据。但这要求对应副本数据必须是完整的，这是 Kafka 数据一致性的基础，所以才需要使用 controller broker 来进行专门的管理。\n注意！follower 副本会周期性地同步 leader 副本的数据，同步数据的过程是有一定延迟的，所以副本之间的数据可能是不同的。\nKafka 的单个主题被分为多个分区，每个分区可以有多个副本 (可以在创建主题时使用 replication-factor 参数进行指定)。其中一个副本是 Leader 副本，所有的读写请求都直接发送给 Leader 副本；其他副本是 Follower 副本，分布在不同的 broker 上，需要通过复制来保持与 Leader 副本数据一致，当 Leader 副本不可用时，会从 ISR 中选一个 Follower 副本成为新 Leader。Leader 选举依赖 Controller。\n手动调整分区副本存储 kafka 默认均匀分布在所有 broker 上 手动调整副本存储，需要创建存储计划 json 文件，然后在创建主题分区及副本的时候，指定存储计划文件。\n副本 Leader 分区自动平衡 但是自动平衡会消耗大量资源，影响性能，不建议频繁触发自动平衡。\n增加副本数量 增加副本因子不能通过命令行直接增加，需要创建副本存储计划 json 文件并执行副本计划。\n副本 Leader 选举 副本 Leader 故障恢复 leader 故障后会从 ISR 踢出，从 follower 产生新的 leader，此时可能出现其他的 follower 数据比新的 leader 多，那么多的数据就会截掉，保证和 leader 数据一致。但是只能保证数据一致，不能保证不丢失。\n副本 Follower 故障恢复 follower2 故障，被踢出 ISR follower2 恢复之后截掉 HW 之后的数据\nfollower 从 HW 位置开始向 Leader 同步数据，等 follower 的 LEO\u0026gt;=该分区的 HW，也就是追上 Leader，就可以重新加入 ISR。\nISR 机制 每个分区都有一个 ISR(in-sync Replica) 列表，用于维护所有同步的、可用的副本。Leader 副本必然是同步副本，而对于 Follower 副本来说，它需要满足以下条件才能被认为是同步副本：\n与 Zookeeper 之间有一个活跃的会话，即必须定时向 Zookeeper 发送心跳； 在规定的时间（replica.lag.time.max.ms，默认 30s）内从 Leader 副本那里低延迟地获取过消息。 如果副本不满足上面条件的话，就会被从 ISR 列表中移除，直到满足条件才会被再次加入。\nOSR\n不在 ISR 中的副本就在 OSR，OSR 和 ISR 统称 AR（assigned Repllicas）\n不完全首领选举 对于副本机制，在 broker 级别有一个可选的配置参数 unclean.leader.election.enable ，默认值 为 fasle，代表禁止不完全的首领选举。这是针对当 Leader 副本挂掉且 ISR 中没有其他可用副本时，是否允许某个不完全同步的副本成为 Leader 副本，这可能会导致数据丢失或者数据不一致，在某些对数据一致 性要求较高的场景 (如金融领域)，这可能无法容忍的，所以其默认值为 false，如果你能够允许部分数据 不一致的话，可以配置为 true。\n最少同步副本 ISR 机制的另外一个相关参数是 min.insync.replicas , 可以在 broker 或者主题级别进行配置，代表 ISR 列表中至少要有几个可用副本。这里假设设置为 2，那么当可用副本数量小于该值时，就认为整个分区处于不可用状态。此时客户端再向分区写入数据时候就会抛出异常 org.apache.kafka.common.errors.NotEnoughReplicasExceptoin: Messages are rejected since there are fewer in-sync replicas than required。\n副本数（replication-factor）：消息保存在几个 broker（服务器）上， 一般情况下副本数等于 broker 的个数。 follower 通过拉的方式从 leader 同步数据。 消费者和生产者都是从 leader 读写数据，不与 follower 交互。如果 follower 副本也对外提供服务那会怎么样呢？首先，性能是肯定会有所提升的。但同时，会出现一系列问题。类似数据库事务中的幻读，脏读。 副本因子的作用：让 kafka 读取数据和写入数据时的可靠性。 副本因子是包含本身，相同的副本因子不能放在同一个 broker。 如果某一个分区有三个副本因子，就算其中一个挂掉，那么只会剩下的两个中， 选择一个 leader。 如果所有的副本都挂了，生产者如果生产数据到指定分区的话，将写入不成功。 1sr 表示：当前可用的副本。 数据请求 请求机制 在所有副本中，只有 leader 副本才能进行消息的读写处理。由于不同分区的leader副本可能在不同的 broker 上，如果某个 broker 收到了一个分区请求，但是该分区的领导副本并不在该 broker 上，那么 它就会向客户端返回一个 Not a Leader for Partition 的错误响应。 为了解决这个问题，Kafka 提供了元数据请求机制。\n首先集群中的每个 broker 都会缓存所有主题的分区副本信息，客户端会定期发送元数据请求，然后将获取的元数据进行缓存。定时刷新元数据的时间间隔可以通过为客户端配置 metadata.max.age.ms 来进行指定。有了元数据信息后，客户端就知道了领导副本所在的 broker，之后直接将读写请求发送给对应的 broker 即可。 如果在定时请求的时间间隔内发生的分区副本的选举，则意味着原来缓存的信息可能已经过时了，此时 还有可能会收到 Not a Leader for Partition 的错误响应，这种情况下客户端会再次求发出元数据请求，然后刷新本地缓存，之后再去正确的 broker 上执行对应的操作。 需要注意的是，并不是所有保存在分区首领上的数据都可以被客户端读取到，为了保证数据一致性，只有被所有同步副本 (ISR 中所有副本) 都保存了的数据才能被客户端读取到。 Kafka 所有数据的写入和读取都是通过零拷贝来实现的 生产者详解 生产者发送消息的过程 Kafka 的main 线程会将发送消息包装为 ProducerRecord 对象， ProducerRecord 对象包含了目标主题和要发 送的内容，同时还可以指定键和分区。在发送 ProducerRecord 对象前，生产者会先把键和值对象序列化成字节数组，这样它们才能够在网络上传输。 接下来，数据被传给分区器。如果之前已经在 ProducerRecord 对象里指定了分区，那么分区器 就不会再做任何事情。如果没有指定分区 ，那么分区器会根据 ProducerRecord 对象的键来选择一个分区。 紧接着，这条记录被添加到一个记录批次里，该批次数据积累到一定值（batch.size）后，这个批次里的所有消息会被sender 线程发送到相同的主题和分区上。如果在规定时间（linger.ms）内该批次没有达到规定的 batch.size，sender 线程同样会把数据发送。linger.ms 单位是 ms，默认 0ms 服务器在收到这些消息时会返回一个响应。如果消息成功写入 Kafka，就返回一个 RecordMetaData 对象，它包含了主题和分区信息，以及记录在分区里的偏移量。如果写入失败，则会返回一个错误。sender 线程会重新执行写入请求，如果达到指定的重试次数后还没有成功，则直接抛出异常，不再重试。写入成功后，请求会从 sender 线程中删除。（后面消息可靠性会详解） \u0026gt; 消息可靠性 对于生产者发送的数据，我们有的时候是不关心数据是否已经发送成功的，我们只要发送就可以了。在这种场景中，消息可能会因为某些故障或问题导致丢失，我们将这种情况称之为消息不可靠。虽然消息数据可能会丢失，但是在某些需要高吞吐，低可靠的系统场景中，这种方式也是可以接受的，甚至是必须的。\n但是在更多的场景中，需要确定数据是否发送成功且 Kafka 是否接收到数据，也就是要保证数据不丢失，这就是所谓的消息可靠性保证。\n而这个确定的过程一般是通过 Kafka 给我们返回的响应确认结果（Acknowledgement）来决定的，这里的响应确认结果简称为 ACK 应答。\n根据场景，Kafka 提供了 3 种应答处理，可以通过配置对象进行配置。\nACK 应答 ACK=0\n当生产数据时，生产者将数据通过网络客户端将数据发送到网络数据流中的时候，Kafka 就对当前的数据请求进行了响应（确认应答），如果是同步发送数据，此时就可以发送下一条数据了。如果是异步发送数据，回调方法就会被触发。但这其实并不能保证 Kafka 能正确地接收到数据，消息不可靠，但是通信效率高，消息吞吐量也高。\nACK=1\n当生产数据时，Leader 副本将数据接收到并写入到了日志文件后，就会对当前的数据请求进行响应（确认应答），如果是同步发送数据，此时就可以发送下一条数据了。如果是异步发送数据，回调方法就会被触发。这种方式消息可靠性较高，但是此时只有 Leader 节点存储了数据，还没有备份到 follower 副本，那么一旦当前存储数据的 broker 节点出现了故障，数据也依然会丢失。\nACK=-1（默认）\n当生产数据时，Leader 副本和 Follower 副本都已经将数据接收到并写入到了日志文件后，再对当前的数据请求进行响应（确认应答），如果是同步发送数据，此时就可以发送下一条数据了。如果是异步发送数据，回调方法就会被触发。这种是消息最可靠的，但是吞吐量有所下降。注意！这里同步的是 ISR 中的 follower 副本，只要 ISR 中的所有副本接收到了数据就会响应。\n数据重试 由于网络或服务节点的故障，Kafka 在传输数据时，可能会导致数据丢失，所以我们才会设置 ACK 应答机制，尽可能提高数据的可靠性。但其实在某些场景中，数据的丢失并不是真正地丢失，而是“虚假丢失”，比如 ACK 应答设置为 1，也就是说一旦 Leader 副本将数据写入文件后，Kafka 就可以对请求进行响应了。\n此时，如果由于网络故障的原因，Kafka 并没有成功将 ACK 应答信息发送给 Producer，那么此时对于 Producer 来讲，以为 kafka 没有收到数据，所以就会一直等待响应，一旦超过某个时间阈值，就会发生超时错误，Producer 就会认为数据已经丢了。\n此时，Producer 会尝试对超时的请求数据进行**重试(retry)操作，**将数据再次发送给 Kafka，就可能出现数据重复。\n数据乱序 数据重试(retry)功能除了可能会导致数据重复以外，还可能会导致数据乱序。假设需要将编号为 1，2，3 的三条连续数据发送给 Kafka。每条数据会对应于一个连接请求，此时，如果第一个数据的请求出现了故障，而第二个数据和第三个数据的请求正常，那么 Broker 就收到了第二个数据和第三个数据，并进行了应答。\n为了保证数据的可靠性，Producer 会将第一条数据重新放回到缓冲区的第一个。进行重试操作，如果重试成功，Broker 就会收到第一条数据，数据的顺序已经被打乱了。\n如上图，在 1.x 版本之前，为了保证数据有序性，每个 broker 只能缓存一个请求，在 1.x 之后，开启幂等性，可以缓存多个请求，请求会在 kafka 服务端重新排序。\n同步发送 异步发送 异步发送是生产者和 RecordAccumulator 的异步，上面的同步发送，是生产者把每批次数据发送给 RecordAccumulator，该批次满足要求后由 sender 线程拉取发送到 kafka 集群，然后才是下一批次，按批次一批批发送给 kafka 集群，而异步则是不管上一批有没有发送到 kafka 集群，下一批直接由生产者发送到 RecordAccumulator。\n生产者提高吞吐量 压缩算法 配置参数 compression.type，默认为 none，支持 snappy、gzip、lz4、zstd 压缩算法\n消费者详解 消费者通常是消费者群组的一部分，多个消费者群组共同读取同一个主题时，彼此之间互不影响。Kafka 之所以要引入消费者群组这个概念是因为 Kafka 消费者经常会做一些高延迟的操作，比如把数据写到数据库或 HDFS ，或者进行耗时的计算，在这些情况下，单个消费者无法跟上数据生成的速度。此时可以增加更多的消费者，让它们分担负载，分别处理部分分区的消息，这就是 Kafka 实现横向伸缩的主要手段。\npush\u0026amp;pull 如果数据由 Kafka 进行推送（push），那么多个分区的数据同时推送给消费者进行处理，明显一个消费者的消费能力是有限的，那么消费者无法快速处理数据，就会导致数据的积压，从而导致网络，存储等资源造成极大的压力，影响吞吐量和数据传输效率。 如果 kafka 的分区数据在内部可以存储的时间更长一些，再由消费者根据自己的消费能力向 kafka 申请（拉取）数据，那么整个数据处理的通道就会更顺畅一些。Kafka 的 Consumer 就采用的这种拉取数据的方式。 消费者组调度器 消费者想要拉取数据，首先必须要加入到一个组中，成为消费组中的一员，同样道理，如果消费者出现了问题，也应该从消费者组中剥离。而这种加入组和退出组的处理，都应该由专门的管理组件进行处理，这个组件在 kafka 中，我们称之为消费者组调度器（Group Coordinator）\nGroup Coordinator 是 Broker 上的一个组件，用于管理和调度消费者组的成员、状态、分区分配、偏移量等信息。每个 Broker 都有一个 Group Coordinator 对象，负责管理多个消费者组，但每个消费者组只有一个 Group Coordinator\n消费者分配分区策略 同一个消费者组的消费者都订阅同一个主题，所以消费者组中的多个消费者可以共同消费一个主题中的所有数据。 为了避免数据被重复消费，一个分区的数据只能被同组中的一个消费者消费，但是反过来，一个消费者是可以消费多个分区数据的。 消费者组中的消费者数量最好不要超出主题分区数量，就会导致多出的消费者是无法消费数据的，造成了资源的浪费。 消费者 Leader 消费者中的每个消费者到底消费哪一个主题分区，这个分配策略其实是由消费者的 Leader决定的，这个 Leader 称之为群主。群主是多个消费者中，第一个加入组中的消费者，其他消费者称之为 Follower，称呼上有点类似与分区副本的 Leader 和 Follower。\n当消费者加入群组的时候，会发送一个 JoinGroup 请求。群主负责给每一个消费者分配分区。每个消费者只知道自己的分配信息，只有群主知道群组内所有消费者的分配信息。\n指定分配策略的基本流程：\n第一个消费者设定 group.id 为 test，向当前负载最小的节点发送请求查找消费调度器 找到消费调度器后，消费者向调度器节点发出 JOIN_GROUP 请求，加入消费者组。 当前消费者当选为群主后，根据消费者配置中分配策略设计分区分配方案，并将分配好的方案告知调度器 此时第二个消费者申请加入消费者组 加入成功后，kafka 将消费者组状态切换到准备 rebalance，关闭和消费者的所有链接，等待它们重新加入。客户端重新申请加入，kafka 从消费者组中挑选一个作为 leader，其它的作为 follower。 Leader 会按照分配策略对分区进行重分配，并将方案发送给调度器，由调度器通知所有的成员新的分配方案。组成员会按照新的方案重新消费数据 分区再均衡 因为群组里的消费者共同读取主题的分区，所以当一个消费者被关闭或发生崩溃时，它就离开了群组， 原本由它读取的分区将由群组里的其他消费者来读取。同时在主题发生变化时 ， 比如添加了新的分区，也会发生分区与消费者的重新分配，分区的所有权从一个消费者转移到另一个消费者，这样的行为被称为再均衡。正是因为再均衡，所以消费费者群组才能保证高可用性和伸缩性。 消费者通过向群组协调器所在的 broker 发送心跳来维持它们和群组的从属关系以及它们对分区的所有权。只要消费者以正常的时间间隔发送心跳，就被认为是活跃的，说明它还在读取分区里的消息。消费 者会在轮询消息或提交偏移量时发送心跳。如果消费者停止发送心跳的时间足够长，会话就会过期，群组协调器认为它已经死亡，就会触发再均衡。 监听分区再均衡 因为分区再均衡会导致分区与消费者的重新划分，有时候希望在再均衡前执行一些操作：比如提交已经处理但是尚未提交的偏移量，关闭数据库连接等。此时可以在订阅主题时候，调用 subscribe 的重载方法传入自定义的分区再均衡监听器。\n偏移量 Offset Kafka 的每一条消息都有一个偏移量属性，记录了其在分区中的位置，偏移量是一个单调递增的整数。 消费者通过往一个叫作 ＿consumer_offset 的特殊主题发送消息，消息里包含每个分区的偏移量。 如果消费者一直处于运行状态，那么偏移量就没有什么用处。不过，如果有消费者退出或者新分区加入，此时就会触发再均衡。完成再均衡之后，每个消费者可能分配到新的分区，而不是之前处理的那个。为了能够继续之前的工作，消费者需要读取每个分 区最后一次提交的偏移量，然后从偏移量指定的地方继续处理。 因为这个原因，所以如果不能正确提交偏移量，就可能会导致数据丢失或者重复出现消费。\nLSO 起始偏移量（Log Start Offset），每个分区副本都有起始偏移量，用于表示副本数据的起始偏移位置，初始值为 0。\nLSO 一般情况下无需更新，但是如果数据过期，或用户手动删除数据时，Leader 的 Log Start Offset 可能发生变化，Follower 副本的日志需要和 Leader 保持严格的一致，因此，如果 Leader 的该值发生变化，Follower 自然也要发生变化保持一致。\nLEO 日志末端位移（Log End Offset），表示下一条待写入消息的 offset，每个分区副本都会记录自己的 LEO。对于 Follower 副本而言，它能读取到 Leader 副本 LEO 值以下的所有消息。\nHW 高水位值（High Watermark），定义了消息可见性（对于消费者而言），标识了一个特定的消息偏移量，消费者只能读取到水位线以下的的数据。同时这个偏移量还可以帮助 Kafka 完成副本数据同步操作。\n这就是所谓的木桶理论：木桶中容纳水的高度，只能是水桶中最短的那块木板的高度。这里将整个分区看成一个木桶，其中的数据看成水，而每一个副本就是木桶上的一块木板，那么这个分区（木桶）可以被消费者消费的数据（容纳的水）其实就是数据最少的那个副本的最后数据位置（木板高度）。\nHW 高水位线会随着 follower 的数据同步操作，而不断上涨，也就是说，follower 同步的数据越多，那么水位线也就越高，那么消费者能访问的数据也就越多。\n（详见上面的 follower 故障）\n手动提交偏移量 用户可以通过将 enable.auto.commit 设为 false ，然后手动提交偏移量。基于用户需求手动提交偏移量可以分为两大类：\n手动提交当前偏移量：即手动提交当前轮询的最大偏移量；\n手动提交固定偏移量：即按照业务需求，提交某一个固定的偏移量。\n按照 Kafka API，手动提交偏移量又可以分为同步提交和异步提交。\n同步提交 通过调用 consumer.commitSync() 来进行同步提交，不传递任何参数时提交的是当前轮询的最大偏 移量。\n1 2 3 4 5 6 7 8 9 while (true) { ConsumerRecords\u0026lt;String, String\u0026gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord\u0026lt;String, String\u0026gt; record : records) { System.out.println(record); } /*同步提交*/ consumer.commitSync(); } 如果某个提交失败，同步提交还会进行重试，这可以保证数据能够最大限度提交成功，但是同时也会降 低程序的吞吐量。基于这个原因，Kafka 还提供了异步提交的 API。\n异步提交 异步提交可以提高程序的吞吐量，因为此时你可以尽管请求数据，而不用等待 Broker 的响应。代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 while (true) { ConsumerRecords\u0026lt;String, String\u0026gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord\u0026lt;String, String\u0026gt; record : records) { System.out.println(record); } /*异步提交并定义回调*/ consumer.commitAsync(new OffsetCommitCallback() { @Override public void onComplete(Map\u0026lt;TopicPartition, OffsetAndMetadata\u0026gt; offsets, Exception exception) { if (exception != null) { System.out.println(\u0026#34;错误处理\u0026#34;); offsets.forEach((x, y) -\u0026gt; System.out.printf(\u0026#34;topic = %s,partition = %d, offset = %s \\n\u0026#34;, x.topic(), x.partition(), y.offset())); } } }); } 异步提交存在的问题是，在提交失败的时候不会进行自动重试，实际上也不能进行自动重试。假设程序 同时提交了 200 和 300 的偏移量，此时 200 的偏移量失败的，但是紧随其后的 300 的偏移量成功了， 此时如果重试就会存在 200 覆盖 300 偏移量的可能。同步提交就不存在这个问题，因为在同步提交的 情况下，300 的提交请求必须等待服务器返回 200 提交请求的成功反馈后才会发出。基于这个原因，某 些情况下，需要同时组合同步和异步两种提交方式。 虽然程序不能在失败时候进行自动重试，但是我们是可以手动进行重试的，你可以通过一个 Map offsets 来维护你提交的每个分区的偏移量，然后当失败时候，你 可以判断失败的偏移量是否小于你维护的同主题同分区的最后提交的偏移量，如果小于则代表你 已经提交了更大的偏移量请求，此时不需要重试，否则就可以进行手动重试。 自动提交偏移量 将消费者的 enable.auto.commit 属性配置为 true 即可完成自动提交的配置。 此时每隔固定 的时间，消费者就会把 poll() 方法接收到的最大偏移量进行提交，提交间隔由 auto.commit.interval.ms 属性进行配置，默认值是 5s。\n使用自动提交是存在隐患的，假设我们使用默认的 5s 提交时间间隔，在最近一次提交之后的 3s 发生了 再均衡，再均衡之后，消费者从最后一次提交的偏移量位置开始读取消息。这个时候偏移量已经落后了 3s ，所以在这 3s 内到达的消息会被重复处理。可以通过修改提交时间间隔来更频繁地提交偏移量，减 小可能出现重复消息的时间窗，不过这种情况是无法完全避免的。基于这个原因，Kafka 也提供了手动提交偏移量的 API，使得用户可以更为灵活的提交偏移量。\n截至 尚硅谷 kafka3.x P39 ","date":"2024-05-05T17:42:50Z","permalink":"/zh-cn/post/2024/05/kafka%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F%E4%B8%87%E5%AD%97%E8%AF%A6%E8%A7%A3%E5%9B%BE%E6%96%87%E5%B9%B6%E8%8C%82/","title":"Kafka入门到入土——万字详解，图文并茂"},{"content":"事务（Put、Take） put 事务把数据批处理写入临时缓冲区 putList，，然后 doCommit 去检查 Channel 内存队列是否足够合并，如果不够，就回滚数据，如果够，就把 putList 的数据写入到 Channel，然后由 take 事务从 channel 中拉取，写入到临时缓冲区 takeList，然后把数据从 takeList 发送到 HDFS，发送完毕后清空缓冲区，如果某个数据发送失败，就回滚到 channel。\n架构原理 在拦截阶段可以进行数据过滤清洗，洗掉脏数据。\nChannelSelector 因为一个 source 可以对应对各 channel ，ChannelSelector 的作用就是选出 Event 将要被发往哪个 Channel。其共有两种类型， 分别是 Replicating（复制）和 Multiplexing（多路复用）。 ReplicatingSelector 会将同一个 Event 发往所有的 Channel，Multiplexing 会根据自定义的配置，将不同的 Event 发往不同的 Channel，Multiplexing 要结合拦截器使用，Multiplexing 会根据数据的头信息来决定发送到哪个 channel。\nSinkProcessor 一个 sink 只能绑定一个 channel，一个 channel 能绑定多个 sink。SinkProcessor 共 有 三 种 类 型 ， 分 别 是 DefaultSinkProcessor 、LoadBalancingSinkProcessor 和 FailoverSinkProcessor。\nDefaultSinkProcessor 对 应 的 是 单个的 Sink ， LoadBalancingSinkProcessor 和 FailoverSinkProcessor 对应的是 Sink Group,LoadBalancingSinkProcessor 可以实现负载均衡的功能，FailoverSinkProcessor 可以错误恢复的功能。\nLoadBalancingSinkProcessor 负载均衡：\n一个 channel 会发给多个 sink\nFailoverSinkProcessor 故障转移：\n当一个 sink 故障，任务会转移到其他 sink\n拓扑结构 简单串联 复制和多路复用 负载均衡和故障转移 聚合 单源多出口案例 前置条件： linux01 上启动 hive，hdfs，在 linux03 上部署 3 个 flume 任务，启动 hdfs。linux01 和 linux03 配置 ssh 免密登录。\n要求： flume1 在 linux03 监听 linux01 的 hive 日志，把 hive 日志的新内容发送给 linux03 上的 flume2 和 flume3，flume2 把内容写到 hdfs，flume3 把内容写到 linux03 的本地文件/export/server/flume/job/group1/datas 文件夹中。\n剧透： flume3 成功把 hive 日志的新内容写到 datas 文件夹，说明 linux03 确实监听到了 linux01 的 hive 日志并且成功把日志从 linux01 弄到了 linux03，但是 flume2 却没有把新内容写到 hdfs，猜想的可能是因为在 linux03 上写 flume2 的配置文件**sinks.k1.hdfs.path = hdfs://linux01:9820/flume/group1/%Y%m%d/%H，**linux01 和 linux03 是不同的服务器，跨服务器没写进去，所以建议在同一台服务器搞。\n在 flume/job 目录中新建文件夹 group1 来存放本次案例的任务配置文件\n1 2 3 4 5 mkdir group1 cd group1 vim flume-file-flume.conf vim flume-flume-hdfs.conf vim flume-flume-dir.conf 三个 conf 配置如下：\nflume-file-flume.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # Name the components on this agent a1.sources = r1 a1.sinks = k1 k2 a1.channels = c1 c2 # 将数据流复制给所有 channel a1.sources.r1.selector.type = replicating # Describe/configure the source a1.sources.r1.type = exec a1.sources.r1.command = ssh root@linux01 \u0026#39;tail -F /export/server/hive/logs/hive.log\u0026#39; #因为hive在linux01，flume在linux03，为了跨服务器监听，这里用了ssh免密登录 a1.sources.r1.shell = /bin/bash -c # Describe the sink # sink 端的 avro 是一个数据发送者 a1.sinks.k1.type = avro a1.sinks.k1.hostname = linux03 a1.sinks.k1.port = 4141 a1.sinks.k2.type = avro a1.sinks.k2.hostname = linux03 a1.sinks.k2.port = 4142 # Describe the channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 a1.channels.c2.type = memory a1.channels.c2.capacity = 1000 a1.channels.c2.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 c2 a1.sinks.k1.channel = c1 a1.sinks.k2.channel = c2 flume-flume-hdfs.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #Name the components on this agent a2.sources = r1 a2.sinks = k1 a2.channels = c1 #Describe/configure the source #source 端的 avro 是一个数据接收服务 a2.sources.r1.type = avro a2.sources.r1.bind = linux03 a2.sources.r1.port = 4141 #Describe the sink a2.sinks.k1.type = hdfs a2.sinks.k1.hdfs.path = hdfs://linux01:9820/flume/group1/%Y%m%d/%H #这里在 linux03 把路径配置为 linux01 的 hdfs，可能就是出错原因 #上传文件的前缀 a2.sinks.k1.hdfs.filePrefix = group1- #是否按照时间滚动文件夹 a2.sinks.k1.hdfs.round = true #多少时间单位创建一个新的文件夹 a2.sinks.k1.hdfs.roundValue = 1 #重新定义时间单位 a2.sinks.k1.hdfs.roundUnit = hour #是否使用本地时间戳 a2.sinks.k1.hdfs.useLocalTimeStamp = true #积攒多少个 Event 才 flush 到 HDFS 一次 a2.sinks.k1.hdfs.batchSize = 100 #设置文件类型，可支持压缩 a2.sinks.k1.hdfs.fileType = DataStream #多久生成一个新的文件 a2.sinks.k1.hdfs.rollInterval = 30 #设置每个文件的滚动大小大概是 128M a2.sinks.k1.hdfs.rollSize = 134217700 #文件的滚动与 Event 数量无关 a2.sinks.k1.hdfs.rollCount = 0 #Describe the channel a2.channels.c1.type = memory a2.channels.c1.capacity = 1000 a2.channels.c1.transactionCapacity = 100 #Bind the source and sink to the channel a2.sources.r1.channels = c1 a2.sinks.k1.channel = c1 flume-flume-dir.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #Name the components on this agent a3.sources = r1 a3.sinks = k1 a3.channels = c2 # Describe/configure the source a3.sources.r1.type = avro a3.sources.r1.bind = linux03 a3.sources.r1.port = 4142 # Describe the sink a3.sinks.k1.type = file_roll a3.sinks.k1.sink.directory = /export/server/flume/job/group1/datas # Describe the channel a3.channels.c2.type = memory a3.channels.c2.capacity = 1000 a3.channels.c2.transactionCapacity = 100 # Bind the source and sink to the channel a3.sources.r1.channels = c2 a3.sinks.k1.channel = c2 3 个配置文件写好后，在 linux03 启动 hdfs，然后在 flume 文件夹下启动三个 flume 任务\nflume-ng agent -c conf/ -n a3 -f job/group1/flume-flume-dir.conf\nflume-ng agent -c conf/ -n a2 -f job/group1/flume-flume-hdfs.conf\nflume-ng agent -c conf/ -n a1 -f job/group1/flume-file-flume.conf\n在 linux01 启动 hdfs ，然后启动 hivemetastore 和 hive，开始操作 hive，就会产生 hive 日志记录在 hive.log。\n注意！hive.log 没产生新内容可能是因为 hive 日志配置出错，去 conf 文件夹找 hive-log4j2.properties，找到 hive.log.dir。修改成自己的 logs 路径，默认路径可能要用到 hive 环境变量，如果环境变量没有就直接写绝对路径。 datas 确实产生了新文件，但是有很多空的，不知道咋回事，可能是任务配置问题。 故障转移案例 前置：确保 4142、4141、44444 端口没被占用 在 linux03 的 flume/job 目录建 group2 文件夹，里面有如下 3 个配置文件：\nflume-flume-console1.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Name the components on this agent a2.sources = r1 a2.sinks = k1 a2.channels = c1 # Describe/configure the source a2.sources.r1.type = avro a2.sources.r1.bind = linux03 a2.sources.r1.port = 4141 # Describe the sink a2.sinks.k1.type = logger # Describe the channel a2.channels.c1.type = memory a2.channels.c1.capacity = 1000 a2.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a2.sources.r1.channels = c1 a2.sinks.k1.channel = c1 flume-flume-console2.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 #Name the components on this agent a3.sources = r1 a3.sinks = k1 a3.channels = c2 # Describe/configure the source a3.sources.r1.type = avro a3.sources.r1.bind = linux03 a3.sources.r1.port = 4142 # Describe the sink a3.sinks.k1.type = logger # Describe the channel a3.channels.c2.type = memory a3.channels.c2.capacity = 1000 a3.channels.c2.transactionCapacity = 100 # Bind the source and sink to the channel a3.sources.r1.channels = c2 a3.sinks.k1.channel = c2 flume-netcat-flume.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # Name the components on this agent a1.sources = r1 a1.channels = c1 a1.sinkgroups = g1 a1.sinks = k1 k2 # Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 44444 a1.sinkgroups.g1.processor.type = failover a1.sinkgroups.g1.processor.priority.k1 = 5 a1.sinkgroups.g1.processor.priority.k2 = 10 #配置优先级，k1=5，优先级更高，因此数据会优先发给k1,当k1故障时，才会转移到k2 a1.sinkgroups.g1.processor.maxpenalty = 10000 # Describe the sink a1.sinks.k1.type = avro a1.sinks.k1.hostname = linux03 a1.sinks.k1.port = 4141 a1.sinks.k2.type = avro a1.sinks.k2.hostname = linux03 a1.sinks.k2.port = 4142 # Describe the channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinkgroups.g1.sinks = k1 k2 a1.sinks.k1.channel = c1 a1.sinks.k2.channel = c1 分别启动 3 个任务\n1）flume-ng agent -c conf/ -n a3 -f job/group2/flume-flume-console2.conf -Dflume.root.logger=INFO,console\n2）flume-ng agent -c conf/ -n a2 -f job/group2/flume-flume-console1.conf -Dflume.root.logger=INFO,console\n3）flume-ng agent -c conf/ -n a1 -f job/group2/flume-netcat-flume.conf 运行 nc localhost 44444 并发送内容: 在 console2 接收到（发送的汉字显示\u0026hellip;\u0026hellip;） 找到 flume 进程，制造故障杀死 console2 任务，此时发生故障 ，任务会转移到 console1： 可以看到 console2 被杀死 继续发送数据，数据被 console1 接收 console1 接收成功 。 负载均衡案例 尚硅谷 P25\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Name the components on this agent a1.sources = r1 a1.channels = c1 a1.sinkgroups = g1 a1.sinks = k1 k2 # Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 44444 a1.sinkgroups.g1.processor.type = load_balance # Describe the sink a1.sinks.k1.type = avro a1.sinks.k1.hostname = linux03 a1.sinks.k1.port = 4141 a1.sinks.k2.type = avro a1.sinks.k2.hostname = linux03 a1.sinks.k2.port = 4142 # Describe the channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinkgroups.g1.sinks = k1 k2 a1.sinks.k1.channel = c1 a1.sinks.k2.channel = c1 配置文件和故障转移案例一样，只有 flume-netcat-flume.conf 需要改。\nFlume 聚合案例 三台服务器：linux01、linux02、linux03，监控 linux03 的 job/group.log 文件和 linux02 的 44444 端口，把监测到的数据传给 linux01，在 linux01 的控制台输出。\nlinux03 的任务配置文件：flume3-logger-flume.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = exec a1.sources.r1.command = tail -F /export/server/flume/job/group.log a1.sources.r1.shell = /bin/bash -c # Describe the sink a1.sinks.k1.type = avro a1.sinks.k1.hostname = linux01 a1.sinks.k1.port = 4141 # Describe the channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 linux02 的任务配置文件：flume2-netcat-flume.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Name the components on this agent a2.sources = r1 a2.sinks = k1 a2.channels = c1 # Describe/configure the source a2.sources.r1.type = netcat a2.sources.r1.bind = linux02 a2.sources.r1.port = 44444 # Describe the sink a2.sinks.k1.type = avro a2.sinks.k1.hostname = linux01 a2.sinks.k1.port = 4141 # Use a channel which buffers events in memory a2.channels.c1.type = memory a2.channels.c1.capacity = 1000 a2.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a2.sources.r1.channels = c1 a2.sinks.k1.channel = c1 linux01 的任务配置文件：flume1-flume-logger.conf\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Name the components on this agent a3.sources = r1 a3.sinks = k1 a3.channels = c1 # Describe/configure the source a3.sources.r1.type = avro a3.sources.r1.bind = linux01 a3.sources.r1.port = 4141 # Describe the sink # Describe the sink a3.sinks.k1.type = logger # Describe the channel a3.channels.c1.type = memory a3.channels.c1.capacity = 1000 a3.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a3.sources.r1.channels = c1 a3.sinks.k1.channel = c1 自定义拦截器 案例需求\n使用 Flume 采集服务器本地日志，需要按照日志类型的不同，将不同种类的日志发往不 同的分析系统。\n需求分析\n实际的开发中，一台服务器产生的日志类型可能有很多种，不同类型的日志可能需要 发送到不同的分析系统。此时会用到 Flume 拓扑结构中的 Multiplexing 结构，Multiplexing 的原理是，根据 event 中 Header 的某个 key 的值，将不同的 event 发送到不同的 Channel，所以我们需要自定义一个 Interceptor，为不同类型的 event 的 Header 中的 key 赋予不同的值。 在该案例中，我们以端口数据模拟日志，以是否包含”atguigu”模拟不同类型的日志， 我们需要自定义 interceptor 区分数据中是否包含”atguigu”，将其分别发往不同的分析 系统（Channel）。\n自定义拦截器打成 jar 包 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 import org.apache.flume.Context; import org.apache.flume.Event; import org.apache.flume.interceptor.Interceptor; import java.util.ArrayList; import java.util.List; import java.util.Map; /** * @Author:懒大王Smile * @Date: 2024/4/28 * @Time: 19:54 * @Description: */ public class TypeInterceptor implements Interceptor { //声明一个存放事件的集合 private List\u0026lt;Event\u0026gt; addHeaderEvents; @Override public void initialize() { //初始化存放事件的集合 addHeaderEvents = new ArrayList\u0026lt;\u0026gt;(); } //单个事件拦截 @Override public Event intercept(Event event) { //1.获取事件中的头信息 Map\u0026lt;String, String\u0026gt; headers = event.getHeaders(); //2.获取事件中的 body 信息 String body = new String(event.getBody()); //3.根据 body 中是否有\u0026#34;atguigu\u0026#34;来决定添加怎样的头信息 if (body.contains(\u0026#34;sereins\u0026#34;)) { //4.添加头信息 headers.put(\u0026#34;type\u0026#34;, \u0026#34;sereins\u0026#34;); } else { //4.添加头信息 headers.put(\u0026#34;type\u0026#34;, \u0026#34;other\u0026#34;); } return event; } //批量事件拦截 @Override public List\u0026lt;Event\u0026gt; intercept(List\u0026lt;Event\u0026gt; events) { //1.清空集合 addHeaderEvents.clear(); //2.遍历 events for (Event event : events) { //3.给每一个事件添加头信息 addHeaderEvents.add(intercept(event)); } //4.返回结果 return addHeaderEvents; } @Override public void close() { } //静态内部类 public static class Builder implements Interceptor.Builder { @Override public Interceptor build() { return new TypeInterceptor(); } @Override public void configure(Context context) { } } } 所需依赖：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.smile\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;interceptor\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;maven.compiler.source\u0026gt;8\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;8\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flume\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flume-ng-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.9.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/project\u0026gt; jar 包打好放到 flume 的 lib 目录下，flume 启动时会加载 lib 的所有 jar 包。\n注意！！自定义拦截器的 jar 包源代码是定制的，里面的过滤拦截规则需要根据实际业务来编写，并且 jdk 最好是 1.8。\n任务配置文件 inux01 服务器的 flume 配置文件 job/group4/interceptor-flume1.conf：\n1 配置 1 个 netcat source，1 个 sink group（2 个 avro sink）， 并配置相应的 ChannelSelector 和 interceptor。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # Name the components on this agent a1.sources = r1 a1.sinks = k1 k2 a1.channels = c1 c2 # Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 44444 a1.sources.r1.interceptors = i1 a1.sources.r1.interceptors.i1.type = TypeInterceptor$Builder a1.sources.r1.selector.type = multiplexing a1.sources.r1.selector.header = type a1.sources.r1.selector.mapping.sereins = c1 a1.sources.r1.selector.mapping.other = c2 # Describe the sink a1.sinks.k1.type = avro a1.sinks.k1.hostname = linux02 a1.sinks.k1.port = 4141 a1.sinks.k2.type=avro a1.sinks.k2.hostname = linux03 a1.sinks.k2.port = 4242 # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Use a channel which buffers events in memory a1.channels.c2.type = memory a1.channels.c2.capacity = 1000 a1.channels.c2.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 c2 a1.sinks.k1.channel = c1 a1.sinks.k2.channel = c2 linux02 服务器的 flume 配置文件 job/group4/interceptor-flume2.conf：\n配置一个 avro source 和一个 logger sink\n1 2 3 4 5 6 7 8 9 10 11 12 a1.sources = r1 a1.sinks = k1 a1.channels = c1 a1.sources.r1.type = avro a1.sources.r1.bind = linux02 a1.sources.r1.port = 4141 a1.sinks.k1.type = logger a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 a1.sinks.k1.channel = c1 a1.sources.r1.channels = c1 linux03 服务器的 flume 配置文件 job/group4/interceptor-flume3.conf：\n配置一个 avro source 和一个 logger sink\n1 2 3 4 5 6 7 8 9 10 11 12 a1.sources = r1 a1.sinks = k1 a1.channels = c1 a1.sources.r1.type = avro a1.sources.r1.bind = linux03 a1.sources.r1.port = 4242 a1.sinks.k1.type = logger a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 a1.sinks.k1.channel = c1 a1.sources.r1.channels = c1 以上配置完后，在 linux01 nc localhost 44444,然而\n尚硅谷 Flume P33 后面的以后再搞\n自定义 Source 自定义 Sink 事务源码 ","date":"2024-04-28T21:47:38Z","permalink":"/zh-cn/post/2024/04/flume%E8%BF%9B%E9%98%B6--%E4%B8%87%E5%AD%97%E8%AF%A6%E8%A7%A3%E8%80%81%E5%A4%A7%E7%88%B7%E4%B9%9F%E8%83%BD%E5%AD%A6%E4%BC%9A/","title":"Flume进阶--万字详解【老大爷也能学会】"},{"content":"概述 Flume 是 Cloudera 提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传 输的系统。Flume 基于流式架构，灵活简单。\n基础架构 Flume 运行的核心是 Agent。Flume 是以 agent 为最小的独立运行单位。一个 agent 就是一个 JVM。它是 一个完整的数据收集工具，含有三个核心组件，分别是 source、 channel、 sink。通过这些组件， Event 可以从一个地方流向另一个地方。如下图所示：\nAgent Agent 是一个 JVM 进程，它以事件的形式将数据从源头送至目的。 Agent 主要有 3 个部分组成，Source、Channel、Sink。同一台服务器可以运行多个 Agent，每个 Agent 可以有多个 source、sink、channel。Agent 的名字可以相同但是不能同时启动任务，否则会出现冲突。 Source Source 是负责接收数据到 Flume Agent 并传给 Channel 的组件。 Source 组件可以处理各种类型、各种格式的日志数据，包括 avro、thrift、exec、jms、spooling directory、netcat、taildir、 sequence generator、syslog、http、legacy 这些不同的数据源。 Sink Sink 不断地轮询 Channel 中的事件且批量地移除它们，并将这些事件批量写入到存储系统或索引系统、或者被发送到另一个 Flume Agent。 Sink 组件目的地包括 hdfs、logger、avro、thrift、ipc、file、HBase、solr、自定义。 Channel Channel 是位于 Source 和 Sink 之间的缓冲区。因此，Channel 允许 Source 和 Sink 运作在不同的速率上。 Channel 是线程安全的，可以同时处理几个 Source 的写入操作和几个 Sink 的读取操作。 Flume 自带两种 Channel：Memory Channel 和 File Channel。 Memory Channel 是内存中的队列。Memory Channel 在不需要关心数据丢失的情景下适 用。如果需要关心数据丢失，那么 Memory Channel 就不应该使用，因为程序死亡、机器宕 机或者重启都会导致数据丢失。 File Channel 将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数 据。 selector 选择器，作用于 source 端，然后决定数据发往哪个目标。\ninterceptor 拦截器，flume 允许使用拦截器拦截数据。允许使用拦截器链，作用于 source 和 sink 阶段。\nEvent 传输单元，Flume 数据传输的基本单元，以 Event 的形式将数据从源头送至目的地。 Event 由 Header 和 Body 两部分组成，Header 用来存放该 event 的一些属性，为 K-V 结构， Body 用来存放该条数据，形式为字节数组。 安装部署 解压\ntar -zxvf /export/server/apache-flume-1.9.0-bin.tar.gz /export/server/\n为了让 flume1.9 兼容 hadoop3.x，要删除 flume lib 包下的 guava-11.0.2.jar\nrm guava-11.0.2.jar\nNetcat 安装\nsudo yum install -y nc\n简单案例\nFlume 入门案例 netcat 本机端口监控 在 flume 文件夹下创建工作目录 job\nmkdir job\n在 job 目录下建立任务配置文件，文件名任取，建议见名知意，net 表示数据源是端口，logger 表示数据是日志文件\nvim net-flume-logger.conf\n配置文件内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Name the components on this agent a1.sources = r1 #a1是该agent名，不可重复 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 4444 # Describe the sink a1.sinks.k1.type = logger # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 #最多接收1000个event a1.channels.c1.transactionCapacity = 100 #100个事务，一次最多发送100个event，事务失败会回滚。capacity应该 \u0026lt; transactionCapacity # Bind the source and sink to the channel a1.sources.r1.channels = c1 #一个source可以绑定多个channel a1.sinks.k1.channel = c1 #一个sink只能绑定一个channel 启动两个终端，一个终端启动监听任务：\n1 2 3 4 5 6 7 8 在 flume 目录下运行： flume-ng agent -c conf/ -n a1 -f job/flume-netcat-logger.conf -D flume.root.logger=INFO,console 参数说明： --conf/-c：表示配置文件存储在 conf/目录 --name/-n：表示给 agent 起名为 a1 --conf-file/-f：flume 本次启动读取的配置文件是在 job 文件夹下的 flume-telnet.conf 文件. -Dflume.root.logger=INFO,console ：-D 表示 flume 运行时动态修改 flume.root.logger 参数属性值，并将控制台日志打印级别设置为 INFO 级别。日志级别包括:log、info、warn、 error。 另一个终端使用 netcat 向监听的端口发送内容：\nnc localhost 4444 检查启动任务的端口是否收到。\n监控 hive 日志上传 hdfs 在 job 目录下新建任务的配置文件 flume-file-hdfs.conf,内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 # Name the components on this agent a2.sources = r2 a2.sinks = k2 a2.channels = c2 # Describe/configure the source a2.sources.r2.type = exec a2.sources.r2.command = tail -F /export/server/hive/logs/metastore.log #这里我监控的是hive的元数据日志 # Describe the sink a2.sinks.k2.type = hdfs a2.sinks.k2.hdfs.path = hdfs://linux01:8020/flume/%Y%m%d/%H #8020端口不要搞错，具体查看hadoop的core-site.xml #上传文件的前缀 a2.sinks.k2.hdfs.filePrefix = logs- #是否按照时间滚动文件夹 a2.sinks.k2.hdfs.round = true #多少时间单位创建一个新的文件夹 a2.sinks.k2.hdfs.roundValue = 1 #重新定义时间单位 a2.sinks.k2.hdfs.roundUnit = hour #是否使用本地时间戳 a2.sinks.k2.hdfs.useLocalTimeStamp = true #注意：对于所有与时间相关的转义序列，Event Header 中必须存在以 “timestamp”的 key（除非 hdfs.useLocalTimeStamp 设置为 true，此方法会使用 TimestampInterceptor 自 动添加 timestamp）。 #积攒多少个 Event 才 flush 到 HDFS 一次 a2.sinks.k2.hdfs.batchSize = 100 #设置文件类型，可支持压缩 a2.sinks.k2.hdfs.fileType = DataStream #多久生成一个新的文件 a2.sinks.k2.hdfs.rollInterval = 60 #设置每个文件的滚动大小 a2.sinks.k2.hdfs.rollSize = 134217700 #文件的滚动与 Event 数量无关 a2.sinks.k2.hdfs.rollCount = 0 # Use a channel which buffers events in memory a2.channels.c2.type = memory a2.channels.c2.capacity = 1000 a2.channels.c2.transactionCapacity = 100 # Bind the source and sink to the channel a2.sources.r2.channels = c2 a2.sinks.k2.channel = c2 1 2 3 4 5 6 7 8 先在 flume 文件夹下启动 flume 的监听任务： bin/flume-ng agent -c conf/ -n a2 -f job/flume-file-hdfs.conf -D flume.root.logger=INFO,console 启动 hdfs 和 hive 的元数据服务 start-dfs.sh start-hivemetastore.sh（自己写的脚本） 启动 hive 开始操作 hive 会产生元数据记录在 metastore.log 中，然后就会被 flume 监听到，flume 就会把监听到的日志写到 hdfs 的 flume 文件夹中。浏览器打开 linux01:9870 查看 hdfs 的文件目录，发现新建了 flume 文件夹，表示操作成功。 ![](image-6.png) 注意！监听的 metastore.log 一定要是有效的，如果无效那么 hive 的日志就不会写到里面，flume 就检测不到，具体去看 hive 的日志配置教程。另外启动的 agent 的任务名字和配置文件不要搞错了，是 a2 和 flume-file-hdfs.conf。\n实时读取目录文件到 hdfs job 目录下编写 flume-dir-hdfs.conf 配置文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 a3.sources = r3 a3.sinks = k3 a3.channels = c3 # Describe/configure the source # source类型是目录 a3.sources.r3.type = spooldir #定义监控目录 a3.sources.r3.spoolDir = /export/server/flume/upload #定义文件上传完后缀 a3.sources.r3.fileSuffix = .COMPLETED #是否有文件头 a3.sources.r3.fileHeader = true #忽略所有以.tmp 结尾的文件，不上传 a3.sources.r3.ignorePattern = ([^ ]*\\.tmp) # Describe the sink a3.sinks.k3.type = hdfs a3.sinks.k3.hdfs.path = hdfs://linux01:8020/flume/upload/%Y%m%d/%H #hdfs的upload文件夹要提前手动创建好，flume不会自己创建，否则会报错。 #上传文件的前缀 a3.sinks.k3.hdfs.filePrefix = upload- #是否按照时间滚动文件夹 a3.sinks.k3.hdfs.round = true #多少时间单位创建一个新的文件夹 a3.sinks.k3.hdfs.roundValue = 1 #重新定义时间单位 a3.sinks.k3.hdfs.roundUnit = hour #是否使用本地时间戳 a3.sinks.k3.hdfs.useLocalTimeStamp = true #积攒多少个 Event 才 flush 到 HDFS 一次 a3.sinks.k3.hdfs.batchSize = 100 #设置文件类型，可支持压缩 a3.sinks.k3.hdfs.fileType = DataStream #多久生成一个新的文件 a3.sinks.k3.hdfs.rollInterval = 60 #设置每个文件的滚动大小大概是 128M a3.sinks.k3.hdfs.rollSize = 134217700 #文件的滚动与 Event 数量无关 a3.sinks.k3.hdfs.rollCount = 0 # Use a channel which buffers events in memory a3.channels.c3.type = memory a3.channels.c3.capacity = 1000 a3.channels.c3.transactionCapacity = 100 # Bind the source and sink to the channel a3.sources.r3.channels = c3 a3.sinks.k3.channel = c3 flume 目录下启动 agent 任务：\nbin/flume-ng agent -c conf/ -n a3 -f job/flume-dir-hdfs.conf\n注意不要有多余的空格或者不可见字符，启动失败就去 logs 文件夹看日志\n任务启动后就往监控目录/flume/upload 文件夹里面放文件 ，放了 3 个不同的文件，其中 tmp 后缀的文件没有上传到 hdfs，因为在 conf 配置文件中把 tmp 后缀的排除了，其他两个上传完毕，并且文件后缀改成 COMPLETED： 进入 linux01:9870 查看 hdfs 文件目录， 确实上传成功了。 注意！ 配置文件的 a3.sinks.k3.hdfs.path 指定了 linux01:8020，那么 flume 任务就得在 linux01 上启动，在 linux02 上启动不会生效。我的 linux01 是主机，linux02 和 03 是从机，就算在 linux02 上启动 flume 任务，把 a3.sinks.k3.hdfs.path 改成 linux02:8020 也不行，必须在 linux01 上启动。 注意！向/flume/upload 文件夹放的文件不能是以上传完成的后缀结尾，比如文件上传成功后缀是 COMPLETED，那么向里面放的文件后缀就不能是 COMPLETED。另外不能向 upload 里放文件名相同的文件，文件名相同的文件只有第一个会上传到 hdfs，之后的不会，因为 linux 同一目录不允许同名文件产生。\n实时监控目录下的多个追加文件 案例 2 的 exec source 适用于监控一个实时追加的文件，不能断点续传，案例 3 的 spooldir source 适用于同步新文件，但不适用于实时监听同步追加日志的文件，而该案例的 Taildir Source 就适合于监听多个实时追加的文件，并能实现断点续传。 job 目录下新建 flume-dir-hdfs.conf 配置文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 a3.sources = r3 a3.sinks = k3 a3.channels = c3 # Describe/configure the source #定义source类型 a3.sources.r3.type = TAILDIR a3.sources.r3.positionFile = /export/server/apache-flume-1.9.0-bin/tail_dir.json #注意！！这里我把软链接flume换成了本来的真实目录apache-flume-1.9.0-bin，原因后面讲 #文件组 a3.sources.r3.filegroups = f1 f2 a3.sources.r3.filegroups.f1 = /export/server/flume/files/.*file.* a3.sources.r3.filegroups.f2 = /export/server/flume/files2/.*log.* # Describe the sink a3.sinks.k3.type = hdfs a3.sinks.k3.hdfs.path = hdfs://linux01:8020/flume/upload2/%Y%m%d/%H #上传文件的前缀 a3.sinks.k3.hdfs.filePrefix = upload- #是否按照时间滚动文件夹 a3.sinks.k3.hdfs.round = true #多少时间单位创建一个新的文件夹 a3.sinks.k3.hdfs.roundValue = 1 #重新定义时间单位 a3.sinks.k3.hdfs.roundUnit = hour #是否使用本地时间戳 a3.sinks.k3.hdfs.useLocalTimeStamp = true #积攒多少个 Event 才 flush 到 HDFS 一次 a3.sinks.k3.hdfs.batchSize = 100 #设置文件类型，可支持压缩 a3.sinks.k3.hdfs.fileType = DataStream #多久生成一个新的文件 a3.sinks.k3.hdfs.rollInterval = 60 #设置每个文件的滚动大小大概是 128M a3.sinks.k3.hdfs.rollSize = 134217700 #文件的滚动与 Event 数量无关 a3.sinks.k3.hdfs.rollCount = 0 # Use a channel which buffers events in memory a3.channels.c3.type = memory a3.channels.c3.capacity = 1000 a3.channels.c3.transactionCapacity = 100 # Bind the source and sink to the channel a3.sources.r3.channels = c3 a3.sinks.k3.channel = c3 hdfs 文件中提前创建好 upload2 文件夹：\nhdfs dfs -mkdir /flume/upload2 flume 文件夹中创建 files 和 files2 文件夹，分别在里面写 file1.txt 和 log1.log 用于追加内容让 flume 任务监控。\nflume 文件下启动监控任务：\nbin/flume-ng agent -c conf/ -n a3 -f job/flume-taildir-hdfs.conf\n用 echo 命令向 file1.txt 和 log1.log 追加内容，追加的内容就会被 flume 检测到，filume 就会把追加的新内容上传到 hdfs 的 upload2 文件夹。 追加的内容被检测到，上传到 hdfs，案例成功！ 注意！\n配置文件中，之前是 a3.sources.r3.positionFile = /export/server/flume/tail_dir.json，此时启动 flume 任务能成功，但是追加的内容不会上传到 hdfs，也就是该案例没有成功。去 logs 文件中查看 flume.log 日志，发现有一段报错如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 21 四月 2024 22:52:16,844 ERROR [conf-file-poller-0] (org.apache.flume.node.AbstractConfigurationProvider.loadSources:355) - Source r3 has been removed due to an error during configuration org.apache.flume.FlumeException: Error creating positionFile parent directories at org.apache.flume.source.taildir.TaildirSource.configure(TaildirSource.java:170) at org.apache.flume.conf.Configurables.configure(Configurables.java:41) at org.apache.flume.node.AbstractConfigurationProvider.loadSources(AbstractConfigurationProvider.java:325) at org.apache.flume.node.AbstractConfigurationProvider.getConfiguration(AbstractConfigurationProvider.java:105) at org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:145) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:750) Caused by: java.nio.file.FileAlreadyExistsException: /export/server/flume at sun.nio.fs.UnixException.translateToIOException(UnixException.java:88) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384) at java.nio.file.Files.createDirectory(Files.java:674) at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781) at java.nio.file.Files.createDirectories(Files.java:727) at org.apache.flume.source.taildir.TaildirSource.configure(TaildirSource.java:168) ... 11 more 给 chatgpt 看看： 大概意思是 positionfile 文件创建失败，原因是出现命名冲突。因为我的 flume 是个软链接，类似于快捷方式，但是写到配置文件里面，flume 程序就会把配置文件的 flume 当成真实目录，进而就会尝试创建名为 flume 的目录并且去进到创建的 flume 目录创建 r3，然而我已经存在了名为 flume 的软链接，程序就会创建 flume 目录失败，进而无法创建 r3。所以把配置文件的 flume 换成真实的 apache-flume-1.9.0-bin 目录就可以了，这样就可以生成 r3，也就是 positionfile = tail_dir.json 文件。当然另一种解决方法就是把 positionfile 的位置放到 flume 软链接外面。\ntail_dir.json 文件内容如下：\n1 2 3 4 5 {\u0026#34;inode\u0026#34;:83899573,\u0026#34;pos\u0026#34;:44,\u0026#34;file\u0026#34;:\u0026#34;/export/server/flume/files/file1.txt\u0026#34;} inode是文件的唯一标识，即使文件重命名也不会变，除非文件删除 pos表示读到哪里 file：监控文件的绝对路径 json文件靠inode和file两个值表示pos位置信息。 注意！ log4j 日志框架每天凌晨会自动把前一天的 hive.log 的文件改名，后缀加上日期，这点对我们监控空间极不友好，假如我们监控的是 hive.log，然而 hive.log 会自动更名 hive.log.2024-xx-xx,监测的文件名发生改变,而 inode 不变，然而 json 文件中记录的绝对路径仍然是 hive.log，此时的 hive.log 是新的文件，inode 变化，就无法实现断点续传。\n解决方案：1）不使用 log4j 2）修改 flume 源码包 修改源码包的 TailFile 和 ReliableTaildirEventReader： 修改后重新打包生成 flume-taildir-source-1.9.0.jar,进入 flume/lib 目录下，把原来的 jar 包替换掉： 把原来的后缀改成 bak。\n","date":"2024-04-22T15:22:55Z","permalink":"/zh-cn/post/2024/04/flume%E5%85%A5%E9%97%A8--%E4%B8%87%E5%AD%97%E8%AF%A6%E8%A7%A3/","title":"Flume入门--万字详解"},{"content":"概述 ZooKeeper 是一个开源的分布式协调服务，它的设计目标是为那些高吞吐的大型分布式系统提供一个高性能、高可用、且具有严格顺序访问控制 能力的分布式协调服务，并以一系列简单易用的接口提供给用户使用。\nZooKeeper 将数据存全量储在内存中以保持高性能，并通过服务集群来实现高可用，由于 Zookeeper 的所有更新和删除都是基于事务的，所以其在读多写少的应用场景中有着很高的性能表现。\n简单来说 zookeeper 就是动物园管理者，管理协调大数据里面的一堆组件，比如 hadoop、hive、habse 等等。Zookeeper 可以用于实现分布 式系统中常见的发布/订阅、负载均衡、命令服务、分布式协调/通知、集群管理、Master 选举、分布式 锁和分布式队列等功能。\n特点 -顺序一致性：从一个客户端发起的事务请求，最终都会严格按照其发起顺序被应用到 Zookeeper 中； -原子性：所有事务请求的处理结果在整个集群中所有机器上都是一致的；不存在部分机器应用了该事务，而另一部分没有应用的情况，一次数据更新要么成功要么失败。 -单一视图（全局数据一致）：每个 server 保存相同的数据副本，无论 client 连接哪个 server，看到的数据一致； -可靠性：一旦服务端成功应用了一个事务，则其引起的改变会一直保留，直到被另外一个事务所更改； -实时性：一旦一个事务被成功应用后，在一定时间范围内，Zookeeper 可以保证客户端立即可以读取到这个事务变更后的最新状态的数据。\n集群中只要有半数以上节点存活，zk 集群就可以正常服务，所以 zk 适合安装奇数台。 一个 leader，多个 follower，leader 挂掉之后会从 follower 中重新选举。 集群配置 可以由一组 Zookeeper 服务构成 Zookeeper 集群，集群中每台机器都会单独在内存中维护自身的状 态，并且每台机器之间都保持着通讯，只要集群中有半数机器能够正常工作，那么整个集群就可以正常提供服务。对于来自客户端的每个更新请求，Zookeeper 都会分配一个全局唯一的递增 ID，这个 ID 反映了所有事 务请求的先后顺序。\n以下是集群环境搭建！！不是单机环境 解压、安装、配置环境变量并生效这三步省略，直接修改配置：\n进入 conf/目录下，拷贝配置样本并进行修改：\ncp zoo_sample.cfg zoo.cfg\n指定数据存储目录和日志文件目录（此时还没有目录，稍后手动创建），修改后完整配置如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 tickTime=2000 #用于计算的基础时间单元。比如 session 超时：N*tickTime； initLimit=10 #用于集群，允许从节点连接并同步到 master 节点的初始化连接时间，以 tickTime 的倍数来表示； syncLimit=5 #用于集群， master 主节点与从节点之间发送消息，请求和应答时间长度（心 跳机制）； dataDir=/export/server/zookeeper/data #数据存储位置；稍后手动创建 dataLogDir=/export/server/zookeeper/logs #日志目录；稍后手动创建 clientPort=2181 #用于客户端连接的端口，默认 2181 # server.1 这个1是服务器的标识，可以是任意有效数字，标识这是第几个服务器节点，这个标识要写到 dataDir目录下面myid文件里，如果没有myid文件要自己创建 # 指名集群间通讯端口和选举端口 server.1=linux01:2888:3888 server.2=linux02:2888:3888 server.3=linux03:2888:3888 标识节点序号\n分别在三台主机的 dataDir 目录下新建 myid 文件,并写入对应的节点标识。Zookeeper 集群通过 myid 文件识别集群节点，并通过上文配置的节点通信端口和选举端口来进行节点通信，选举出 Leader 节点。\n在每个服务器上的/export/server/zookeeper/下创建 data 目录，在里面创建 myid 文件并写入各自序号，这个序号必须和 zoo.cfg 文件的序号相同。\n分别在三台主机上启动 ZK 集群\nzkServer.sh start\n集群验证\nzkServer.sh status\n可以看到一个 leader，两个 follower，那么 zk 集群配置成功\n启动客户端\nzkCli.sh\n集群角色 ZK 集群有一个 leader 和多个 follower。\nLeader 为客户端提供读写服务，并维护集群状态，它是由集群选举所产生的；\nFollower 为客户端提供读写服务，并定期向 Leader 汇报自己的节点状态。同时也参与写操作 “过半写成功”的策略和 Leader 的选举；\nObserver 为客户端提供读写服务，并定期向 Leader 汇报自己的节点状态，但不参与写操作“过 半写成功”的策略和 Leader 的选举，因此 Observer 可以在不影响写性能的情况下提升集群的读性 能。\n会话 Zookeeper 客户端通过 TCP 长连接连接到服务集群，会话 (Session) 从第一次连接开始就已经建立，之 后通过心跳检测机制来保持有效的会话状态。通过这个连接，客户端可以发送请求并接收响应，同时也 可以接收到 Watch 事件的通知。 关于会话中另外一个核心的概念是sessionTimeOut(会话超时时间)，当由于网络故障或者客户端主动 断开等原因，导致连接断开，此时只要在会话超时时间之内重新建立连接，则之前创建的会话依然有效。 Watcher Zookeeper 中一个常用的功能是 Watcher(事件监听器)，它允许用户在指定节点上针对感兴趣的事件注 册监听，当事件发生时，监听器会被触发，并将事件信息推送到客户端。该机制是 Zookeeper 实现分布式协调服务的重要特性。\n节点的值变化监听 1）在 linux01 主机上注册监听/sanguo 节点数据变化\n[zk: localhost:2181(CONNECTED) 26] get -w /sanguo\n2）在 linux02 主机上修改/sanguo 节点的数据\n[zk: localhost:2181(CONNECTED) 1] set /sanguo \u0026ldquo;xisi\u0026rdquo;\n3）观察 linux01 主机收到数据变化的监听\nWATCHER::\nWatchedEvent state:SyncConnected ype:NodeDataChanged\npath:/sanguo\n注意：在 linux02 再多次修改/sanguo 的值，linux01 上不会再收到监听。因为注册 一次，只能监听一次。想再次监听，需要再次注册。\n节点的子节点变化监听（路径变化） 1）在 linux01 主机上注册监听/sanguo 节点的子节点变化\n[zk: localhost:2181(CONNECTED) 1] ls -w /sanguo [shuguo, weiguo]\n2）在 linux02 主机/sanguo 节点上创建子节点\n[zk: localhost:2181(CONNECTED) 2] create /sanguo/jin \u0026ldquo;simayi\u0026rdquo;\nCreated /sanguo/jin\n3）观察 linux01 主机收到子节点变化的监听\nWATCHER::\nWatchedEvent state:SyncConnected type:NodeChildrenChanged\npath:/sanguo\n注意：节点的路径变化，也是注册一次，生效一次。想多次生效，就需要多次注册\n工作机制 从设计模式的角度来理解，zk 是基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper 就将负责通知已经在 Zookeeper 上注册的那些观察者做出相应的反应。\n集群中只要有半数以上节点存活，Zookeeper 集群就能正常服务。所以 Zookeeper 适合安装奇数台服务器。\n客户端向服务端写数据流程 写请求发给 leader 写请求发给 follower 客户端向服务端读数据流程 由于 ZK 满足的是 CAP 中的 CP，，没有满足 Available，因此读出的数据可能是老数据。\n选举机制 初次启动 非处次启动 集群脑裂 对于一个集群，通常多台机器会部署在不同机房，来提高这个集群的可用性。保证可用性的同时，会发生一种机房间网络线路故障，导致机房间网络不通，而集群被割裂成几个小集群。这时候子集群各自选主导致“脑裂”的情况。\n过半机制是如何防止脑裂现象产生的？\nZooKeeper 的过半机制导致不可能产生 2 个 leader，因为少于等于一半是不可能产生 leader 的，这就使得不论机房的机器如何分配都不可能发生脑裂。\n数据模型 Zookeeper 数据模型是由一系列基本数据单元 Znode (数据节点) 组成的节点树，其中根节点为 / ，每个节点上都会保存自己的数据和节点信息。不过和常见的文件系统不同，Zookeeper 将数据全量存储在内存中，以此来实现高吞吐，减少访 问延迟。\n节点类型 Zookeeper 中节点可以分为两大类：\n持久节点：节点一旦创建，除非被主动删除，否则一直存在；\n临时节点：一旦创建该节点的客户端会话失效，则所有该客户端创建的临时节点都会被删除。\n临时节点和持久节点都可以添加一个特殊的属性： SEQUENTIAL ，代表该节点是否具有递增属性。如果指定该属性，那么在这个节点创建时，Zookeeper 会自动在其节点名称后面追加一个由父节点维护的递增数字。这个递增数字可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序。\n节点信息 集群操作 创建节点 create [-s] [-e] path data acl #其中-s 为有序节点，-e 临时节点\n创建有序节点，此时创建的节点名为指定节点名 + 自增序号：\n[zk: localhost:2181(CONNECTED) 23] create -s /a \u0026ldquo;aaa\u0026rdquo;\nCreated /a0000000022\n[zk: localhost:2181(CONNECTED) 24] create -s /b \u0026ldquo;bbb\u0026rdquo;\nCreated /b0000000023\n[zk: localhost:2181(CONNECTED) 25] create -s /c \u0026ldquo;ccc\u0026rdquo;\nCreated /c0000000024\n创建临时节点，临时节点会在会话过期后被删除：\n[zk: localhost:2181(CONNECTED) 26] create -e /tmp \u0026ldquo;tmp\u0026rdquo;\nCreated /tmp\n查看节点 get path [watch]\n[zk: localhost:2181(CONNECTED) 31] get /hadoop\n123456 #节点数据\ncZxid = 0x14b\nctime = Fri May 24 17:03:06 CST 2019\nmZxid = 0x14b\nmtime = Fri May 24 17:03:06 CST 2019\npZxid = 0x14b\ncversion = 0\ndataVersion = 0\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 6\nnumChildren = 0\n节点各个属性如下表。其中一个重要的概念是 Zxid(ZooKeeper Transaction Id)，ZooKeeper 节点的 每一次更改都具有唯一的 Zxid，如果 Zxid1 小于 Zxid2，则 Zxid1 的更改发生在 Zxid2 更改之前。\n查看节点状态 stat path [watch]\n它和 get 类似，但不会返回节点数据内容\n更新节点 [zk: localhost:2181(CONNECTED) 33] set /hadoop 345\ncZxid = 0x14b\nctime = Fri May 24 17:03:06 CST 2019\nmZxid = 0x14c\nmtime = Fri May 24 17:13:05 CST 2019\npZxid = 0x14b\ncversion = 0\ndataVersion = 1 # 注意更改后此时版本号为 1，默认创建时为 0\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 3\nnumChildren = 0\n也可以基于版本号进行更改，此时类似于乐观锁机制，当你传入的数据版本号 (dataVersion) 和当前节 点的数据版本号不符合时，zookeeper 会拒绝本次修改：\n[zk: localhost:2181(CONNECTED) 34] set /hadoop 678 0\nversion No is not valid : /hadoop #无效的版本号\n删除节点 delete path [version]\n和更新节点数据一样，也可以传入版本号，当你传入的数据版本号 (dataVersion) 和当前节点的数据版 本号不符合时，zookeeper 不会执行删除操作。\n[zk: localhost:2181(CONNECTED) 36] delete /hadoop 0\nversion No is not valid : /hadoop #无效的版本号\n[zk: localhost:2181(CONNECTED) 37] delete /hadoop 1\n[zk: localhost:2181(CONNECTED) 38]\n要想删除某个节点及其所有后代节点，可以使用递归删除，命令为rmr path。\n退出 ZK [zk: localhost:2181(CONNECTED) 12] quit\n应用场景 统一命名服务 统一配置管理 统一集群管理 服务器动态上下线 软负载均衡 分布式锁 拜占庭将军问题（Paxos 算法） Paxos 算法是一种基于消息传递且具有高度容错特性的一致性算法。解决如何快速正确的在一个分布式系统中对某个数据值达成一致，并且保证任何异常都不会破坏整个系统的一致性。\n算法描述 算法流程 ZAB 协议 ZAB 协议并不像 Paxos 算法那样是一种通用的分布式一致性算法，ZAB 是一种特别为 Zookeeper 设计的崩溃可恢复的原子消息广播算法。在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。ZAB 包括以下两种模式:\n崩溃恢复 当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进入恢复模式并选举产生新的 Leader 服务器。当选举产生了新的 Leader 服务器，同时集群中已经有过半的机器与该 Leader 服务器完成了状态同步之后，ZAB 协议就会退出恢复模式。其中，所谓的状态同步是指数据同步，用来保证集群中存在过半的机器能够和 Leader 服务器的数据状态保持一致。\n消息广播 当集群中已经有过半的 Follower 服务器完成了和 Leader 服务器的状态同步，那么整个服务框架就可以进入消息广播模式了。 当一台同样遵守 ZAB 协议的服务器启动后加入到集群中时，如果此时集群中已经存在一个 Leader 服务器在负责进行消息广播，那么新加入的服务器就会自觉地进入数据恢复模式：找到 Leader 所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。 CAP 理论 ZK 源码图示 小小面试题 选举机制 半数机制，超过半数的投票通过，即通过。\n第一次启动选举规则： 投票过半数时，服务器 id 大的胜出 第二次启动选举规则： EPOCH 大的直接胜出\nEPOCH 相同，事务 id 大的胜出\n事务 id 相同，服务器 id 大的胜出\n生产集群安装多少 zk 合适？ 安装奇数台。\n生产经验：\n10 台服务器：3 台 zk； 20 台服务器：5 台 zk； 100 台服务器：11 台 zk； 200 台服务器：11 台 zk ","date":"2024-04-19T16:43:51Z","permalink":"/zh-cn/post/2024/04/%E5%A4%A7%E6%95%B0%E6%8D%AEzookeeper%E9%9B%86%E7%BE%A4%E5%85%A5%E9%97%A8%E5%8F%8A%E4%BD%BF%E7%94%A8/","title":"大数据—Zookeeper集群入门及使用"},{"content":"HA 概述 1）所谓 HA（High Availablity），即高可用（7*24 小时不中断服务）。\n2）实现高可用最关键的策略是消除单点故障（传统的主从模式集群单个节点发生故障会影响整个集群）。HA 严格来说应该分成各个组件的 HA 机制：HDFS 的 HA 和 YARN 的 HA。\n3）NameNode 主要在以下两个方面影响 HDFS 集群\nNameNode 机器发生意外，如宕机，集群将无法使用，直到管理员重启 NameNode 机器需要升级，包括软件、硬件升级，此时集群也将无法使用 HDFS HA 功能通过配置多个 NameNode(Active/Standby)实现在集群中对 NameNode 的热备来解决上述问题。如果出现故障，如机器崩溃或机器需要升级维护，这时可以启动另一台机器上的 NameNode 继续维护整个集群的运行（集群中同时只能有一台 active 的 NN，其他 NN 处于 standby（备用） ）。而这种启动方式分为手动和自动（推荐），但是在这之前，我们必须通过某种方式保证所有 NN 的元数据一致，这样才能保证 active 状态的 NN 故障后，另一个处于 standby 状态的 NN 激活为 active 能够正常维持集群运行，类似于公司员工的任务的交接。 HDFS 高可用 保证所有 NN 的数据一致性 在处于 active 的 NN 正常运行时，他会生成 Fsimage 文件，让其他处于 standby 的 NN 同步，同时引入 JournalNode 节点来保证 edits 文件数据的一致性，JournalNode 作为 active 的 NN 和 standby 的 NN 的中间节点，activeNN 会把 edits 发送给 JournalNode，然后 standbyNN 从 JournalNode 获取 edits。同时为了保证 JournalNode 的可靠性，JournalNode 本身也是一个多节点的集群。\nJournalNode 节点会在集群自动的选择一个\u0026quot;主\u0026quot;节点出来，Active 节点会和 JournalNode 的主节点通信，然后 JournalNode 集群的主节点会将数据发送给其他的节点，只要有过半的节点完成了数据的存储（过半写成功），JournalNode 集群的主节点，就会将成功信息返回给 Active 节点。当 JournalNode 集群的主节点挂掉，其他的 JournalNode 节点会快速选举出新的\u0026quot;主\u0026quot;节点来。\n同时在 HA 架构中，并没有 SecondaryNameNode，那么定期合并 fsimage 的 eedits 的任务是由 standby 的 NN 来完成的。\n手动模式 配置 core-site.xml\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026lt;configuration\u0026gt; \u0026lt;!-- 指定hdfs的nameservice为ns1 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.defaultFS\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://mycluster/\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定hadoop临时目录 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.tmp.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 配置 hdfs-site.xml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 \u0026lt;configuration\u0026gt; \u0026lt;!--指定hdfs的nameservice为mycluster，需要和core-site.xml中的保持一致 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.nameservices\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mycluster\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- hadoop-ha下面有两个NameNode，分别是nn1，nn2 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.ha.namenodes.mycluster\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;nn1,nn2,nn3\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- nn1的RPC通信地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.rpc-address.mycluster.nn1\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux01:8020\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- nn1的http通信地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.http-address.mycluster.nn1\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux01:9870\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- nn2的RPC通信地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.rpc-address.mycluster.nn2\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux02:8020\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- nn2的http通信地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.http-address.mycluster.nn2\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux02:9870\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- nn3的RPC通信地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.rpc-address.mycluster.nn3\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux03:8020\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- nn3的http通信地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.http-address.mycluster.nn3\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux03:9870\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定NameNode的edits元数据在JournalNode上的存放位置 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.shared.edits.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;qjournal://linux01:8485;linux02:8485;linux03:8485/mycluster\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- NameNode 数据存储目录 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.namenode.name.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;file://${hadoop.tmp.dir}/name\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- DataNode 数据存储目录 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.datanode.data.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;file://${hadoop.tmp.dir}/data\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- JournalNode数据存储目录 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.journalnode.edits.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;${hadoop.tmp.dir}/data\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 配置失败自动切换实现方式 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.client.failover.proxy.provider.mycluster\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.ha.fencing.methods\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;sshfence\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.ha.fencing.ssh.private-key-files\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/root/.ssh/id_rsa\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; HA 集群的相关文件配置省略。以 3 台服务器的 HA 为例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 在各个节点上，输入以下命令启动该节点的journalNode服务： hdfs --daemon start journalnode 在NN1上进行格式化并启动 hdfs namenode -format hdfs --daemon start namenode 分别在NN2和NN3上运行如下命令，同步NN1的元数据信息 hdfs namenode -bootstrapStandby 启动NN2，NN3 hdfs --daemon start namenode 此时所有NN处于standby，启动所有节点的datanode hdfs --daemon start datanode 切换NN1为active hdfs haadmin start -transitionToActive linux01 修改后重新分发文件！ 自动模式 自动模式需要引入 zookeeper 和 ZKFailoverController（ZKFC）\n配置 hdfs-site.xml\n1 2 3 4 5 \u0026lt;!-- 启用 nn 故障自动转移 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.ha.automatic-failover.enabled\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; 配置 core-site.xml\n1 2 3 4 5 \u0026lt;!-- 指定 zkfc 要连接的 zkServer 地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;ha.zookeeper.quorum\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux01:2181,linux02:2181,linux03:2181\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; 修改后重新分发文件！\n1 2 3 4 5 6 7 8 在每台服务器运行以下命令启动zookeeper集群： zkServer.sh start 启动后初始化HA在zookeeper中的状态： hdfs zkfc -formatZK 杀死active namenode查看是否有standby namenode激活： kill -9 namenode的进程id 运行zkCli.sh查看namenode选举内容： zkCli.sh 解决 NN 连接不上 JN 的问题 自动故障转移配置好以后，然后使用 start-dfs.sh 群起脚本启动 hdfs 集群，有可能 会遇到 NameNode 起来一会后，进程自动关闭的问题。查看 NameNode 日志，报错信息如下： 查看报错日志，可分析出报错原因是因为 NameNode 连接不上 JournalNode，而利 用 jps 命令查看到三台 JN 都已经正常启动，为什么 NN 还是无法正常连接到 JN 呢？这 是因为 start-dfs.sh 群起脚本默认的启动顺序是先启动 NN，再启动 DN，然后再启动 JN， 并且默认的 rpc 连接参数是重试次数为 10，每次重试的间隔是 1s，也就是说启动完 NN 以后的 10s 中内，JN 还启动不起来，NN 就会报错了。\ncore-default.xml 里面有两个参数如下：\n1 2 3 4 5 6 7 8 9 10 \u0026lt;!--NN连接JN重试次数，默认10次--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;ipc.client.connect.max.retries\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;10\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!--重试时间间隔，默认1s--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;ipc.client.connect.retry.interval\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;1000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; 解决方案：可以先 JN 成功启动，然后启动三台 NN 或者 在 core-site.xml 调大上面的参数：\n1 2 3 4 5 6 7 8 9 10 \u0026lt;!--NN连接JN重试次数，默认10次--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;ipc.client.connect.max.retries\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;20\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!--重试时间间隔，默认1s--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;ipc.client.connect.retry.interval\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;5000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; Yarn 高可用 核心问题 1 2 3 4 如果当前 active rm 挂了，其他 rm 怎么将其他 standby rm 上位 核心原理跟 hdfs 一样，利用了 zk 的临时节点 当前 rm 上有很多的计算程序在等待运行,其他的 rm 怎么将这些程序接手过来接着跑 rm会将当前的所有计算程序的状态存储在 zk 中,其他 rm 上位后会去读取，然后接着跑 配置 yarn-site.xml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.aux-services\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mapreduce_shuffle\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 启用 resourcemanager ha --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.ha.enabled\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 声明两台 resourcemanager 的地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.cluster-id\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;cluster-yarn1\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!--指定 resourcemanager 的逻辑列表--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.ha.rm-ids\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;rm1,rm2,rm3\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- ========== rm1 的配置 ========== --\u0026gt; \u0026lt;!-- 指定 rm1 的主机名 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.hostname.rm1\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux01\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定 rm1 的 web 端地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.webapp.address.rm1\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux01:8088\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定 rm1 的内部通信地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.address.rm1\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux01:8032\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定 AM 向 rm1 申请资源的地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.scheduler.address.rm1\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux01:8030\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定供 NM 连接的地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.resource-tracker.address.rm1\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux01:8031\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- ========== rm2 的配置 ========== --\u0026gt; \u0026lt;!-- 指定 rm2 的主机名 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.hostname.rm2\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux02\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.webapp.address.rm2\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux02:8088\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.address.rm2\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux02:8032\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.scheduler.address.rm2\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux02:8030\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.resource-tracker.address.rm2\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux02:8031\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- ========== rm3 的配置 ========== --\u0026gt; \u0026lt;!-- 指定 rm3 的主机名 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.hostname.rm3\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux03\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定 rm3 的 web 端地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.webapp.address.rm3\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux03:8088\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定 rm3 的内部通信地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.address.rm3\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux03:8032\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定 AM 向 rm3 申请资源的地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.scheduler.address.rm3\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux03:8030\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定供 NM 连接的地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.resource-tracker.address.rm3\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux03:8031\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定 zookeeper 集群的地址 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.zk-address\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;linux01:2181,linux02:2181,linux03:2181\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 启用自动恢复 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.recovery.enabled\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 指定 resourcemanager 的状态信息存储在 zookeeper 集群 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.resourcemanager.store.class\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateSt ore\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;!-- 环境变量的继承 --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.env-whitelist\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLAS SPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 配置后重新分发配置文件！\n1 2 3 4 5 6 7 启动Yarn start-yarn.sh 查看服务状态 yarn rmadmin -getServiceState rm1 运行zkCli.sh查看RM选举内容 zkCli.sh 可以通过8088端口查看Yarn状态 HA 最终规划 以上来自尚硅谷 Hadoop HA 高可用\n","date":"2024-04-15T19:23:57Z","permalink":"/zh-cn/post/2024/04/hahadoop%E9%AB%98%E5%8F%AF%E7%94%A8/","title":"HA—Hadoop高可用"},{"content":"Hadoop 简介 狭义来说，hadoop 是 Apache 基金会开发的分布式系统基础架构，用来解决海量数据的存储和海量数据的分析计算问题。广义上来说，Hadoop 通常是指一个更广泛的概念 \u0026mdash;\u0026mdash; Hadoop 生态圈。\nHadoop 三大发行版本 Apache、Cloudera、Hortonworks\nApache 版本最原始（最基础）的版本，对于入门学习最好。\nCloudera 在大型互联网企业中用的较多。其主要产品有 CDH、Cloudera Manager，Cloudera Support\nHadoop 优势 高可靠性： Hadoop 底层维护多个数据副本，所以即使 Hadoop 某个计算元素或存储出现故障，也不会导致数据的丢失。\n高扩展性： 在集群间分配任务数据，可方便的扩展数以千计的节点。\n高效性： 在 MapReduce 的思想下，Hadoop 是并行工作的，以加快任务处理速度。\n高容错性： 能够自动将失败的任务重新分配。\n**低成本：**Hadoop 不要求机器的配置达到极高的标准，大部分普通商用服务器即可满足要求，通过提供多个副本和容错机制提高集群的可靠性\nHadoop 基本组成 常用 Shell 命令 hdfs dfs -ls \u0026lt;path\u0026gt;：列出指定 HDFS 路径下的文件和目录 hdfs dfs -mkdir \u0026lt;path\u0026gt;：在 HDFS 中创建新目录 hdfs dfs -put \u0026lt;localsrc\u0026gt; \u0026lt;dst\u0026gt;：将本地文件（或目录）复制到 HDFS hdfs dfs -get \u0026lt;src\u0026gt; \u0026lt;localdst\u0026gt;：将 HDFS 上的文件（或目录）复制到本地 hdfs dfs -mv \u0026lt;src\u0026gt; \u0026lt;dst\u0026gt;：移动 HDFS 中的文件目录或重命名文件目录 hdfs dfs -cp \u0026lt;src\u0026gt; \u0026lt;dst\u0026gt;：复制 HDFS 中的文件或目录 hdfs dfs -rm \u0026lt;path\u0026gt;：删除 HDFS 中的文件 hdfs dfs -cat \u0026lt;path\u0026gt;：在控制台显示 HDFS 文件的内容 hdfs dfs -du \u0026lt;path\u0026gt;：显示 HDFS 文件或目录的大小 hdfs dfs -df \u0026lt;path\u0026gt;：显示 HDFS 的可用空间 hdfs fsck path [-files [-blocks [-location]]] -files列出路径内的文件状态 -files -blocks输出文件块报告（几个块，几个副本） -files -blocks -locations 输出每个block的详情 HDFS 分布存储 HDFS 是一个分布式文件系统，具有高容错、高吞吐 量等特性，分布在多个集群节点上的文件系统。有 NN、DN、SNN 三种角色。\nHDFS 启停 NameNode（NN） HDFS 的主角色，负责管理每个文件的块所在的 DataNode、整个 HDFS 文件系统、存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限）等。\nDataNode（DN） HDFS 从角色，负责处理客户端的读写请求，存储删除文件块，以及块数据校验和。\nSecondaryNameNode（SNN） NN 的辅助角色，帮 NN 打杂，监控 HDFS 状态的辅助后台程序，每隔一段时间获取 HDFS 元数据的快照。\n可通过 9870 端口（默认 9870）访问 web 界面，查看集群各节点状态及信息\n文件写入流程 发送的写入请求通过后，客户端会根据 NN 返回的信息自动把数据分块，向网络距离最近的 DN 写入数据。同时，DN 会完成备份操作，把备份传到其他的 DN，然后由其他的 DN 再次做备份传播，直到满足设置的备份数量。当数据写入完成后，客户端会通知 NN，由 NN 完成元数据记录。 HDFS 架构的稳定性 心跳机制和重新复制 每个 DataNode 定期向 NameNode 发送心跳消息，如果超过指定时间没有收到心跳消息，则将 DataNode 标记为死亡。NameNode 不会将任何新的 IO 请求转发给标记为死亡的 DataNode，也不会 再使用这些 DataNode 上的数据。 由于数据不再可用，可能会导致某些块的复制因子小于其指定值， NameNode 会跟踪这些块，并在必要的时候进行重新复制。\n数据的完整性 由于存储设备故障等原因，存储在 DataNode 上的数据块也会发生损坏。为了避免读取到已经损坏的数 据而导致错误，HDFS 提供了数据完整性校验机制来保证数据的完整性，具体操作如下： 当客户端创建 HDFS 文件时，它会计算文件的每个块的 校验和 ，并将 校验和 存储在同一 HDFS 命名空 间下的单独的隐藏文件中。当客户端检索文件内容时，它会验证从每个 DataNode 接收的数据是否与存 储在关联校验和文件中的 校验和 匹配。如果匹配失败，则证明数据已经损坏，此时客户端会选择从其 他 DataNode 获取该块的其他可用副本。\n元数据的磁盘故障 FsImage 和 EditLog 是 HDFS 的核心数据，这些数据的意外丢失可能会导致整个 HDFS 服务不可 用。为了避免这个问题，可以配置 NameNode 使其支持 FsImage 和 EditLog 多副本同步，这样 FsImage 或 EditLog 的任何改变都会引起每个副本 FsImage 和 EditLog 的同步更新。\n支持快照 快照支持在特定时刻存储数据副本，在数据意外损坏时，可以通过回滚操作恢复到健康的数据状态。\n文件读取流程 存储方式 Block 块和多副本 由于文件大小不一，不利于统一管理，hdfs 设定了统一的存储单位 Block 块，Block 块是 hdfs 最小存储单位，通常每个 128MB（可修改 dfs.blocksize）。hdfs 会按照 Block 块大小把文件切分成多份存储在多个 datanode 上也就是多个服务器上，同时为了保证整个文件的完成性（防止 Block 块丢失或损坏），hdfs 会对每个 Block 块做多个备份存储在其他节点上，备份的数量默认是 3，可以在 hdfs-site.xml 中配置数量，修改后要重新分发该文件，保证每个服务器的配置文件相同！\n1 2 3 4 \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;3\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; 同时还可以临时决定上传文件的副本数量：\nhdfs fs -D dfs.replication=5 -put test.tst /data/test\n还可以修改已存在的 hdfs 文件的副本数量：\nhdfs fs -setrep [-R] 5 path path 是指定文件路径，-R 表示对子目录也生效\nedits 和 fsimage 文件 hdfs 中文件被划分成一堆堆 block 块，为了方便整理记录文件和 block 的关系，namenode 基于一批 edits 文件和一个 fsimage 文件完成整个文件系统的维护管理。\nedits 文件是一个流水账文件，记录了 hdfs 的每一次操作以及该次操作影响的文件及其对应的 block。为了保证 edits 文件检索性能，会有多个 edits 文件，每一个 edits 文件存储到达一定数量会开启新的 edits，保证不出现超大的 edits 文件。最终所有 edits 文件会合并为一个 fsimage 文件，这个 fsimage 文件就记录了最终状态的文件操作信息。如果已经有了 fsimage，就会把全部的 edits 和已存在的 fsimage 进行新的合并，生成新的 fsimage。 元数据合并及控制参数 **注意！**元数据（eidts 和 fsimage）的合并不是由 NN 完成的，而是 SNN，NN 只是基于元数据对整个文件系统进行维护管理，负责元数据记录和权限审批，NN 是管理者，不是员工。\nSNN 会通过 http 从 NN 拉取 edits 和 fsimage 然后合并元数据并提供给 NN 使用 HDFS 漫画 读写数据 HDFS 故障类型和检测方法 MapReduce 分布式并行计算框架 MapReduce 是基于 Yarn 运行的，没有 Yarn 就无法运行 MapReduce，MapReduce 有 RM、NM、AM 三种角色。\nMR 不适合实时计算，不适合流式计算，不适合有向图计算。\n可通过 8042 端口（默认 8042）访问 web 界面，查看 MR 任务的执行信息 计算模式 MapReduce 属于分散汇总。spark、flink 属于中心调度. Map 和 Reduce Map 接口提供“分散”功能，Reduce 提供“汇总聚合”功能，用户可以通过 Java、python 等编程调用 mapreduce 接口完成开发，不过现在已经有了 Hive on MR（稍微过时），sparkSQL 等客户端。不懂编程仅用 SQL 就能完成开发，使用更方便，逐渐成为主流。\nMR 执行原理 Yarn 作业调度、资源管理 Yarn 管控整个集群的资源调度，MR 程序运行时，是在 Yarn 的监督下运行的，MR 程序会把计算任务分成若干个 map 和 reduce，然后向 Yarn 申请资源并运行任务。Yarn 有四种角色：ResourceManager（RM）、NodeManager（NM）、ProxyServer（PS）、JobHistoryServer（JHS） Yarn 启停 ResourceManager 集群资源总管家，整个集群资源调度者，负责协调调度各个程序所需资源。\nNodeManager 单机资源管家，单个服务器的资源调度者，负责协调调度单个服务器的资源供程序使用。同时负责该节点内所有容器的生命周期的管 理，监视资源和跟踪节点健康。具体如下：\n1 2 3 启动时向 ResourceManager 注册并定时发送心跳消息，等待 ResourceManager 的指令； 维护 Container 的生命周期，监控 Container 的资源使用情况； 管理任务运行时的相关依赖，根据 ApplicationMaster 的需要，在启动 Container 之前将需 要的程序及其依赖拷贝到本地。 ApplicationMaster 在用户提交一个应用程序时，YARN 会启动一个轻量级的进程 ApplicationMaster 。 ApplicationMaster 负责协调来自 ResourceManager 的资源，并通过 NodeManager 监视容器内资 源的使用情况，同时还负责任务的监控与容错。具体如下：\n1 2 3 4 根据应用的运行状态来决定动态计算资源需求； 向 ResourceManager 申请资源，监控申请的资源的使用情况； 跟踪任务状态和进度，报告资源的使用情况和应用的进度信息； 负责任务的容错。 运行时可通过服务器的 8088 端口（默认 8088）访问 web 界面\nJobHistoryServer 记录历史运行程序的信息及产生的日志，把每个程序的运行日志统一收集到 hdfs，可通过 19888 端口访问 web 界面 Container Container 是 Yarn 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。\nHadoop 一键启停 ","date":"2024-04-15T14:38:50Z","permalink":"/zh-cn/post/2024/04/hadoop%E5%85%A5%E9%97%A8hdfsmryarn/","title":"Hadoop入门—HDFS、MR、Yarn"},{"content":"数据类型 decimal 类型：\ndecimal(11,2) 代表最多有 11 位数字，其中后 2 位是小数，整数部分是 9 位；如果整数部分超过 9 位，则这个字段就会变成 null；如果小数部分不足 2 位， 则后面用 0 补齐两位，如果小数部分超过两位，则超出部分四舍五入。也可直接写 decimal，后面不指定位数，默认是 decimal(10,0) 整数 10 位，没有小数 map 类型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 zhangsan chinese:90,math:87,english:63,nature:76 lisi chinese:60,math:30,english:78,nature:0 wangwu chinese:89,math:25 create table if not exists map1( name string, score map\u0026lt;string,int\u0026gt;) row format delimited fields terminated by \u0026#39;\\t\u0026#39; collection items terminated by \u0026#39;,\u0026#39; map keys terminated by \u0026#39;:\u0026#39;; load data local inpath \u0026#39;./data/map1.txt\u0026#39; into table map1; #查询数学⼤于35分的学⽣的英语和⾃然成绩： select m.name,m.score[\u0026#39;english\u0026#39;] ,m.score[\u0026#39;nature\u0026#39;] from map1 m where m.score[\u0026#39;math\u0026#39;] \u0026gt; 35; #查看每个⼈的前两科的成绩总和 select m.name,m.score[\u0026#39;chinese\u0026#39;]+m.score[\u0026#39;math\u0026#39;] from map1 m; struct 类型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 zhangsan 90,87,63,76 lisi 60,30,78,0 wangwu 89,25,81,9 create table if not exists struct1( name string, score struct\u0026lt;chinese:int,math:int,english:int,natrue:int\u0026gt; ) row format delimited fields terminated by \u0026#39;\\t\u0026#39; collection items terminated by \u0026#39;,\u0026#39;; 导⼊数据： load data local inpath \u0026#39;./data/arr1.txt\u0026#39; into table struct1; select name,score.english,score.chinese from str2 where score.math \u0026gt; 35; array 类型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 zhangsan 78,89,92,96 lisi 67,75,83,94 王五 23,12 create table if not exists arr1( name string, scores array\u0026lt;string\u0026gt; ) row format delimited fields terminated by \u0026#39;\\t\u0026#39; collection items terminated by \u0026#39;,\u0026#39;; load data local inpath \u0026#39;/data/arr1.txt\u0026#39; into table arr1; select * from arr1; #结果 +--------+---------------+ |name |scores | +--------+---------------+ |zhangsan|[\u0026#34;78,89,92,96\u0026#34;]| |lisi |[\u0026#34;67,75,83,94\u0026#34;]| |王五 |[\u0026#34;23,12\u0026#34;] | +--------+---------------+ 库操作 创建库\n1 2 3 4 CREATE DATABASE [IF NOT EXISTS] database_name [COMMENT database_comment] [LOCATION hdfs_path] [WITH DBPROPERTIES (property_name=property_value, ...)]; 查询库\n1 2 3 SHOW DATABASES [LIKE \u0026#39;identifier_with_wildcards\u0026#39;]; #注：like通配表达式说明：*表示任意个任意字符，|表示或的关系。 查看数据库信息\n1 DESCRIBE DATABASE [EXTENDED] db_name; 修改数据库\n1 2 3 4 5 6 7 8 9 10 11 12 13 用户可以使用alter database命令修改数据库某些信息，其中能够修改的信息包括dbproperties、location、owner user。需要注意的是：修改数据库location，不会改变当前已有表的路径信息，而只是改变后续创建的新表的默认的父目录。 --修改dbproperties ALTER DATABASE database_name SET DBPROPERTIES (property_name=property_value, ...); --修改location ALTER DATABASE database_name SET LOCATION hdfs_path; --修改owner user ALTER DATABASE database_name SET OWNER USER user_name; 删除数据库\n1 2 3 4 5 DROP DATABASE [IF EXISTS] database_name [RESTRICT|CASCADE]; RESTRICT：严格模式，若数据库不为空，则会删除失败，默认为该模式。 CASCADE：级联模式，若数据库不为空，则会将库中的表一并删除。 切换当前数据库\n1 USE database_name; 表操作 表有临时表、外部表、内部表（管理表）、分区表，分桶表\nLinux 上传文件到 hdfs 上\nhdfs dfs -put student.txt 基于其他表的结构建表\ncreate table teacher2 like teacher; 数据导入导出 导入文件到 hive 表\nLOAD DATA [LOCAL] INPATH \u0026lsquo;filepath\u0026rsquo; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 \u0026hellip;)];\n关键字说明：\n1）local：表示从本地加载数据到 Hive 表；否则从 HDFS 加载数据到 Hive 表。\n2）overwrite：表示覆盖表中已有数据，否则表示追加。\n3）partition：表示上传到指定分区，若目标是分区表，需指定分区。\n注意！分桶表不能 load data，可以先 load data 到临时表，再从临时表 insert from 到分桶表导入表到表\n1）create table tbname as select * from tbname;\n2）insert overwrite table tbname select * from tbname;\n数据导出\nINSERT (INTO | OVERWRITE) TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 \u0026hellip;)] select_statement;\n关键字说明：\n1）INTO：将结果追加到目标表\n2）OVERWRITE：用结果覆盖原有数据\n1 2 3 4 5 6 7 8 9 10 #导出数据到本地⽂件系统的⽬录下 insert overwrite local directory \u0026#39;/root/out/00\u0026#39; select * from student; #导出数据到hdfs的⽬录下 insert overwrite directory \u0026#39;/root/out/01\u0026#39; select * from student; #导出的⽂件中字段默认不分隔，修改导出后的列与列之间的格式： insert overwrite local directory \u0026#39;/root/out/01\u0026#39; row format delimited fields terminated by \u0026#39;,\u0026#39;; select * from student; Export\u0026amp;Import\nExport 导出语句可将表的数据和元数据信息一并到处的 HDFS 路径，Import 可将 Export 导出的内容导入 Hive，表的数据和元数据信息都会恢复。Export 和 Import 可用于两个 Hive 实例之间的数据迁移。\nEXPORT TABLE tablename TO \u0026rsquo;export_target_path\u0026rsquo;\nIMPORT TABLE new_or_original_tablename FROM \u0026lsquo;source_path\u0026rsquo; [LOCATION \u0026lsquo;import_target_path\u0026rsquo;]\nROW FORMAT DELIMITED 参数\n[FIELDS TERMINATED BY char]\n#列分隔符\n[COLLECTION ITEMS TERMINATED BY char]\n#map、struct、array 元素间的分隔符\n[MAP KEYS TERMINATED BY char]\n#map 中 key 和 value 的分隔符\n[LINES TERMINATED BY char]\n#行分隔符\nSTORED AS 参数\n指定文件格式，常用的文件格式有，textfile（默认值），sequence file，orc file、parquet file 等等。\n创建表 临时表 1 CREATE [TEMPORARY] table if not exist tb_name; 内部表 1 2 3 4 5 6 7 create table if not exists student( id int, name string ) row format delimited fields terminated by \u0026#39;\\t\u0026#39; location \u0026#39;/user/hive/warehouse/student\u0026#39;; #指定表的数据位置，表删除后，文件也会删除 外部表 1 2 3 4 5 6 7 create external table if not exists student( id int, name string ) row format delimited fields terminated by \u0026#39;\\t\u0026#39; location \u0026#39;/user/hive/warehouse/student\u0026#39;; #表删除后，文件不会删除 内外部表转换 1 2 3 4 5 6 7 #内部表转外部表 alter table tableName set tblproperties(\u0026#39;EXTERNAL\u0026#39;=\u0026#39;TRUE\u0026#39;); 注意：内部表转外部表，True⼀定要⼤写; #外部表转内部表 alter table tableName set tblproperties(\u0026#39;EXTERNAL\u0026#39;=\u0026#39;false\u0026#39;); 说明：false不区分⼤⼩ 分区表 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #一级分区 create table if not exists part1( id int,name string,age int) partitioned by (dt string) row format delimited fields terminated by \u0026#39;,\u0026#39; lines terminated by \u0026#39;\\n\u0026#39;; #导入数据 load data local inpath \u0026#39;./data/user1.txt\u0026#39; into table test_ext partition(dt=\u0026#39;2020-05-05\u0026#39;); #二级分区 create table if not exists part2( id int,name string,age int) partitioned by (year string,month string) row format delimited fields terminated by \u0026#39;,\u0026#39;; #导入数据 load data local inpath \u0026#39;./data/user1.txt\u0026#39; into table part2 partition(year=\u0026#39;2020\u0026#39;,month=\u0026#39;03\u0026#39;); #三级分区 create table if not exists part3( id int,name string,age int) partitioned by (year string,month string,day string) row format delimited fields terminated by \u0026#39;,\u0026#39;; #导入数据 load data local inpath \u0026#39;./data/user1.txt\u0026#39; into table part3 partition(year=\u0026#39;2020\u0026#39;,month=\u0026#39;05\u0026#39;,day=\u0026#39;01\u0026#39;); 1 2 3 #添加分区 ALTER TABLE tablename ADD PARTITION (partition_key=\u0026#39;partition_value\u0026#39;, ......); 1 2 3 4 #删除分区 ALTER TABLE tablename DROP PARTITION (partition_key=\u0026#39;partition_value\u0026#39;); #只是删除元数据，hdfs文件依然保留 1 2 3 4 5 #修改分区值 ALTER TABLE tablename PARTITION (partition_key=\u0026#39;old_partition_value\u0026#39;) RENAME TO PARTITION (partition_key=\u0026#39;new_partition_value\u0026#39;); 注意！\n修改分区只会在元数据中修改，不会同步修改 HDFS 路径，如：\n1）原分区路径为： /user/hive/warehouse/test.db/test_table/month=201910 ，分区名： month=\u0026lsquo;201910\u0026rsquo;\n2）将分区名修改为：201911 后，分区所在路径不变，依旧 是：/user/hive/warehouse/test.db/test_table/month=201910\n如果希望修改分区名后，同步修改 HDFS 的路径，并保证正常可用，需要：\n1）在元数据库中： 找到 SDS 表 -\u0026gt; 找到 LOCATION 列 -\u0026gt; 找到对应分区的路径记录进行修改 如将记录的： /user/hive/warehouse/test.db/test_table/month=201910 修改 为： /user/hive/warehouse/test.db/test_table/month=201911\n2）在 HDFS 中，同步修改文件夹名\n如将文件夹： /user/hive/warehouse/test.db/test_table/month=201910 修改为： /user/hive/warehouse/test.db/test_table/month=201911\n分桶表 分桶表不能使用 load data ！！！只能 insert .. select ..from\n原因：分桶表是要把数据划分成 n 份，划分规则就是基于分桶列的值进行 hash 取模来决定，由于 load data 不会触发 mapreduce，没有计算过程，也就是无法执行 hash 算法，只是简单的移动数据。\nHash 取模算法：\n同样的值在 hash 加密后的结果是一致的，不论计算多少次都不会改变。基于这个特征，\n**假如规定把表划分成 3 个桶，那么对表的分桶列的字段名进行 hash 计算，产生的值再%3 取模，取模的结果必然是 0 或 1 或 2，基于取模的结果，把取模结果相同的放到同一个桶中。**取模相同的一定在同一文件，同一文件的未必取模都相同。\n1 2 3 4 5 6 7 8 9 10 CREATE TABLE course (c_id string,c_name string,t_id string) [PARTITION(partition_key=\u0026#39;partition_value\u0026#39;)] CLUSTERED BY(c_id) INTO 3 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026#39;\\t\u0026#39;; #分桶表需要开启： set hive.enforce.bucketing=true; #设置自动匹配桶数量的reduces task数量 #分桶表不能使用load data 查询表 1 2 3 4 5 6 7 8 9 select .. from .. join [tableName] on .. where .. group by .. having .. order by .. sort by .. limit .. union | union all ... limit\nselect * from emp limit 2,3; \u0026ndash; 表示从第 2 行开始，向下抓取 3 行\nJOIN 由图可知，**左外连接（left join）**就是以左表为主 ，左表多少行，连接的结果表就多少行，**右外连接（right join）**同理。**满外连接（full outer join）**就是左表中有，右表没有的，右表补为 NULL，右表中有，左表没有，左表补为 NULL，类似于求并集。**内连接（inner join）**就是求交集，找两张表都有的\n修改表 重命名表\nALTER TABLE table_name RENAME TO new_table_name\n增加列\n该语句允许用户增加新的列，新增列的位置位于末尾。\nALTER TABLE table_name ADD COLUMNS (col_name data_type [COMMENT col_comment], \u0026hellip;)\n更新列\n该语句允许用户修改指定列的列名、数据类型、注释信息以及在表中的位置。\nALTER TABLE table_name CHANGE COLUMN col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]\n替换列\n该语句允许用户用新的列集替换表中原有的全部列。\nALTER TABLE table_name REPLACE COLUMNS (col_name data_type [COMMENT col_comment], \u0026hellip;)\n删除表 DROP TABLE [IF EXISTS] table_name;\n清空表 TRUNCATE [TABLE] table_name\n注意：truncate 只能清空管理表，不能删除外部表中数据。\n特例-update 和 delete 更新和删除的语法比较简单，和关系型数据库一致。需要注意的是这两个操作都只能在支持 ACID 的表，也就是事务表上才能执行。\n-- 更新\nUPDATE tablename SET column = value [, column = value \u0026hellip;] [WHERE expression]\n--删除\nDELETE FROM tablename [WHERE expression]\n需要配置 hive-site.xml，开启事务支持，配置完成后重启 hive。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.support.concurrency\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.enforce.bucketing\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.exec.dynamic.partition.mode\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;nonstrict\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.txn.manager\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.apache.hadoop.hive.ql.lockmgr.DbTxnManager\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.compactor.initiator.on\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.in.test\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; 视图 Hive 中的视图和 RDBMS 中视图的概念一致，都是一组数据的逻辑表示，本质上就是一条 SELECT 语句 的结果集。视图是纯粹的逻辑对象，没有关联的存储 (Hive 3.0.0 引入的物化视图除外)，当查询引用视 图时，Hive 可以将视图的定义与查询结合起来，例如将查询中的过滤器推送到视图中。\n在 Hive 中可以使用 CREATE VIEW 创建视图，如果已存在具有相同名称的表或视图，则会抛出异常， 建议使用 IF NOT EXISTS 预做判断。在使用视图时候需要注意以下事项：\n视图是只读的，不能用作 LOAD / INSERT / ALTER 的目标； 在创建视图时候视图就已经固定，对基表的后续更改（如添加列）将不会反映在视图； 删除基表并不会删除视图，需要手动删除视图； 视图可能包含 ORDER BY 和 LIMIT 子句。如果引用视图的查询语句也包含这类子句，其执行优先 级低于视图对应字句。例如，视图 custom_view 指定 LIMIT 5，查询语句为 select * from custom_view LIMIT 10 ，此时结果最多返回 5 行。 创建视图时，如果未提供列名，则将从 SELECT 语句中自动派生列名； 视图增删改 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 创建视图 CREATE VIEW [IF NOT EXISTS] [db_name.]view_name -- 视图名称 [(column_name [COMMENT column_comment], ...) ] --列名 [COMMENT view_comment] --视图注释 [TBLPROPERTIES (property_name = property_value, ...)] --额外信息 AS SELECT ...; 查看视图 -- 查看所有视图： 没有单独查看视图列表的语句，只能使用 show tables show tables; -- 查看某个视图 desc view_name; -- 查看某个视图详细信息 desc formatted view_name; 删除视图 DROP VIEW [IF EXISTS] [db_name.]view_name; -- 删除视图时，如果被删除的视图被其他视图所引用，这时候程序不会发出警告，但是引用该视图其他视 图已经失效，需要进行重建或者删除。 修改视图 ALTER VIEW [db_name.]view_name AS select_statement; -- 被更改的视图必须存在，且视图不能具有分区，如果视图具有分区，则修改失败。 修改视图属性 ALTER VIEW [db_name.]view_name SET TBLPROPERTIES (property_name = property_value, property_name = property_value, ...); 示例： ALTER VIEW custom_view SET TBLPROPERTIES (\u0026#39;create\u0026#39;=\u0026#39;heibaiying\u0026#39;,\u0026#39;date\u0026#39;=\u0026#39;2019-05- 05\u0026#39;); 索引 Hive 在 0.7.0 引入了索引的功能，索引的设计目标是提高表某些列的查询速度。如果没有索引，带有谓 词的查询（如\u0026rsquo;WHERE table1.column = 10\u0026rsquo;）会加载整个表或分区并处理所有行。但是如果 column 存在索引，则只需要加载和处理文件的一部分。\n索引原理 在指定列上建立索引，会产生一张索引表（表结构如下），里面的字段包括：索引列的值、该值对应的 HDFS 文件路径、该值在文件中的偏移量。在查询涉及到索引字段时，首先到索引表查找索引列值对应 的 HDFS 文件路径及偏移量，这样就避免了全表扫描。 索引增删改 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 创建索引 CREATE INDEX index_name --索引名称 ON TABLE base_table_name (col_name, ...) --建立索引的列 AS index_type --索引类型 [WITH DEFERRED REBUILD] --重建索引 [IDXPROPERTIES (property_name=property_value, ...)] --索引额外属性 [IN TABLE index_table_name] --索引表的名字 [ [ ROW FORMAT ...] STORED AS ... | STORED BY ... ] --索引表行分隔符 、 存储格式 [LOCATION hdfs_path] --索引表存储位置 [TBLPROPERTIES (...)] --索引表表属性 [COMMENT \u0026#34;index comment\u0026#34;]; --索引注释 查看索引 --显示表上所有列的索引 SHOW FORMATTED INDEX ON table_name; 删除索引 -- 删除索引会删除对应的索引表。 DROP INDEX [IF EXISTS] index_name ON table_name; -- 如果存在索引的表被删除了，其对应的索引和索引表都会被删除。如果索引表的某个分区被删除了， 那么分区对应的分区索引也会被删除。 重建索引 ALTER INDEX index_name ON table_name [PARTITION partition_spec] REBUILD; -- 如果指定了 PARTITION，则仅重建该分区的索引。 查看索引 show index on tb_name; 设置自动使用索引 默认情况下，虽然建立了索引，但是 Hive 在查询时候是不会自动去使用索引的，需要开启相关配置。 开启配置后，涉及到索引列的查询就会使用索引功能去优化查询。\nSET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;\nSET hive.optimize.index.filter=true;\nSET hive.optimize.index.filter.compact.minsize=0;\n索引缺陷 索引表最主要的一个缺陷在于：索引表无法自动 rebuild，这也就意味着如果表中有数据新增或删除， 则必须手动 rebuild，重新执行 MapReduce 作业，生成索引表数据。 同时按照官方文档 的说明，Hive 会从 3.0 开始移除索引功能，主要基于以下两个原因：\n具有自动重写的物化视图 (Materialized View) 可以产生与索引相似的效果（Hive 2.3.0 增加了对 物化视图的支持，在 3.0 之后正式引入）。\n使用列式存储文件格式（Parquet，ORC）进行存储时，这些格式支持选择性扫描，可以跳过不需 要的文件或块。\n","date":"2024-04-14T21:41:05Z","permalink":"/zh-cn/post/2024/04/hive%E5%85%A5%E9%97%A8-hql%E8%A1%A8%E6%93%8D%E4%BD%9C%E5%BA%93%E6%93%8D%E4%BD%9C%E8%A7%86%E5%9B%BE%E7%B4%A2%E5%BC%95%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/","title":"Hive入门-HQL、表操作、库操作、视图、索引、数据类型"},{"content":"Hive 本质 Hive 是构建在 hadoop 上的数据仓库，也可以说是一个操作 hdfs 文件 的客户端，它可以将结构化的数据文件映射成表，并提供类 SQL 查询功能，用于查询的 SQL 语句会被转化为 MapReduce 作业，然后提交到 Hadoop 上运行。Hive 执行引擎可以是 MapReduce、Spark、Tez，如果是 MR，Hive 就会把 HQL 翻译成 MR 进行数据计算。\n由于 Hive 是针对数据仓库应⽤设计的，⽽数据仓库的内容是读多写少的。因此，Hive 中不⽀持 对数据的改写和添加，所有的数据都是在加载的时候中确定好的。\nHive 不适合⽤于联机事务处理(OLTP)，也不提供实时查询功能。它最适合应⽤在基于⼤量不可变数据的批处理 作业。Hive 的特点是可伸缩（在 Hadoop 的集群上动态的添加设备），可扩展、容错、输⼊格式的松散耦合。 Hive 的⼊⼝是 DRIVER ，执⾏的 SQL 语句⾸先提交到 DRIVER 驱动，然后调 COMPILER 解释驱动，最终解释成 MapReduce 任务执⾏，最后将结果返回。\n优点：\n简单、容易上手 (提供了类似 sql 的查询语言 hql)，使得精通 sql 但是不了解 Java 编程的人也能很 好地进行大数据分析；\n灵活性高，可以自定义用户函数 (UDF) 和存储格式；\n为超大的数据集设计的计算和存储能力，集群扩展容易;\n4.统一的元数据管理，可与 presto／impala／sparksql 等共享数据；\n执行延迟高，不适合做数据的实时处理，但适合做海量数据的离线处理。 Hive 主要有以下 3 个模块: ⽤户接⼝模块 含 CLI、HWI、JDBC、Thrift Server 等，⽤来实现对 Hive 的访问。CLI 是 Hive ⾃带 的命令⾏界⾯；HWI 是 Hive 的⼀个简单⽹⻚界⾯；JDBC、ODBC 以及 Thrift Server 可向⽤户提供进 ⾏编程的接⼝，其中 Thrift Server 是基于 Thrift 软件框架开发的，提供\nHive 的 RPC 通信接⼝ 驱动模块（Driver）：含编译器 compiler、优化器 optimizer、执⾏器 executor 等，负责把 HiveQL 语句转换成⼀系列 MR 作业， 所有命令和查询都会进⼊驱动模块，通过该模块的解析变异，对计算过程进⾏优化，然后按照指定 的步骤执⾏。\n元数据存储模块（Metastore） 是⼀个独⽴的关系型数据库，通常与 MySQL 数据库连接后创建的 ⼀个 MySQL 实例，也可以是 Hive ⾃带的 Derby 数据库实例。此模块主要保存表模式和其他系统元数 据，如表的名称、表的列及其属性、表的分区及其属性、表的属性、表中数据所在位置信息等。\n1 metastore 是 Hive 最重要的部件，在 Hive 中，表名、表结构、字段名、字段类型、表的分隔符等统一被称为元数据。所有的元数据默认存储在 Hive 内置的 derby 数据库中，但由于 derby 只能有一个实例，也就是说不能有多个命令行客户端同时访问，所以在实际生产环境中，通常使用 MySQL 中的自建数据库代替 derby。Hive 进行的是统一的元数据管理，就是说你在 Hive 上创建了一张表，然后在 presto、impala、sparksql 中都是可以直接使用的，它们会从 Metastore 中获取统一的元数据信息，同样的你在 presto、impala、sparksql 中创建一张表，在 Hive 中也可以直接使用。 hive 创建的内部表，默认放在 hdfs 的/usr/hive/warehouse 文件夹下 可以看到 db_msg.db、myhive.db 是数据库，其他的是表，而这些表创建时默认放在另一个 default 库中只是在 hdfs 中没有显示，在 hive 中才能显示出来。由此可见 hive 的表和库其实就是一个个 hdfs 文件夹，表和库可以是并列同级关系。表有内外之分，创建时默认是内部表，而 external_stu1 是外部表，外部表和内部表的区别就在于外部表只是把 hdfs 的文件数据和 hive 的表相关联，在 hive 中删除外部表，hdfs 的文件数据依然存在不会被删除，而删除内部表，表的文件数据和表本身会一同删除。 架构 Hive 日志配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 -- Hive中的日志分为两种 1. 系统日志，记录了hive的运行情况，错误状况。 2. Job 日志，记录了Hive 中job的执行的历史过程。 系统日志存储在什么地方呢 ？ 在hive/conf/hive-log4j.properties 文件中记录了Hive日志的存储情况， 如果没有hive-log4j.properties。那么需要找到该文件夹下的hive-log4j.properties.templete,这个是模板文件，运行mv命令把templete重命名成properties文件即可。 properties文件默认的存储情况： property.hive.root.logger=WARN,DRFA property.hive.log.dir=/tmp/${user.name} # 默认的存储位置,一般是/tmp/root，此处改成hive/logs property.hive.log.file=hive.log # 默认的文件名 Job日志又存储在什么地方呢 ？ //Location of Hive run time structured log file HIVEHISTORYFILELOC(\u0026#34;hive.querylog.location\u0026#34;, \u0026#34;/tmp/\u0026#34; + System.getProperty(\u0026#34;user.name\u0026#34;)), 默认存储与在/tmp/{user.name}目录下。但是我没找到。。。 properties 文件的日志存放目录修改之后如下： 日志目录是后来配置的，于是又把/tmp/root 目录下的 hive 日志手动移到了 hive/logs 下： HQL 执行过程 Hive 在执行一条 HQL 的时候，会经过以下步骤：\n语法解析：Antlr 定义 SQL 的语法规则，完成 SQL 词法，语法解析，将 SQL 转化为抽象 语法树 AST Tree； 语义解析：遍历 AST Tree，抽象出查询的基本组成单元 QueryBlock； 生成逻辑执行计划：遍历 QueryBlock，翻译为执行操作树 OperatorTree； 优化逻辑执行计划：逻辑层优化器进行 OperatorTree 变换，合并不必要的 * ReduceSinkOperator，减少 shuffle 数据量； 生成物理执行计划：遍历 OperatorTree，翻译为 MapReduce 任务； 优化物理执行计划：物理层优化器进行 MapReduce 任务的变换，生成最终的执行计划。 Hive 四种玩法： CLI 配置 hive 环境变量（通常是/etc/profile 文件）后，在任意目录下直接输入命令 hive 即可启动（或者 hive \u0026ndash;service cli），前提是要启动 hdfs（start-dfs.sh）和 hive 元数据服务（start-hivemetastore.sh 自己写的脚本配置到环境变量），因为 hive 就是操作 hdfs 的文件的。\n注意！！！ 注意第一行提到 Hive-on -MR is deprecated 在 2.x 版本已经废弃不推荐使用，后续都是 hive on spark （on Tez），但是 MapReduce 的 hive 优化还是建议学一下。 上面这种情况可能就是没启动元数据服务。 hive 通常是在集群环境中使用的，如果只启动了一台服务器，那么在启动 hive 时会报错，如下： name node 处于安全模式，服务器数量少于最小要求数量，这种情况要么等 18s 后重新启动 cli，要么启动第二台服务器并启动上面的 hdfs。\nHiveServer2 启动 hiveserver2 服务，提供 thrift 端口供其他客户连接，启动之后就可以使用 hive 之外的其他工具操作 hdfs 文件，比如 DBserver，IDEA 的数据库插件\n需要在 hdfs 的 core-site.xml 文件中加如下配置：\n1 2 3 4 5 6 7 8 9 10 11 \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.proxyuser.root.groups\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;*\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;允许root用户代理任何其他用户\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.proxyuser.root.hosts\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;*\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;允许代理任意服务器的请求\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; root也可以换成hadoop等其他用户，我这里设置成了超级用户root 任意目录下启动 hiveserver2（前台）或者切换到后台。 自己写的后台脚本，配置到环境变量中\n[root@linux01 bin]# cat start-hiveserver2.sh\n1 2 3 #!/bin/bash nohup $HIVE_HOME/bin/hive --service hiveserver2 \u0026gt;\u0026gt; $HIVE_HOME/logs/hiveserver2.log 2\u0026gt;\u0026amp;1 \u0026amp; #启动hiveserver2服务，提供thrift端口供其他客户连接 Beeline 启动 beeline 必须先启动 hiveserver2，启动 beeline 后，键入\n1 2 !connect jdbc:hive2://linux01:10000 并输入用户名密码即可，这里的登录用户可以是任意用户因为 hadoop 的 core-site.xml 设置了 root 用户可以代理任意用户。linux01 是我的服务器名。 Web UI 在 hive-site-xml 中添加 hive 配置。\n1 2 3 4 5 6 7 8 9 \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.webui.host\u0026lt;/name\u0026gt; \u0026lt;!--主机名或ip--\u0026gt; \u0026lt;value\u0026gt;linux01\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.webui.port\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;10002\u0026lt;/value\u0026gt; \u0026lt;/propert\u0026gt; 启动 hive，浏览器即可访问 10002 端口\n","date":"2024-04-14T12:23:06Z","permalink":"/zh-cn/post/2024/04/hive%E6%9C%AC%E8%B4%A8%E6%9E%B6%E6%9E%84%E7%8E%A9%E6%B3%95/","title":"Hive本质、架构、玩法"},{"content":"Yarn 和 MR 资源配置 配置项参考官网：https://apache.github.io/hadoop/\nYarn 资源配置 修改 yarn-site.xml,调整的 Yarn 参数均与 CPU、内存等资源有关，配置如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.resource.memory-mb\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;65536\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;一个NodeManager节点分配给Container使用的内存。该参数的配置，取决于NodeManager所在节点的总内存容量和该节点运行的其他服务的数量\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.resource.cpu-vcores\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;16\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;一个NodeManager节点分配给Container使用的CPU核数。该参数的配置，同样取决于NodeManager所在节点的总CPU核数和该节点运行的其他服务。\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.scheduler.maximum-allocation-mb\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;16384\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;单个Container能够使用的最大内存。\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.scheduler.minimum-allocation-mb\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;512\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;单个Container能够使用的最小内存。\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; 修改后重新分发该配置文件并重启 Yarn\nMR 资源配置 MapReduce 资源配置主要包括 Map Task 的内存和 CPU 核数，以及 Reduce Task 的内存和 CPU 核数。核心配置参数如下：\nmapreduce.map.memory.mb\n该参数的含义是，单个 Map Task 申请的 container 容器内存大小，其默认值为 1024。该值不能超出 yarn.scheduler.maximum-allocation-mb 和 yarn.scheduler.minimum-allocation-mb 规定的范围。\n该参数需要根据不同的计算任务单独进行配置，在 hive 中，可直接使用如下方式为每个 SQL 语句单独进行配置：set mapreduce.map.memory.mb=2048;\nmapreduce.map.cpu.vcores\n该参数的含义是，单个 Map Task 申请的 container 容器 cpu 核数，其默认值为 1。该值一般无需调整。如需调整要修改 mapred-site.xml 文件（mapred-default.xml）\nmapreduce.reduce.cpu.vcores\n该参数的含义是，单个 Reduce Task 申请的 container 容器 cpu 核数，其默认值为 1。该值一般无需调整。如需调整要修改 mapred-site.xml 文件（mapred-default.xml）\nmapreduce.reduce.memory.mb\n该参数的含义是，单个 Reduce Task 申请的 container 容器内存大小，其默认值为 1024。该值同样不能超出 yarn.scheduler.maximum-allocation-mb 和 yarn.scheduler.minimum-allocation-mb 规定的范围。\n该参数需要根据不同的计算任务单独进行配置，在 hive 中，可直接使用如下方式为每个 SQL 语句单独进行配置：set mapreduce.reduce.memory.mb=2048;\nExplain 查看执行计划 Explain 用于呈现 HQL 语句的详细执行步骤，由一系列 Stage 组成，简单的理解为 HQL 查询语句的不同执行阶段，这一系列 Stage 具有依赖关系，每个 Stage 对应一个 MapReduce Job 或一个文件系统操作等。\n若某个 Stage 对应的一个 MapReduce Job，则其 Map 端和 Reduce 端的计算逻辑分别由 Map Operator Tree 和 Reduce Operator Tree 进行描述，Operator Tree 由一系列的 Operator 组成，一个 Operator 代表在 Map 或 Reduce 阶段的一个单一的逻辑操作，例如 TableScan Operator，Select Operator，Join Operator 等。具体如下图：\n常见的 Operator 及其作用如下\nTableScan：表扫描操作，通常 map 端第一个操作肯定是表扫描操作\nSelect Operator：选取操作\nGroup By Operator：map 端的分组聚合操作，在后面的分组聚合中会讲到\nReduce Output Operator：输出到 reduce 操作\nFilter Operator：过滤操作\nJoin Operator：join 操作\nFile Output Operator：文件输出操作\nFetch Operator 客户端获取数据操作\nExplain 语法\nEXPLAIN [FORMATTED | EXTENDED | DEPENDENCY]\nFORMATTED：将执行计划以 JSON 字符串的形式输出 EXTENDED：输出执行计划中的额外信息，通常是读写的文件名等信息 DEPENDENCY：输出执行计划读取的表及分区 例：\nhive (default)\u0026gt;\nexplain formatted\nselect user_id,count(*) from order_detail group by user_id;\n分组聚合优化 分组聚合是通过 MR Job 实现的，map 端读取数据，并按照分组字段分区，通过 shuffle，把数据发到 reduce，各组数据在 reduce 端完成最终的聚合运算。\n分组聚合的优化主要围绕减少 shuffle 数据量进行，具体做法是 map-side 聚合。map-side 聚合是在 map 端维护一个 hash table，先利用其完成数据的部分聚合，再把聚合的结果按照分组字段分区，发到 reduce 端完成最终聚合，以此提高分组聚合运算效率。简而言之就是增加了一个 map 端的部分聚合过程，以减少 shuffle 的工作量，进而减少 reduce 端的聚合工作量。\nmap-side 聚合相关参数如下\n\u0026ndash;启用 map-side 聚合，默认是 true\nset hive.map.aggr=true;\n\u0026ndash;用于检测源表数据是否适合进行 map-side 聚合。检测的方法是：系统自动先对若干条数据进行 map-side 聚合，若聚合后的条数和聚合前的条数比值小于该值，则认为该表适合进行 map-side 聚合；否则，认为该表数据不适合进行 map-side 聚合，后续数据便不再进行 map-side 聚合。0.5 意味着平均有 2 条数据可以聚合成 1 条，1 意味着没有出现任何的聚合\nset hive.map.aggr.hash.min.reduction=0.5;\n\u0026ndash;用于hive.map.aggr.hash.min.reduction=0.5 检测源表是否适合 map-side 聚合的条数。\nset hive.groupby.mapaggr.checkinterval=100000;\n\u0026ndash;map-side 聚合所用的 hash table 占用 map task 堆内存的最大比例，若超出该值，则会对 hash table 进行一次 flush。\nset hive.map.aggr.hash.force.flush.memory.threshold=0.7;\n优化前 VS 优化后 set hive.map.aggr=false 关闭分组聚合优化，查看执行效果，在 Map 端没有了 Group By Operator\nset hive.map.aggr=true 开启分组聚合优化，查看执行效果，在 Map 端有了 Group By Operator，\n若发生 map-side 优化，优化后比优化前的 HQL 执行耗时应该有所减少，且 map 的 output 数量明显小于 input 数量。\n若没有触发 map-side，则 map 的 output 数量虽然比 input 数量有所减少但可以忽略不计。具体有没有触发 map-side 可以去 web UI 界面查看 map 日志。\n注意！！map-side 聚合不够智能，即 map 端的分组聚合是否执行一定程度上会受到分组字段在表中存储的位置和分布的影响，这是底层存储问题，未必是因为数据真的不适合分组聚合。要解决此问题可以提前对数据分区分桶，使用分区分桶表，使得同一区域存储的数据分布具有一定的相似性，这样聚合结果会有所提升。\n例：\n1）select province_id,count(*) from order_detail group by province_id;\n该语句查询所有订单，根据省份 id 分组聚合，省份只有 34 个，这样 map 后的数据应该只有 34 条，所以聚合结果是应该是比较可观的。所以 group by 的基数越小，一般越适合聚合。\n2）select product_id,count(*) from order_detail group by product_id;\n若 product_id 这一分组字段在 order_detail 表中分布比较散，那么可能会导致 hive 在表中切片抽样进行 map-side 检测的时候测试聚合结果\u0026gt;0.5，那么最终就没有使用 map-side 聚合。所以说如果能保证抽样数据的测试结果\u0026lt;=0.5，就会实现分组聚合，当然也可以调整hive.map.aggr.hash.min.reduction 的值以提高 map-side 的命中率。\n若 100w 的数据集分组聚合之后的输出\u0026gt;100w,可能的原因是多次触发了 hash table 的 flush\nJoin 优化 Join 优化就是控制 HQL 语句走哪种 join 算法，这些 join 算法有的快，有的慢，有的激进，有的保守。我们要做的就是让 HQL 走最适合自己的 join 算法。\nCommon Join(普通 join) 原理 hive 中最稳定的 join 算法，其通过一个 MapReduce Job 完成一个 join 操作。Map 端负责读取 join 操作所需表的数据，并按照关联字段进行分区，通过 Shuffle，将其发送到 Reduce 端，相同 key 的数据在 Reduce 端完成最终的 Join 操作。\n需要注意的是，HQL 语句中的 join 操作和执行计划中的 Common Join 任务并非一对一的关系，即 HQL 中的 A 表 join B 表 join C 表在 common join 中未必也是两个 join 操作，一个 HQL 语句中的相邻的且关联字段相同的多个 join 操作可以合并为一个 Common Join 任务。\n例： 1）hive (default)\nselect a.val, b.val, c.val from\na join b on (a.key = b.key1) join c on (c.key = b.key1)\n上述 sql 语句中两个 join 操作的关联字段均为 b 表的 key1 字段，则该语句中的两个 join 操作可由一个 Common Join 任务实现，也就是可通过 1 个 Map Reduce 任务实现。\n2）hive (default)\u0026gt; select a.val, b.val, c.val from\na join b on (a.key = b.key1) join c on (c.key = b.key2)\n上述 sql 语句中的两个 join 操作关联字段各不相同，则该语句的两个 join 操作需要各自通过一个 Common Join 任务实现，也就是通过 2 个 Map Reduce 任务实现。\nMap Join 原理 Map Join 算法可以通过一个 MR 和一个 MapJoin 阶段完成一个 join 操作，省去了 shuffle 和 reduce，在第二个 map 阶段进行表的 join，不需要进入 reduce 阶段。其适用场景为大表 join 小表。第一个 Job 会读取小表数据，将其制作为 hash table，并上传至 Hadoop 分布式缓存（本质上是上传至 HDFS）。第二个 Job 会先从分布式缓存中读取小表数据，并缓存在 Map Task 的内存中，然后扫描大表数据，这样在 map 端即可完成关联操作。如下图所示： mapreduce local task 是本地任务，读取小表数据，因为小表数据占用内存资源少，所以不上传到 yarn，直接在本地读取效率更高 ，读取后序列化生成 hash table 并上传到 hdfs 的 cache 中。\n其中 Mapper 是实现 Map 阶段功能的代码组件。它接受原始数据作为输入，执行某种转换操作，然后输出一组键值对。这些键值对会作为 Reduce 阶段的输入。\n例：SELECT a.key, a.value FROM a JOIN b ON a.key = b.key\n前提 b 表是一张小表，具体小表有多小，由参数 hive.mapjoin.smalltable.filesize 来决定，默认值是 25M。\n参数列表：\n1）小表自动选择 Mapjoin\nset hive.auto.convert.join=true;\n默认值：false。该参数为 true 时，Hive 自动对左边的表统计量，若是小表就加入内存，即对小表使用 Map join 2）小表阀值 set hive.mapjoin.smalltable.filesize=25000000; ?默认值：25M\n优化 法一：hint 提示 手动指定通过 map join 算法，该方式已经过时，不推荐使用。\nhive (default)\u0026gt; select /_+ map join(ta) _/\nta.id, tb.id from table_a ta join table_b tb on ta.id=tb.id;\n法二：自动触发 Hive 在编译 HQL 语句阶段，起初所有的 join 操作均采用 Common Join 算法实现。\n之后在物理优化阶段，Hive 会根据每个 Common Join 任务所需表的大小判断该 Common Join 任务是否能够转换为 Map Join 任务，若满足要求（小表大小\u0026lt;指定的阈值），便将 Common Join 任务自动转换为 Map Join 任务。\n但有些 Common Join 任务所需的表大小，在 HQL 的编译阶段是未知的（例如对子查询进行 join 操作），所以这种 Common Join 任务是否能转换成 Map Join 任务在编译阶是无法确定的。\n针对这种情况，Hive 会在编译阶段生成一个条件任务（Conditional Task），其下会包含一个计划列表，计划列表中包含转换后的 Map Join 任务以及原有的 Common Join 任务。最终具体采用哪个计划，是在运行时决定的。大致思路如下图所示： Map join 自动转换的具体判断逻辑如下图所示： 图片详情看尚硅谷 P135\n寻找大表候选人时还不知道每张表的大小，那么选择规则是看 join 方式，有 innner join、left join、right join 等等。\ninner join：每个表都可能是大表候选人。\nleft join：默认左表为大表候选人，右表当作小表，这样小表会缓存到内存中，以大表为主，从大表中一条条 join 内存中的小表，如果反过来把大表缓存到内存中，以小表为主，从小表中一条条 join 内存中的大表，若出现大表有该字段而小表没有的情况，这种情况下就会出现大量数据 join 失败，小表数据少，大表数据多，那么会因为小表浪费很多数据，所以通常是左表为大表，右表为小表。\nright join：左表当作小表，右表为大表候选人。\nfull outer join：找不到大表候选人，因为全外联要返回两个表的全部数据，两个表都要去遍历，就无法 map join 优化。\n涉及参数：\n\u0026ndash;启动 Map Join 自动转换\nset hive.auto.convert.join=true;\n\u0026ndash;一个 Common Join operator 转为 Map Join operator 的判断条件：若该 Common Join 相关的表中,把每一个表都当作大表候选人，若除大表之外的任意一张已知大小的表的大小\u0026gt;大表候选人，则该组合不成立，不生成 map join，反之生成一个 Map Join 计划。此时可能存在多种组合均满足该条件,则 hive 会为每种满足条件的组合均生成一个 Map Join 计划,同时还会保留原有的 Common Join 计划作为后备(back up)计划,实际运行时,优先执行 Map Join 计划，若不能执行成功，则启动 Common Join 后备计划。\nset hive.mapjoin.smalltable.filesize=250000;\n\u0026ndash;开启无条件转 Map Join\nset hive.auto.convert.join.noconditionaltask=true; -无条件转 Map Join 时的小表之和阈值,若一个 Common Join operator 相关的表中，存在 n-1 张表的大小总和\u0026lt;=该值,此时 hive 便不会再为每种 n-1 张表的组合均生成 Map Join 计划,同时也不会保留 Common Join 作为后备计划。而是只生成一个最优的 Map Join 计划。 set hive.auto.convert.join.noconditionaltask.size=10000000;\n优化案例 hive (default)\u0026gt; select * from order_detail od\njoin product_info product on od.product_id = product.id\njoin province_info province on od.province_id = province.id;\n优化前\n上述 SQL 语句共有三张表进行两次 join 操作，且两次 join 操作的关联字段不同。故优化前的执行计划应该包含两个 Common Join operator，也就是由两个 MapReduce 任务实现。执行计划如下图所示： 优化思路\n使用如下语句获取表/分区的大小信息：\nhive (default)\u0026gt;\ndesc formatted table_name partition(partition_col=\u0026lsquo;partition\u0026rsquo;);\n经分析，参与 join 的三张表，数据量如下：\n方案一：\n启用 Map Join 自动转换。\nset hive.auto.convert.join=true;\n不使用无条件转 Map Join。\nset hive.auto.convert.join.noconditionaltask=false;\n调整 hive.mapjoin.smalltable.filesize 参数，使其大于等于 product_info。\nset hive.mapjoin.smalltable.filesize=25285707;\n这样可保证将两个 Common Join operator 均可转为 Map Join operator，并保留 Common Join 作为后备计划，保证计算任务的稳定。调整完的执行计划如下图： 方案二：\n启用 Map Join 自动转换。\nset hive.auto.convert.join=true;\n使用无条件转 Map Join。\nset hive.auto.convert.join.noconditionaltask=true;\n调整 hive.auto.convert.join.noconditionaltask.size 参数，使其大于等于 product_info 和 province_info 之和。\nset hive.auto.convert.join.noconditionaltask.size=25286076;\n这样可直接将两个 Common Join operator 转为两个 Map Join operator，并且由于两个 Map Join operator 的小表大小之和小于等于 hive.auto.convert.join.noconditionaltask.size，故两个 Map Join operator 任务可合并为同一个。这个方案计算效率最高，但需要的内存也是最多的。\n调整完的执行计划如下图： 方案三：\n启用 Map Join 自动转换。\nset hive.auto.convert.join=true;\n使用无条件转 Map Join。\nset hive.auto.convert.join.noconditionaltask=true;\n调整 hive.auto.convert.join.noconditionaltask.size 参数，使其等于 product_info。\nset hive.auto.convert.join.noconditionaltask.size=25285707;\n这样可直接将两个 Common Join operator 转为 Map Join operator，但不会将两个 Map Join 的任务合并。该方案计算效率比方案二低，但需要的内存也更少。 调整完的执行计划如下图： Bucket Map Join 原理 Bucket Map Join 是对 Map Join 算法的改进，其打破了 Map Join 只适用于大表 join 小表的限制，可用于大表 join 大表的场景。分桶其实就是把大表化成了“小表”，然后 Map-Side Join 解决。\nBucket Map Join 的核心思想是：若能保证参与 join 的表均为分桶表，且关联字段为分桶字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍，就能保证参与 join 的两张表的分桶之间具有明确的关联关系，所以就可以在两表的分桶间进行 Map Join 操作了。这样一来，第二个 Job 的 Map 端就无需再缓存小表的全表数据了，而只需缓存其所需的分桶即可。其原理如图所示： 第一个 map 对较小的表 tableB 的每个 bucket 序列化成 hash table，上传到 hdfs cache 中，第二个 map 对较大的表 tableA 的每个桶单独切片，有几个桶就有几个 mapper\n优化 hint 提示 Bucket Map Join 不支持自动转换，啊！原来是 hive 团队在 hive2.x 已经放弃维护 MR 计算引擎，建议使用 spark 等计算引擎（看到这乐死我了 tmd 白学了）。 参数：\n\u0026ndash;关闭 cbo 优化，cbo 会导致 hint 信息被忽略\nset hive.cbo.enable=false;\n\u0026ndash;map join hint 默认会被忽略(因为已经过时)，需将如下参数设置为 false\nset hive.ignore.mapjoin.hint=false;\n\u0026ndash;启用 bucket map join 优化功能\nset hive.optimize.bucketmapjoin = true;\n优化案例 hive (default)\u0026gt; select _ from( select _ from order_detail where dt=\u0026lsquo;2020-06-14\u0026rsquo;) od\njoin( select * from payment_detail where dt=\u0026lsquo;2020-06-14\u0026rsquo;) pd on od.id=pd.order_detail_id;\n优化前\n上述 SQL 语句共有两张表一次 join 操作，故优化前的执行计划应包含一个 Common Join 任务，通过一个 MapReduce Job 实现。执行计划如下图所示： 优化思路\n经分析，参与 join 的两张表，数据量如下。 两张表都相对较大，若采用普通的 Map Join 算法，则 Map 端需要较多的内存来缓存数据，可以选择为 Map 段分配更多的内存，来保证任务运行成功。但是，Map 端的内存不可能无上限的分配，所以当参与 Join 的表数据量均过大时，可以考虑采用 Bucket Map Join 算法。\n创建两个分桶表，order_detail 建议分 16 个 bucket，payment_detail 建议分 8 个 bucket,注意分桶个数的倍数关系以及分桶字段。然后向其中导入数据。\n设置优化参数：\n\u0026ndash;关闭 cbo 优化，cbo 会导致 hint 信息被忽略，需将如下参数修改为 false\nset hive.cbo.enable=false;\n\u0026ndash;map join hint 默认会被忽略(因为已经过时)，需将如下参数修改为 false\nset hive.ignore.mapjoin.hint=false;\n\u0026ndash;启用 bucket map join 优化功能,默认不启用，需将如下参数修改为 true\nset hive.optimize.bucketmapjoin = true;\n重写 SQL 语句：\nhive (default)\u0026gt;\nselect /_+ mapjoin(pd) _/ * from order_detail_bucketed od\njoin payment_detail_bucketed pd on od.id = pd.order_detail_id;\n执行结果如下： 使用\nhive (default)\u0026gt;\nexplain extended select /_+ mapjoin(pd) _/ *\nfrom order_detail_bucketed od\njoin payment_detail_bucketed pd on od.id = pd.order_detail_id;查看执行计划，在 Map Join Operator 中看到 “BucketMapJoin: true”\nSort Merge Bucket Map Join(SMB map join) 原理 SMB Map Join 基于 Bucket Map Join。SMB Map Join 要求，参与 join 的表均为分桶表，且需保证分桶内的数据是有序的，且分桶字段、排序字段和关联字段为相同字段，且其中一张表的分桶数量是另外一张表分桶数量的整数倍。\nSMB Map Join 同 Bucket Join 一样，同样是利用两表各分桶之间的关联关系，在分桶之间进行 join 操作，不同的是，分桶之间的 join 操作的实现原理。Bucket Map Join，两个分桶之间的 join 实现原理为 Hash Join 算法；而 SMB Map Join，两个分桶之间的 join 实现原理为 Sort Merge Join 算法。\nHash Join 和 Sort Merge Join 均为关系型数据库中常见的 Join 实现算法。Hash Join 的原理相对简单，就是对参与 join 的一张表构建 hash table，然后扫描另外一张表，然后进行逐行匹配。Sort Merge Join 需要在两张按照关联字段排好序的表中进行，其原理如图所示： Hive 中的 SMB Map Join 就是对两个分桶的数据按照上述思路进行 Join 操作。可以看出，SMB Map Join 与 Bucket Map Join 相比，在进行 Join 操作时，Map 端是无需对整个 Bucket 构建 hash table，也无需在 Map 端缓存整个 Bucket 数据的，每个 Mapper 只需按顺序逐个 key 读取两个分桶的数据进行 join 即可。\n优化 Sort Merge Bucket Map Join 有两种触发方式，包括 Hint 提示和自动转换。Hint 提示已过时，不推荐使用。下面是自动转换的相关参数：\n\u0026ndash;启动 Sort Merge Bucket Map Join 优化\nset hive.optimize.bucketmapjoin.sortedmerge=true;\n\u0026ndash;使用自动转换 SMB Join\nset hive.auto.convert.sortmerge.join=true;\n和 bucket map join 一样，创建分桶表并导入数据 ，设置参数，运行 HQL，结果如下： 数据倾斜优化 数据倾斜问题，通常是指参与计算的数据分布不均，即某个 key 或者某些 key 的数据量远超其他 key，导致在 shuffle 阶段，大量相同 key 的数据被发往同一个 Reduce，进而导致该 Reduce 所需的时间远超其他 Reduce，成为整个任务的瓶颈。\nHive 中的数据倾斜常出现在分组聚合和 join 操作的场景中，下面分别介绍在上述两种场景下的优化思路。\n分组聚合导致的数据倾斜 Hive 中未经优化的分组聚合，是通过一个 MapReduce Job 实现的。Map 端负责读取数据，并按照分组字段分区，通过 Shuffle，将数据发往 Reduce 端，各组数据在 Reduce 端完成最终的聚合运算。\n如果 group by 分组字段的值分布不均，就可能导致大量相同的 key 进入同一 Reduce，从而导致数据倾斜问题。\n由分组聚合导致的数据倾斜问题，有以下两种解决思路：\nMap-Side 聚合 前文提过，此处略过\nSkew-GroupBy 优化 原理是启动两个 MR 任务，第一个 MR 按照随机数分区，将数据分散发送到 Reduce，完成部分聚合，第二个 MR 把打散的数据按照分组字段分区，完成最终聚合。\n优化前 该表数据中的 province_id 字段是存在倾斜的，若不经过优化，通过观察任务的执行过程，是能够看出数据倾斜现象的。 优化后 \u0026ndash;启用 skew-groupby\nset hive.groupby.skewindata=true;\n\u0026ndash;关闭 map-side 聚合（map side 聚合默认是开启的）\nset hive.map.aggr=false;\n开启 Skew-GroupBy 优化后，可以很明显看到该 sql 执行在 yarn 上启动了两个 mr 任务，第一个 mr 打散数据，第二个 mr 把打散后的数据进行分组聚合。 Join 导致的数据倾斜 未经优化的 join 操作，默认是使用 common join 算法，也就是通过一个 MapReduce Job 完成计算。Map 端负责读取 join 操作所需表的数据，并按照关联字段进行分区，通过 Shuffle，将其发送到 Reduce 端，相同 key 的数据在 Reduce 端完成最终的 Join 操作。\n如果关联字段的值分布不均，就可能导致大量相同的 key 进入同一 Reduce，从而导致数据倾斜问题。由 join 导致的数据倾斜问题，有如下三种解决方案：\nmap join 略过\nskew join 原理是为倾斜的大 key 单独启动一个 map join 任务进行计算，其余 key 进行正常的 common join。原理图如下： \u0026ndash;启用 skew join 优化\nset hive.optimize.skewjoin=true;\n\u0026ndash;触发 skew join 的阈值，若某个 key 的行数超过该参数值，则触发\nset hive.skewjoin.key=100000;\n这种方案对参与 join 的源表大小没有要求，但是对两表中倾斜的 key 的数据量有要求，要求一张表中的倾斜 key 的数据量比较小（方便走 map join）。\n任务并行度优化 Hive 的计算任务由 MapReduce 完成，故并行度的调整需要分为 Map 端和 Reduce 端。\nMap 端并行度 Map 端的并行度，也就是 Map 的个数。是由输入文件的切片数决定的。一般情况下，Map 端的并行度无需手动调整。\n以下特殊情况可考虑调整 map 端并行度： 1）查询的表中存在大量小文件\n按照 Hadoop 默认的切片策略，一个小文件会单独启动一个 map task 负责计算。若查询的表中存在大量小文件，则会启动大量 map task，造成计算资源的浪费。这种情况下，可以使用 Hive 提供的 CombineHiveInputFormat，多个小文件合并为一个切片，从而控制 map task 个数。相关参数如下：\nset hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n2）map 端有复杂的查询逻辑\n若 SQL 语句中有正则替换、json 解析等复杂耗时的查询逻辑时，map 端的计算会相对慢一些。若想加快计算速度，在计算资源充足的情况下，可考虑增大 map 端的并行度，令 map task 多一些，每个 map task 计算的数据少一些。相关参数如下：\n\u0026ndash;一个切片的最大值\nset mapreduce.input.fileinputformat.split.maxsize=256000000;\nReduce 端并行度 Reduce 端的并行度，可由用户自己指定，也可由 Hive 自行根据该 MR Job 输入的文件大小进行估算。\nReduce 端的并行度的相关参数如下：\n\u0026ndash;指定 Reduce 端并行度，默认值为-1，表示用户未指定\nset mapreduce.job.reduces;\n\u0026ndash;Reduce 端并行度最大值\nset hive.exec.reducers.max;\n\u0026ndash;单个 Reduce Task 计算的数据量，用于估算 Reduce 并行度\nset hive.exec.reducers.bytes.per.reducer;\nReduce 端并行度的确定逻辑如下：\n若指定参数 mapreduce.job.reduces 的值为一个非负整数，则 Reduce 并行度为指定值。否则，Hive 自行估算 Reduce 并行度，估算逻辑如下：\n假设 Job 输入的文件大小为 totalInputBytes\n参数 hive.exec.reducers.bytes.per.reducer 的值为 bytesPerReducer。\n参数 hive.exec.reducers.max 的值为 maxReducers。\n则 Reduce 端的并行度为： 根据上述描述，可以看出，Hive 自行估算 Reduce 并行度时，是以整个 MR Job 输入的文件大小作为依据的。因此，在某些情况下其估计的并行度很可能并不准确，此时就需要用户根据实际情况来指定 Reduce 并行度了。\n在默认情况下，是会进行 map-side 聚合的，也就是 Reduce 端接收的数据，实际上是 map 端完成聚合之后的结果。观察任务的执行过程，会发现，每个 map 端输出的数据只有 34 条记录，共有 5 个 map task。 也就是说 Reduce 端实际只会接收 170（34*5）条记录，故理论上 Reduce 端并行度设置为 1 就足够了。这种情况下，用户可通过以下参数，自行设置 Reduce 端并行度为 1，这样把 5 个文件合并为只输出 1 个文件。\n\u0026ndash;指定 Reduce 端并行度，默认值为-1，表示用户未指定\nset mapreduce.job.reduces=1;\n小文件合并优化 Map 端输入的小文件合并，和 Reduce 端输出的小文件合并。\n合并 Map 端输入的小文件 将多个小文件划分到一个切片中，进而由一个 Map Task 去处理。目的是防止为单个小文件启动一个 Map Task，浪费计算资源。\n相关参数为：\n\u0026ndash;可将多个小文件切片，合并为一个切片，进而由一个 map 任务处理（默认） set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n合并 Reduce 端输出的小文件 将多个小文件合并成大文件。目的是减少 HDFS 小文件数量。其原理是根据计算任务输出文件的平均大小进行判断，若符合条件，则单独启动 1 个额外的任务进行合并。\n相关参数为：\n\u0026ndash;开启合并 map only 任务输出的小文件，默认 false\nset hive.merge.mapfiles=true;\n\u0026ndash;开启合并 map reduce 任务输出的小文件，默认 false\nset hive.merge.mapredfiles=true;\n\u0026ndash;合并后的文件大小\nset hive.merge.size.per.task=256000000;\n\u0026ndash;触发小文件合并任务的阈值，若某计算任务输出的文件平均大小低于该值，则触发合并\nset hive.merge.smallfiles.avgsize=16000000;\n若 reduce 端设置并行度为 5，则输出 5 个文件。下图为输出文件，可以看出，5 个均为小文件： 要避免 5 个小文件产生，可以设置 reduce 端并行度为 1，有几个 reduce 并行就有几个文件产生，保证其输出结果只有一个文件或启用 hive 合并小文件优化。\n启用 Hive 合并小文件优化\n设置以下参数：\n\u0026ndash;开启合并 map reduce 任务输出的小文件\nset hive.merge.mapredfiles=true;\n\u0026ndash;合并后的文件大小\nset hive.merge.size.per.task=256000000;\n\u0026ndash;触发小文件合并任务的阈值，若某计算任务输出的文件平均大小低于该值，则触发合并\nset hive.merge.smallfiles.avgsize=16000000;\n这样输出文件就合并为一个了 其他优化 CBO 优化 CBO 是指 Cost based Optimizer，即基于计算成本的优化。\n在 Hive 中，计算成本模型考虑到了：数据的行数、CPU、本地 IO、HDFS IO、网络 IO 等方面。Hive 会计算同一 SQL 语句的不同执行计划的计算成本，并选出成本最低的执行计划。目前 CBO 在 hive 的 MR 引擎下主要用于 join 的优化，例如多表 join 的 join 顺序。\n相关参数为：\n\u0026ndash;是否启用 cbo 优化\nset hive.cbo.enable=true;\n1）示例 HQL\nhive (default)\u0026gt; select * from order_detail od\njoin product_info product on od.product_id=product.id\njoin province_info province on od.province_id=province.id;\n2）关闭 CBO 优化\n\u0026ndash;关闭 cbo 优化\nset hive.cbo.enable=false;\n\u0026ndash;为了测试效果更加直观，关闭 map join 自动转换\nset hive.auto.convert.join=false;\n根据执行计划，可以看出，三张表的 join 顺序如下： 3）开启 CBO 优化\n\u0026ndash;开启 cbo 优化\nset hive.cbo.enable=true;\n\u0026ndash;为了测试效果更加直观，关闭 map join 自动转换\nset hive.auto.convert.join=false;\n根据执行计划，可以看出，三张表的 join 顺序如下： CBO 优化对于执行计划中 join 顺序是有影响的，其之所以会将 province_info 的 join 顺序提前，是因为 province info 的数据量较小，将其提前，会有更大的概率使得中间结果的数据量变小，从而使整个计算任务的数据量减小，也就是使计算成本变小。\n谓词下推 谓词下推（predicate pushdown）是指，尽量将过滤操作前移，以减少后续计算步骤的数据量。\n相关参数为：\n\u0026ndash;是否启动谓词下推（predicate pushdown）优化\nset hive.optimize.ppd = true;\n需要注意的是：CBO 优化也会完成一部分的谓词下推优化工作，因为在执行计划中，谓词越靠前，整个计划的计算成本就会越低。 矢量化查询 Hive 的矢量化查询优化，依赖于 CPU 的矢量化计算，CPU 的矢量化计算的基本原理如下图： 相关参数如下：\nset hive.vectorized.execution.enabled=true;\n若执行计划中，出现“Execution mode: vectorized”字样，即表明使用了矢量化计算。\nFetch 抓取 Fetch 抓取是指，Hive 中对某些情况的查询可以不必使用 MapReduce 计算。例如：select * from emp;在这种情况下，Hive 可以简单地读取 emp 对应的存储目录下的文件，然后输出查询结果到控制台。\n相关参数如下：\n\u0026ndash;是否在特定场景转换为 fetch 任务\n\u0026ndash;设置为 none 表示不转换\n\u0026ndash;设置为 minimal 表示支持 select *，分区字段过滤，Limit 等\n\u0026ndash;设置为 more 表示支持 select 任意字段,包括函数，过滤，和 limit 等\nset hive.fetch.task.conversion=more;\n本地模式 大多数的 Hadoop Job 是需要 Hadoop 提供的完整的可扩展性来处理大数据集的。不过，有时 Hive 的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际 job 的执行时间要多的多。对于大多数这种情况，Hive 可以通过本地模式在单台机器上处理所有的任务，不必提交到 Yarn。对于小数据集，执行时间可以明显被缩短。\n相关参数如下：\n\u0026ndash;开启自动转换为本地模式\nset hive.exec.mode.local.auto=true;\n\u0026ndash;设置 local MapReduce 的最大输入数据量，当输入数据量小于这个值时采用 local MapReduce 的方式，默认为 134217728，即 128M\nset hive.exec.mode.local.auto.inputbytes.max=50000000;\n\u0026ndash;设置 local MapReduce 的最大输入文件个数，当输入文件个数小于这个值时采用 local MapReduce 的方式，默认为 4\nset hive.exec.mode.local.auto.input.files.max=10;\n并行执行 Hive 会将一个 SQL 语句转化成一个或者多个 Stage，每个 Stage 对应一个 MR Job。默认情况下，Hive 同时只会执行一个 Stage。但是某 SQL 语句可能会包含多个 Stage，但这多个 Stage 可能并非完全互相依赖，也就是说有些 Stage 是可以并行执行的。此处提到的并行执行就是指这些 Stage 的并行执行。\n相关参数如下：\n\u0026ndash;启用并行执行优化\nset hive.exec.parallel=true;\n\u0026ndash;同一个 sql 允许最大并行度，默认为 8\nset hive.exec.parallel.thread.number=8;\n严格模式 Hive 可以通过设置某些参数防止危险操作：\n1）分区表不使用分区过滤\n将 hive.strict.checks.no.partition.filter 设置为 true 时，对于分区表，除非 where 语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。\n2）使用 order by 没有 limit 过滤\n将 hive.strict.checks.orderby.no.limit 设置为 true 时，对于使用了 order by 语句的查询，要求必须使用 limit 语句。因为 order by 为了执行排序过程会将所有的结果数据分发到同一个 Reduce 中进行处理，强制要求用户增加这个 limit 语句可以防止 Reduce 额外执行很长一段时间（开启了 limit 可以在数据进入到 Reduce 之前就减少一部分数据）。\n3）笛卡尔积\n将 hive.strict.checks.cartesian.product 设置为 true 时，会限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在执行 JOIN 查询的时候不使用 ON 语句而是使用 where 语句，这样关系数据库的执行优化器就可以高效地将 WHERE 语句转化成那个 ON 语句。不幸的是，Hive 并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。\n","date":"2024-04-13T20:49:38Z","permalink":"/zh-cn/post/2024/04/hive%E8%B0%83%E4%BC%98/","title":"Hive调优"},{"content":"1.修饰变量 const int* p int n=10; int m=20; const int* p=\u0026amp;m; //此时const修饰 *p，也就是p保存的地址里面的值不能改变，若*p=60，则会报错 但是可以改变p存放的地址： p=\u0026amp;n; printf(\u0026quot;%d\u0026quot;,*p);//打印结果是10 int* const p int n=10; int m=20; int* const p=\u0026amp;n; //此时const只修饰p，也就是p的值只能为\u0026amp;n，无法修改，若p=\u0026amp;m 会报错 但是可以改变*p的值： *p=60; //虽然p存放的\u0026amp;n无法修改，但是*p得到的n的值可以修改为60 printf(\u0026quot;*p=%d,n=\u0026quot;,*p,n);//打印结果为60，60 2.修饰函数 修饰函数的形参 void StringCopy(char* dest, const char* src); 这个字符串 copy 函数中，把 src 指向的字符串复制到 dest 中，也就是 src 的值是不需要改变的，而 dest 指针指向的值改变。对*src 加上 const 后就可以防止其中的值被修改，起到保护作用。\n而起到保护作呕也能够的原因是：\n在实参中，指针会指向一段内存地址，调用函数之后，函数会产生一个临时指针变量，这个临时指针变量的地址与实参的地址不同，但却指向同一块空间地址。那么**形参加上 const 修饰之后，保护了这一块内存地址不被修改，如果刻意修改这一块内存，编译器会报错。当然，如果使用 c++中的引用，效率会比指针更高，因为引用本身是一个变量，而这个变量仅仅是另外一个变量的别名，它不是指针，不占用内存空间，不会产生临时指针变量，并且不存在 NULL 空引用，无需 assert，比指针更安全。**同时，理论上指针的级数没有限制;但引用只有一级。\n即不存在引用的引用，但可以有指针的指针。\n修饰函数的返回值 如果用 const 修饰返回值类型为指针，那么函数返回值（即指针）的内容不能被修改，该返回值只能被赋给加 const 修饰的同类型指针。\nconst char * GetString(void); //const修饰的函数返回值类型是char* 那么：\nchar *str = GetString();//报错，没有const修饰 const char *str = GetString();//正确 ","date":"2023-08-04T22:32:09Z","permalink":"/zh-cn/post/2023/08/const%E4%BD%BF%E7%94%A8%E5%8F%8A%E6%84%8F%E4%B9%89-----const-int-p%E5%92%8Cint-const-p/","title":"const使用及意义-----const int* p和int* const p"},{"content":"引导图 正文 数组指针 先不讨论什么是数组指针，举个例子\n1 2 3 4 5 int* p1; //p1是整型指针，它指向的是整型，指针类型是int* char* p2; //p2是字符指针，它指向的是字符，指针类型是char* 注意！！！为什么我写成了 int* p1 而不是 int *p1，其实两种写法都可以，只不过为了方便理解，把 int*看成一种类似于 int，char，float 的变量类型，同理 char*也是如此，后文不再阐述。\nSo 数组指针就是指向数组的指针！\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 int arr[5]={1,2,3,4,5}; //数组名arr相当于数组首元素arr[0]的地址,+1跨越了一个数组元素 //但是有一个例外，sizeof(数组名)计算的是整个数组的大小，而非一个数组元素的大小 //\u0026amp;arr才是数组真正的地址，+1跨越了一个数组 int (*parr)[5];//取数组地址\u0026amp;arr赋给数组指针parr //因为*parr是带括号的！！所以*parr一体，所以把*parr拿掉，剩下int [5]，左边表示元素类型int，右边表示元素个数[5]，这就是数组指针的本质。 //具体如何理解呢？ //因为原数组arr有5个元素，所以数组指针parr指向的数组元素个数也得为5，写作[5], 就好比5个人乘车，车上必须有5个座位，如果有4个，总不能一个人坐车顶吧。 //而且，原数组arr元素皆为int，所以数组指针为int型， 那么在上句的基础上写作int [5]，最后再加上(*parr),就有了int (*parr)[5]。 当然，若原数组arr元素为int*,那么数组指针写成int* (*parr)[5]; 记住，它只是一个数组指针，只负责指向一个数组，至于数组的元素类型是 int 还是 int*，元素个数是 5 还是 6,都是它所指向的数组决定的，而非指针决定，以下所有内容均可这样理解。至于为何写成（*parr）而不是*parr，这和符号优先级有关，看完下文的\u0026quot;指针数组\u0026quot;就明白了~\n数组指针作为形参接收二维数组 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 void test1(int arr[][3]) {函数体} void test2(int (*parr)[3]) { for(int i=0;i\u0026lt;2;i++) { for(int j=0;j\u0026lt;3;j++) { printf(\u0026#34;%d \u0026#34;,*(*(parr+i)+j)); } printf(\u0026#34;\\n\u0026#34;); } void test3(int* arr) {函数体} void test4(int* arr[3]) {函数体} void test5(int** arr) {函数体} int main() { int arr[2][3]={{9，8，7},{6，5，4}}; test1(arr); //√ test2(arr); //√ test3(arr); //× test4(arr); //× test5(arr); //× } test1 中，二维数组作形参接收二维数组参数，没毛病；在 test2 中，首先要说明的是，二维数组作为参数传参，传过去的是二维数组的第一行的地址（看个人理解），也就是 parr 里面放的是{9，8，7}这个一维数组的地址。我们让其+i 就是第 i 行的地址，然后*（parr+i），把第 i 行的地址取出来，再+j，此时得到第 i 行的第 j 个元素的地址(*(parr+i)+j)，再对其解引用*(*(parr+i)+j)得到第 i 行第 j 个元素的值。至于 test3，形参是整型指针，而实参是一个一维数组的地址，要用数组指针接收才行，错。test4，形参是整型指针数组，用来接收整型指针，而我们的实参是一维数组地址，驴唇不对马嘴，pass。test5，形参是二级指针，用来接收一级指针变量的地址或者二级指针，同样驴唇不对马嘴，pass。\n指针数组 同上，先不讨论什么是指针数组，举个例子\n1 2 3 4 5 int arr1[5]={1,2,3,4,5}; //这是 整型 数组，存放整型元素 char arr2[5]={\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;d\u0026#39;,\u0026#39;e\u0026#39;}; //这是 字符 数组，存放字符型元素 So 指针数组就是存放指针的数组！\n1 2 3 4 5 6 7 8 9 10 11 12 13 int arr[5]={1,2,3,4,5}; int* p; int m; char* k; int* parr[3]={arr,p,\u0026amp;m}; //* parr没有带括号！！那么只把parr拿掉，只剩int* [3],和数组指针一样，左边表示元素类型int*，右边表示元素个数[3],这是指针数组的本质。 //当然还可以写成int* [4],只要≥3就行，因为parr至少要存放3个元素 //注意！！！ k是字符型指针，无法存放到整型指针数组中 //如何理解？ //首先，arr和\u0026amp;m是地址，p是指针，一共3个元素全放到指针数组parr里，就有了元素个数[3] //其次，arr和\u0026amp;m以及p都是int*(注意，m是int，\u0026amp;m取了地址就是int*，而arr本身就是地址)，所以指针数组元素类型为int*，于是有了int* [3],数组取名为parr 没错，只要是 int*类型的都可以放到数组 parr 里，因为 parr 就是存放整型指针的数组。\n不用管数组 arr 里面几个元素，我们已经在 parr 里面放了数组 arr 的首元素地址，那么就可以借此访问数组 arr 的所有元素。\n指向指针数组的数组指针 顾名思义，这是一个数组指针，只不过它指向的是指针数组，而这个数组里面存放的是指针。\n它长个什么样呢？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 //6个指针数组的元素 int *p0,*p1,*p2; int p3，p4; int p5[]={7,8,9，10}; //2个指针数组 int *pa[3]={p0,\u0026amp;p3,p5}; int *pb[3]={p1,p2 ,\u0026amp;p4}; //OK,接下来我要用 数组指针 指着这俩 指针数组 ~ int* (pc[2])[3]={pa,pb};//pc就是指向指针数组的数组指针 //如何理解呢？ //首先pc[2]带了括号，那就把pc[2]拿掉，剩下int* [3],这不就是个指针数组吗？左边元素类型int*，右边数组元素个数[3]. //当然可以写成int* [4],总之\u0026gt;=3就行，因为数组pa和pb元素个数都为3，至少得放得下3个 //如果再加上pc[2]，那就是说明我有俩指针pc[0]和pc[1],分别指向两个数组pa和pb，pa和pb各有3个元素 //当然pc[2]可以写成pc[3],只要\u0026gt;=2就行，因为数组指针pc至少要指向2个指针数组pa和pb 存放数组指针的指针数组 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 //2个数组 int p1[3]={4,5,6}; int p5[3]={7,8,9}; //2个数组指针 int (*pa)[3]=p1; int (*pb)[3]=p5; //1个指针数组存放2个数组指针 int* (pc[2])[3]={pa,pb};//pc就是存放数组指针的指针数组 //如何理解？ //老规矩，pc[2]带了括号，是一体的，把pc[2]拿掉，剩下int* [3], //左边就是元素类型int*，右边是数组指针指向的数组的元素个数[3]， //当然还可以写成int* [4],总之\u0026gt;=3就行，因为数组p1和p5最多3个元素，至少得放得下3个； //同理pc[2]可以写成pc[3]，\u0026gt;=2就行，因为至少要存放2个数组指针pa和pb。 函数指针 函数指针作形参接收参数 先看看这个 qsort 函数，是 C 语言库函数之一，用于对任何元素类型的数组排序。它有 4 个形参，第一个是要排序数组的地址，第二个是数组元素个数，第三个是数组元素的字节大小，也就是跨越一个元素的步长，第四个就是本节重点，一个函数指针，用来接收数组排序所需的排序函数，比如冒泡，选择排序\u0026hellip;\nOK，请看下面的代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #include\u0026lt;stdio.h\u0026gt; int add(int x,int y) { return x+y;} int sub(int x,int y) { return x-y;} int Calc(int (*p)(int,int))//函数指针作为形参接收函数地址 { int x=0,y=0; printf(\u0026#34;请输入两个操作数:\u0026gt;\u0026#34;); scanf(\u0026#34;%d %d\u0026#34;,\u0026amp;x,\u0026amp;y); return (*p)(x,y);//p(x,y)也可以 } int main() { int ret=Calc(add);//把add函数地址传过去，用函数指针接收 printf(\u0026#34;加法运算 %d\\n\u0026#34;,ret);//结果是8 ret=Calc(sub); printf(\u0026#34;减法运算 %d\\n\u0026#34;,ret);//结果是-2 return 0; } 运算结果如下：\n这就是函数指针。。。\n函数指针接收函数地址 如图：函数名本身就是函数的地址，和\u0026amp;函数名效果相同，*ptr 说明其是指针，int （*ptr）说明函数的返回值是 int，而(int , int)则是函数 add 的形参。当然，函数指针也可以和数组指针一样放在指针数组中，比如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 int add(int x,int y) { return x+y;} int sub(int x,int y) { return x-y;} int main() { int (*p1)(int,int)=add; int (*p2)(int,int)=sub; int (*p[2])(int ,int)={p1,p2};//两个函数指针放到函数指数组里 (*p1)(3,5);//结果是8 (*p2)(3,5);//结果是-2 } 当然我还可以再找一个指针指向这个函数指针数组，然后把这个指针再放到指针数组里，这样无限套娃\u0026hellip;..\n玩点好玩的 1 (* (void(*)()) 0)() 这个出自《C 语言陷阱与缺陷》这本书，作用是调用首地址为 0 地址的子例程。首先 0，用(void(*)())\n把它转化成函数指针，也就是(void(*)()) 0，然后对这个函数指针调用（* (void(*)()) 0) ( ),显然形参为空。看解释：\n1 2 3 4 5 6 //调用0地址处的函数 //该函数无参，返回类型是void //1.void(*) () ---函数指针类型 //2.(void(*) ()) 0 ---对0强制类型转化，成为一个函数地址 //3.*(void(*) ()) 0 ---我TM直接对地址解引用 //4.(*(void(*) ()) 0) () ---调用0地址处的函数 OK，本文到此为止，NND 写了我四五个小时\u0026hellip;.\n","date":"2023-08-04T16:08:31Z","permalink":"/zh-cn/post/2023/08/c-%E6%8C%87%E9%92%88%E8%BF%9B%E9%98%B6--%E6%95%B0%E7%BB%84%E6%8C%87%E9%92%88-%E6%8C%87%E9%92%88%E6%95%B0%E7%BB%84-%E5%87%BD%E6%95%B0%E6%8C%87%E9%92%88%E5%8F%8A%E4%BC%A0%E5%8F%82/","title":"C 指针进阶--数组指针 指针数组 函数指针及传参"}]